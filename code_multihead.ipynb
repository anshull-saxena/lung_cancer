{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lung Histopathology Classification: ACA / N / SCC\n",
    "## Multi-CNN + Channel Attention + GA + KNN/SVM/RF + Fusion\n",
    "\n",
    "This notebook implements a comprehensive lung histopathology classification system that combines:\n",
    "- Multiple CNN backbones (DenseNet121, ResNet50, VGG16)\n",
    "- Channel attention mechanism (SE blocks)\n",
    "- Genetic Algorithm for feature selection\n",
    "- Ensemble of classical ML classifiers (KNN, SVM, Random Forest)\n",
    "- Majority voting fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.25.2\n",
      "  Using cached numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting tensorflow==2.15.0\n",
      "  Using cached tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
      "Collecting keras==2.15.0\n",
      "  Using cached keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow==2.15.0)\n",
      "  Using cached absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow==2.15.0)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow==2.15.0)\n",
      "  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow==2.15.0)\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow==2.15.0)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=2.9.0 (from tensorflow==2.15.0)\n",
      "  Using cached h5py-3.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow==2.15.0)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting ml-dtypes~=0.2.0 (from tensorflow==2.15.0)\n",
      "  Using cached ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow==2.15.0)\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting packaging (from tensorflow==2.15.0)\n",
      "  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.15.0)\n",
      "  Using cached protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Collecting setuptools (from tensorflow==2.15.0)\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting six>=1.12.0 (from tensorflow==2.15.0)\n",
      "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow==2.15.0)\n",
      "  Using cached termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting typing-extensions>=3.6.6 (from tensorflow==2.15.0)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.15.0)\n",
      "  Using cached wrapt-1.14.2-cp310-cp310-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow==2.15.0)\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow==2.15.0)\n",
      "  Using cached grpcio-1.74.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.0)\n",
      "  Using cached tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
      "  Using cached tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Using cached google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting google-auth-oauthlib<2,>=0.5 (from tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Using cached google_auth_oauthlib-1.2.2-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Using cached markdown-3.9-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting requests<3,>=2.21.0 (from tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Using cached cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Using cached pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Using cached rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Using cached charset_normalizer-3.4.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Using cached certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow==2.15.0)\n",
      "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Using cached oauthlib-3.3.1-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Using cached MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Using cached numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Using cached tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
      "Using cached keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "Using cached grpcio-1.74.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
      "Using cached ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "Using cached protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "Using cached tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
      "Using cached google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
      "Using cached cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Using cached google_auth_oauthlib-1.2.2-py3-none-any.whl (19 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Using cached charset_normalizer-3.4.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (152 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "Using cached tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached wrapt-1.14.2-cp310-cp310-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (76 kB)\n",
      "Using cached absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Using cached certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "Using cached flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached h5py-3.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "Using cached markdown-3.9-py3-none-any.whl (107 kB)\n",
      "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Using cached pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached oauthlib-3.3.1-py3-none-any.whl (160 kB)\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached tensorflow_io_gcs_filesystem-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "Using cached termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\n",
      "Using cached packaging-25.0-py3-none-any.whl (66 kB)\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, wheel, urllib3, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, six, setuptools, pyasn1, protobuf, packaging, opt-einsum, oauthlib, numpy, MarkupSafe, markdown, keras, idna, grpcio, gast, charset_normalizer, certifi, cachetools, absl-py, werkzeug, rsa, requests, pyasn1-modules, ml-dtypes, h5py, google-pasta, astunparse, requests-oauthlib, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "\u001b[2K  Attempting uninstall: libclang\n",
      "\u001b[2K    Found existing installation: libclang 18.1.1\n",
      "\u001b[2K    Uninstalling libclang-18.1.1:\n",
      "\u001b[2K      Successfully uninstalled libclang-18.1.1\n",
      "\u001b[2K  Attempting uninstall: flatbuffers━━━━━━━━━━━━━\u001b[0m \u001b[32m 0/41\u001b[0m [libclang]\n",
      "\u001b[2K    Found existing installation: flatbuffers 25.2.102m 0/41\u001b[0m [libclang]\n",
      "\u001b[2K    Uninstalling flatbuffers-25.2.10:━━━━━━━\u001b[0m \u001b[32m 0/41\u001b[0m [libclang]\n",
      "\u001b[2K      Successfully uninstalled flatbuffers-25.2.10[32m 0/41\u001b[0m [libclang]\n",
      "\u001b[2K  Attempting uninstall: wrapt━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 0/41\u001b[0m [libclang]\n",
      "\u001b[2K    Found existing installation: wrapt 1.14.2[0m \u001b[32m 0/41\u001b[0m [libclang]\n",
      "\u001b[2K    Uninstalling wrapt-1.14.2:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/41\u001b[0m [wrapt]\n",
      "\u001b[2K      Successfully uninstalled wrapt-1.14.2━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/41\u001b[0m [wrapt]\n",
      "\u001b[2K  Attempting uninstall: wheel━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/41\u001b[0m [wrapt]\n",
      "\u001b[2K    Found existing installation: wheel 0.45.1━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/41\u001b[0m [wrapt]\n",
      "\u001b[2K    Uninstalling wheel-0.45.1:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/41\u001b[0m [wrapt]\n",
      "\u001b[2K      Successfully uninstalled wheel-0.45.1━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/41\u001b[0m [wrapt]\n",
      "\u001b[2K  Attempting uninstall: urllib3━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/41\u001b[0m [wrapt]\n",
      "\u001b[2K    Found existing installation: urllib3 2.5.0━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/41\u001b[0m [wrapt]\n",
      "\u001b[2K    Uninstalling urllib3-2.5.0:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/41\u001b[0m [wrapt]\n",
      "\u001b[2K      Successfully uninstalled urllib3-2.5.0━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/41\u001b[0m [wrapt]\n",
      "\u001b[2K  Attempting uninstall: typing-extensions━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/41\u001b[0m [urllib3]\n",
      "\u001b[2K    Found existing installation: typing_extensions 4.15.0━━━━━\u001b[0m \u001b[32m 4/41\u001b[0m [urllib3]\n",
      "\u001b[2K    Uninstalling typing_extensions-4.15.0:━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/41\u001b[0m [urllib3]\n",
      "\u001b[2K      Successfully uninstalled typing_extensions-4.15.0━━━━━━━━━━━\u001b[0m \u001b[32m 5/41\u001b[0m [typing-extensions]\n",
      "\u001b[2K  Attempting uninstall: termcolor━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/41\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Found existing installation: termcolor 3.1.0━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/41\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Uninstalling termcolor-3.1.0:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/41\u001b[0m [typing-extensions]\n",
      "\u001b[2K      Successfully uninstalled termcolor-3.1.0━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/41\u001b[0m [typing-extensions]\n",
      "\u001b[2K  Attempting uninstall: tensorflow-io-gcs-filesystem━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/41\u001b[0m [termcolor]ons]\n",
      "\u001b[2K    Found existing installation: tensorflow-io-gcs-filesystem 0.37.1[32m 6/41\u001b[0m [termcolor]\n",
      "\u001b[2K    Uninstalling tensorflow-io-gcs-filesystem-0.37.1:━━━━━━━━━\u001b[0m \u001b[32m 6/41\u001b[0m [termcolor]\n",
      "\u001b[2K      Successfully uninstalled tensorflow-io-gcs-filesystem-0.37.1 \u001b[32m 6/41\u001b[0m [termcolor]\n",
      "\u001b[2K  Attempting uninstall: tensorflow-estimator━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/41\u001b[0m [termcolor]\n",
      "\u001b[2K    Found existing installation: tensorflow-estimator 2.15.0━━\u001b[0m \u001b[32m 6/41\u001b[0m [termcolor]\n",
      "\u001b[2K    Uninstalling tensorflow-estimator-2.15.0:━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/41\u001b[0m [termcolor]\n",
      "\u001b[2K      Successfully uninstalled tensorflow-estimator-2.15.0━━━━\u001b[0m \u001b[32m 6/41\u001b[0m [termcolor]\n",
      "\u001b[2K  Attempting uninstall: tensorboard-data-server━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/41\u001b[0m [tensorflow-estimator]\n",
      "\u001b[2K    Found existing installation: tensorboard-data-server 0.7.2\u001b[0m \u001b[32m 8/41\u001b[0m [tensorflow-estimator]\n",
      "\u001b[2K    Uninstalling tensorboard-data-server-0.7.2:━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/41\u001b[0m [tensorflow-estimator]\n",
      "\u001b[2K      Successfully uninstalled tensorboard-data-server-0.7.2━━\u001b[0m \u001b[32m 8/41\u001b[0m [tensorflow-estimator]\n",
      "\u001b[2K  Attempting uninstall: six[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/41\u001b[0m [tensorboard-data-server]\n",
      "\u001b[2K    Found existing installation: six 1.17.0━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/41\u001b[0m [tensorboard-data-server]\n",
      "\u001b[2K    Uninstalling six-1.17.0:90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/41\u001b[0m [tensorboard-data-server]\n",
      "\u001b[2K      Successfully uninstalled six-1.17.0━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/41\u001b[0m [six]ard-data-server]\n",
      "\u001b[2K  Attempting uninstall: setuptools━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/41\u001b[0m [six]\n",
      "\u001b[2K    Found existing installation: setuptools 80.9.0━━━━━━━━━━━━\u001b[0m \u001b[32m10/41\u001b[0m [six]\n",
      "\u001b[2K    Uninstalling setuptools-80.9.0:━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/41\u001b[0m [six]\n",
      "\u001b[2K      Successfully uninstalled setuptools-80.9.0━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/41\u001b[0m [setuptools]\n",
      "\u001b[2K  Attempting uninstall: pyasn10m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/41\u001b[0m [setuptools]\n",
      "\u001b[2K    Found existing installation: pyasn1 0.6.1━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/41\u001b[0m [setuptools]\n",
      "\u001b[2K    Uninstalling pyasn1-0.6.1:90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/41\u001b[0m [setuptools]\n",
      "\u001b[2K      Successfully uninstalled pyasn1-0.6.1━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/41\u001b[0m [setuptools]\n",
      "\u001b[2K  Attempting uninstall: protobufm━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/41\u001b[0m [setuptools]\n",
      "\u001b[2K    Found existing installation: protobuf 4.25.8━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/41\u001b[0m [setuptools]\n",
      "\u001b[2K    Uninstalling protobuf-4.25.8:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/41\u001b[0m [setuptools]\n",
      "\u001b[2K      Successfully uninstalled protobuf-4.25.8━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/41\u001b[0m [protobuf]\n",
      "\u001b[2K  Attempting uninstall: packaging0m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/41\u001b[0m [protobuf]\n",
      "\u001b[2K    Found existing installation: packaging 25.0━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/41\u001b[0m [protobuf]\n",
      "\u001b[2K    Uninstalling packaging-25.0:[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/41\u001b[0m [packaging]\n",
      "\u001b[2K      Successfully uninstalled packaging-25.0━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/41\u001b[0m [packaging]\n",
      "\u001b[2K  Attempting uninstall: opt-einsum0m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/41\u001b[0m [packaging]\n",
      "\u001b[2K    Found existing installation: opt_einsum 3.4.0━━━━━━━━━━━━━\u001b[0m \u001b[32m14/41\u001b[0m [packaging]\n",
      "\u001b[2K    Uninstalling opt_einsum-3.4.0:0m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/41\u001b[0m [packaging]\n",
      "\u001b[2K      Successfully uninstalled opt_einsum-3.4.0━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/41\u001b[0m [packaging]\n",
      "\u001b[2K  Attempting uninstall: oauthlib\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/41\u001b[0m [opt-einsum]\n",
      "\u001b[2K    Found existing installation: oauthlib 3.3.1━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/41\u001b[0m [opt-einsum]\n",
      "\u001b[2K    Uninstalling oauthlib-3.3.1:\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/41\u001b[0m [opt-einsum]\n",
      "\u001b[2K      Successfully uninstalled oauthlib-3.3.1━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/41\u001b[0m [opt-einsum]\n",
      "\u001b[2K  Attempting uninstall: numpy[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/41\u001b[0m [opt-einsum]\n",
      "\u001b[2K    Found existing installation: numpy 1.25.2━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/41\u001b[0m [opt-einsum]\n",
      "\u001b[2K    Uninstalling numpy-1.25.2:91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/41\u001b[0m [numpy]]\n",
      "\u001b[2K      Successfully uninstalled numpy-1.25.2━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/41\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: MarkupSafe\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/41\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: MarkupSafe 3.0.2━━━━━━━━━━━━━\u001b[0m \u001b[32m17/41\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling MarkupSafe-3.0.2:\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/41\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled MarkupSafe-3.0.2━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/41\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: markdown0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/41\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: Markdown 3.8━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/41\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling Markdown-3.8:\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/41\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled Markdown-3.8━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/41\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: kerasm\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19/41\u001b[0m [markdown]\n",
      "\u001b[2K    Found existing installation: keras 2.15.0━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19/41\u001b[0m [markdown]\n",
      "\u001b[2K    Uninstalling keras-2.15.0:m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/41\u001b[0m [keras]\n",
      "\u001b[2K      Successfully uninstalled keras-2.15.0━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/41\u001b[0m [keras]\n",
      "\u001b[2K  Attempting uninstall: idna[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/41\u001b[0m [keras]\n",
      "\u001b[2K    Found existing installation: idna 3.10━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/41\u001b[0m [keras]\n",
      "\u001b[2K    Uninstalling idna-3.10:\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/41\u001b[0m [keras]\n",
      "\u001b[2K      Successfully uninstalled idna-3.100m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/41\u001b[0m [keras]\n",
      "\u001b[2K  Attempting uninstall: grpcio1m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/41\u001b[0m [keras]\n",
      "\u001b[2K    Found existing installation: grpcio 1.74.0━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/41\u001b[0m [keras]\n",
      "\u001b[2K    Uninstalling grpcio-1.74.0:m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/41\u001b[0m [keras]\n",
      "\u001b[2K      Successfully uninstalled grpcio-1.74.0━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/41\u001b[0m [keras]\n",
      "\u001b[2K  Attempting uninstall: gast━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/41\u001b[0m [grpcio]\n",
      "\u001b[2K    Found existing installation: gast 0.6.0m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/41\u001b[0m [grpcio]\n",
      "\u001b[2K    Uninstalling gast-0.6.0:m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/41\u001b[0m [grpcio]\n",
      "\u001b[2K      Successfully uninstalled gast-0.6.090m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/41\u001b[0m [grpcio]\n",
      "\u001b[2K  Attempting uninstall: charset_normalizer0m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/41\u001b[0m [grpcio]\n",
      "\u001b[2K    Found existing installation: charset-normalizer 3.4.3━━━━━\u001b[0m \u001b[32m22/41\u001b[0m [grpcio]\n",
      "\u001b[2K    Uninstalling charset-normalizer-3.4.3:0m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/41\u001b[0m [grpcio]\n",
      "\u001b[2K      Successfully uninstalled charset-normalizer-3.4.3━━━━━━━\u001b[0m \u001b[32m22/41\u001b[0m [grpcio]\n",
      "\u001b[2K  Attempting uninstall: certifi90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/41\u001b[0m [grpcio]\n",
      "\u001b[2K    Found existing installation: certifi 2025.8.3━━━━━━━━━━━━━\u001b[0m \u001b[32m22/41\u001b[0m [grpcio]\n",
      "\u001b[2K    Uninstalling certifi-2025.8.3:╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/41\u001b[0m [grpcio]\n",
      "\u001b[2K      Successfully uninstalled certifi-2025.8.3━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/41\u001b[0m [grpcio]\n",
      "\u001b[2K  Attempting uninstall: cachetools╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/41\u001b[0m [grpcio]\n",
      "\u001b[2K    Found existing installation: cachetools 5.5.2━━━━━━━━━━━━━\u001b[0m \u001b[32m22/41\u001b[0m [grpcio]\n",
      "\u001b[2K    Uninstalling cachetools-5.5.2:╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/41\u001b[0m [grpcio]\n",
      "\u001b[2K      Successfully uninstalled cachetools-5.5.2━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/41\u001b[0m [grpcio]\n",
      "\u001b[2K  Attempting uninstall: absl-py90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/41\u001b[0m [grpcio]\n",
      "\u001b[2K    Found existing installation: absl-py 2.3.1━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/41\u001b[0m [grpcio]\n",
      "\u001b[2K    Uninstalling absl-py-2.3.1:90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/41\u001b[0m [grpcio]\n",
      "\u001b[2K      Successfully uninstalled absl-py-2.3.1━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/41\u001b[0m [grpcio]\n",
      "\u001b[2K  Attempting uninstall: werkzeug━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m27/41\u001b[0m [absl-py]\n",
      "\u001b[2K    Found existing installation: Werkzeug 3.1.30m━━━━━━━━━━━━━\u001b[0m \u001b[32m27/41\u001b[0m [absl-py]\n",
      "\u001b[2K    Uninstalling Werkzeug-3.1.3:0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m27/41\u001b[0m [absl-py]\n",
      "\u001b[2K      Successfully uninstalled Werkzeug-3.1.3[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m27/41\u001b[0m [absl-py]\n",
      "\u001b[2K  Attempting uninstall: rsa━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m28/41\u001b[0m [werkzeug]\n",
      "\u001b[2K    Found existing installation: rsa 4.9.1[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m28/41\u001b[0m [werkzeug]\n",
      "\u001b[2K    Uninstalling rsa-4.9.1:━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m28/41\u001b[0m [werkzeug]\n",
      "\u001b[2K      Successfully uninstalled rsa-4.9.1╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m28/41\u001b[0m [werkzeug]\n",
      "\u001b[2K  Attempting uninstall: requests[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m28/41\u001b[0m [werkzeug]\n",
      "\u001b[2K    Found existing installation: requests 2.32.50m━━━━━━━━━━━━\u001b[0m \u001b[32m28/41\u001b[0m [werkzeug]\n",
      "\u001b[2K    Uninstalling requests-2.32.5:0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m28/41\u001b[0m [werkzeug]\n",
      "\u001b[2K      Successfully uninstalled requests-2.32.5[90m━━━━━━━━━━━━\u001b[0m \u001b[32m28/41\u001b[0m [werkzeug]\n",
      "\u001b[2K  Attempting uninstall: pyasn1-modules0m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m28/41\u001b[0m [werkzeug]\n",
      "\u001b[2K    Found existing installation: pyasn1_modules 0.4.2━━━━━━━━━\u001b[0m \u001b[32m28/41\u001b[0m [werkzeug]\n",
      "\u001b[2K    Uninstalling pyasn1_modules-0.4.2:0m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m28/41\u001b[0m [werkzeug]\n",
      "\u001b[2K      Successfully uninstalled pyasn1_modules-0.4.2━━━━━━━━━━━\u001b[0m \u001b[32m28/41\u001b[0m [werkzeug]\n",
      "\u001b[2K  Attempting uninstall: ml-dtypes━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m31/41\u001b[0m [pyasn1-modules]\n",
      "\u001b[2K    Found existing installation: ml-dtypes 0.2.0\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m31/41\u001b[0m [pyasn1-modules]\n",
      "\u001b[2K    Uninstalling ml-dtypes-0.2.0:━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m31/41\u001b[0m [pyasn1-modules]\n",
      "\u001b[2K      Successfully uninstalled ml-dtypes-0.2.00m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m31/41\u001b[0m [pyasn1-modules]\n",
      "\u001b[2K  Attempting uninstall: h5py━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m31/41\u001b[0m [pyasn1-modules]\n",
      "\u001b[2K    Found existing installation: h5py 3.14.0\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m31/41\u001b[0m [pyasn1-modules]\n",
      "\u001b[2K    Uninstalling h5py-3.14.0:━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m31/41\u001b[0m [pyasn1-modules]\n",
      "\u001b[2K      Successfully uninstalled h5py-3.14.0m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m31/41\u001b[0m [pyasn1-modules]\n",
      "\u001b[2K  Attempting uninstall: google-pasta━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m33/41\u001b[0m [h5py]dules]\n",
      "\u001b[2K    Found existing installation: google-pasta 0.2.0[90m━━━━━━━\u001b[0m \u001b[32m33/41\u001b[0m [h5py]\n",
      "\u001b[2K    Uninstalling google-pasta-0.2.0:\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m33/41\u001b[0m [h5py]\n",
      "\u001b[2K      Successfully uninstalled google-pasta-0.2.0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m33/41\u001b[0m [h5py]\n",
      "\u001b[2K  Attempting uninstall: astunparse━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m33/41\u001b[0m [h5py]\n",
      "\u001b[2K    Found existing installation: astunparse 1.6.3m\u001b[90m━━━━━━━\u001b[0m \u001b[32m33/41\u001b[0m [h5py]\n",
      "\u001b[2K    Uninstalling astunparse-1.6.3:━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m33/41\u001b[0m [h5py]\n",
      "\u001b[2K      Successfully uninstalled astunparse-1.6.3[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m33/41\u001b[0m [h5py]\n",
      "\u001b[2K  Attempting uninstall: requests-oauthlib[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m33/41\u001b[0m [h5py]\n",
      "\u001b[2K    Found existing installation: requests-oauthlib 2.0.0━━━━━━\u001b[0m \u001b[32m33/41\u001b[0m [h5py]\n",
      "\u001b[2K    Uninstalling requests-oauthlib-2.0.0:[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m33/41\u001b[0m [h5py]\n",
      "\u001b[2K      Successfully uninstalled requests-oauthlib-2.0.0m━━━━━━━\u001b[0m \u001b[32m33/41\u001b[0m [h5py]\n",
      "\u001b[2K  Attempting uninstall: google-auth━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m36/41\u001b[0m [requests-oauthlib]\n",
      "\u001b[2K    Found existing installation: google-auth 2.40.30m\u001b[90m━━━━\u001b[0m \u001b[32m36/41\u001b[0m [requests-oauthlib]\n",
      "\u001b[2K    Uninstalling google-auth-2.40.3:━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m36/41\u001b[0m [requests-oauthlib]\n",
      "\u001b[2K      Successfully uninstalled google-auth-2.40.3\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m36/41\u001b[0m [requests-oauthlib]\n",
      "\u001b[2K  Attempting uninstall: google-auth-oauthlib[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m36/41\u001b[0m [requests-oauthlib]\n",
      "\u001b[2K    Found existing installation: google-auth-oauthlib 1.2.2━━━\u001b[0m \u001b[32m36/41\u001b[0m [requests-oauthlib]\n",
      "\u001b[2K    Uninstalling google-auth-oauthlib-1.2.2:[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m36/41\u001b[0m [requests-oauthlib]\n",
      "\u001b[2K      Successfully uninstalled google-auth-oauthlib-1.2.2m━━━━\u001b[0m \u001b[32m36/41\u001b[0m [requests-oauthlib]\n",
      "\u001b[2K  Attempting uninstall: tensorboard━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m38/41\u001b[0m [google-auth-oauthlib]\n",
      "\u001b[2K    Found existing installation: tensorboard 2.15.2\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m38/41\u001b[0m [google-auth-oauthlib]\n",
      "\u001b[2K    Uninstalling tensorboard-2.15.2:━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m38/41\u001b[0m [google-auth-oauthlib]\n",
      "\u001b[2K      Successfully uninstalled tensorboard-2.15.2m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m38/41\u001b[0m [google-auth-oauthlib]\n",
      "\u001b[2K  Attempting uninstall: tensorflow━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m39/41\u001b[0m [tensorboard]hlib]\n",
      "\u001b[2K    Found existing installation: tensorflow 2.15.0m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m39/41\u001b[0m [tensorboard]\n",
      "\u001b[2K    Uninstalling tensorflow-2.15.0:━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m40/41\u001b[0m [tensorflow]board]\n",
      "\u001b[2K      Successfully uninstalled tensorflow-2.15.0[0m\u001b[90m╺\u001b[0m \u001b[32m40/41\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41/41\u001b[0m [tensorflow]nsorflow]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.2 absl-py-2.3.1 astunparse-1.6.3 cachetools-5.5.2 certifi-2025.8.3 charset_normalizer-3.4.3 flatbuffers-25.2.10 gast-0.6.0 google-auth-2.40.3 google-auth-oauthlib-1.2.2 google-pasta-0.2.0 grpcio-1.74.0 h5py-3.14.0 idna-3.10 keras-2.15.0 libclang-18.1.1 markdown-3.9 ml-dtypes-0.2.0 numpy-1.25.2 oauthlib-3.3.1 opt-einsum-3.4.0 packaging-25.0 protobuf-4.25.8 pyasn1-0.6.1 pyasn1-modules-0.4.2 requests-2.32.5 requests-oauthlib-2.0.0 rsa-4.9.1 setuptools-80.9.0 six-1.17.0 tensorboard-2.15.2 tensorboard-data-server-0.7.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0 tensorflow-io-gcs-filesystem-0.37.1 termcolor-3.1.0 typing-extensions-4.15.0 urllib3-2.5.0 werkzeug-3.1.3 wheel-0.45.1 wrapt-1.14.2\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --force-reinstall numpy==1.25.2 tensorflow==2.15.0 keras==2.15.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os, random, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.applications import DenseNet121, ResNet50, EfficientNetB0, InceptionV3\n",
    "from tensorflow.keras.applications.densenet import preprocess_input as pre_densenet\n",
    "from tensorflow.keras.applications.resnet import preprocess_input as pre_resnet\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input as pre_efficientnet\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input as pre_inception\n",
    "from tensorflow.keras.layers import (Input, GlobalAveragePooling2D, GlobalMaxPooling2D,\n",
    "                                     Concatenate, Dense, Reshape, Multiply, Lambda)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from scipy.stats import mode\n",
    "\n",
    "# from deap import base, creator, tools  # GA removed, not needed\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration set:\n",
      "Data Directory: /teamspace/studios/this_studio/lung_cancer/dataset/lung_image_sets\n",
      "Image Size: (224, 224)\n",
      "Batch Size: 24\n",
      "Random Seed: 42\n"
     ]
    }
   ],
   "source": [
    "# Configuration and Data Setup\n",
    "DATA_DIR   = \"/teamspace/studios/this_studio/lung_cancer/dataset/lung_image_sets\"  # << set this\n",
    "IMG_SIZE   = (224, 224)\n",
    "BATCH_SIZE = 24\n",
    "SEED       = 42\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "print(f\"Configuration set:\")\n",
    "print(f\"Data Directory: {DATA_DIR}\")\n",
    "print(f\"Image Size: {IMG_SIZE}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Random Seed: {SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Global Model Hyperparameters ===\n",
    "# Number of attention heads for multi-head channel attention\n",
    "NUM_ATTENTION_HEADS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12000 images belonging to 3 classes.\n",
      "Found 3000 images belonging to 3 classes.\n",
      "Classes: {'lung_aca': 0, 'lung_n': 1, 'lung_scc': 2}\n",
      "Number of classes: 3\n",
      "Training samples: 12000\n",
      "Validation samples: 3000\n"
     ]
    }
   ],
   "source": [
    "# Data Generators Setup\n",
    "# Only lung classes will be present in this directory if you set DATA_DIR as above:\n",
    "# expected subfolders: lung_aca / lung_n / lung_scc\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    validation_split=0.20,\n",
    "    rotation_range=20,\n",
    "    horizontal_flip=True,\n",
    "    # IMPORTANT: no rescale here, since we feed raw to model-specific preprocessors\n",
    ")\n",
    "\n",
    "def make_gen(subset):\n",
    "    return train_datagen.flow_from_directory(\n",
    "        DATA_DIR,\n",
    "        target_size=IMG_SIZE,\n",
    "        class_mode='categorical',\n",
    "        batch_size=BATCH_SIZE,\n",
    "        subset=subset,\n",
    "        seed=SEED,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "train_gen = make_gen('training')\n",
    "val_gen   = make_gen('validation')\n",
    "num_classes = train_gen.num_classes\n",
    "class_indices = train_gen.class_indices\n",
    "id2label = {v:k for k,v in class_indices.items()}\n",
    "\n",
    "print(\"Classes:\", class_indices)\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Training samples: {train_gen.samples}\")\n",
    "print(f\"Validation samples: {val_gen.samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-head attention block function defined successfully!\n"
     ]
    }
   ],
   "source": [
    "# Channel Attention (Multi-Headed) Implementation\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Dense, Reshape, Permute, Concatenate\n",
    "\n",
    "class MultiHeadChannelAttention(Layer):\n",
    "    def __init__(self, num_heads=4, reduction=16, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.channel = input_shape[-1]\n",
    "        self.dense1 = [Dense(self.channel // self.reduction, activation='relu') for _ in range(self.num_heads)]\n",
    "        self.dense2 = [Dense(self.channel) for _ in range(self.num_heads)]\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        # Global pooling\n",
    "        gap = tf.reduce_mean(x, axis=[1,2])  # shape: (batch, channels)\n",
    "        gmp = tf.reduce_max(x, axis=[1,2])   # shape: (batch, channels)\n",
    "        heads = []\n",
    "        for i in range(self.num_heads):\n",
    "            d1_gap = self.dense1[i](gap)\n",
    "            d1_gmp = self.dense1[i](gmp)\n",
    "            d2_gap = self.dense2[i](d1_gap)\n",
    "            d2_gmp = self.dense2[i](d1_gmp)\n",
    "            scale = tf.nn.sigmoid(d2_gap + d2_gmp)\n",
    "            scale = Reshape((1,1,self.channel))(scale)\n",
    "            heads.append(x * scale)\n",
    "        # Concatenate heads along channel axis\n",
    "        out = Concatenate(axis=-1)(heads)\n",
    "        return out\n",
    "\n",
    "# Usage in your lane function:\n",
    "# from multi_head_attention import MultiHeadChannelAttention\n",
    "# x = MultiHeadChannelAttention(num_heads=4, reduction=16)(x)\n",
    "\n",
    "def multi_head_attention_block(x, reduction=16, name=None):\n",
    "    \"\"\"Multi-Headed Channel Attention block for CNN feature maps\"\"\"\n",
    "    attn = MultiHeadChannelAttention(num_heads=NUM_ATTENTION_HEADS, reduction=reduction, name=name)(x)\n",
    "    return attn\n",
    "\n",
    "print(\"Multi-head attention block function defined successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lane function updated for multi-head attention!\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing Lanes (one per backbone)\n",
    "def lane(tensor, backbone=\"resnet\", reduction=16):\n",
    "    \"\"\"Create a processing lane for each CNN backbone with multi-head channel attention\"\"\"\n",
    "    if backbone == \"resnet\":\n",
    "        x = Lambda(pre_resnet, name=\"pre_resnet\")(tensor)\n",
    "        x = ResNet50(include_top=False, weights='imagenet')(x)\n",
    "    elif backbone == \"densenet\":\n",
    "        x = Lambda(pre_densenet, name=\"pre_densenet\")(tensor)\n",
    "        x = DenseNet121(include_top=False, weights='imagenet')(x)\n",
    "    elif backbone == \"efficientnet\":\n",
    "        x = Lambda(pre_efficientnet, name=\"pre_efficientnet\")(tensor)\n",
    "        x = EfficientNetB0(include_top=False, weights='imagenet')(x)\n",
    "    elif backbone == \"inception\":\n",
    "        x = Lambda(pre_inception, name=\"pre_inception\")(tensor)\n",
    "        x = InceptionV3(include_top=False, weights='imagenet')(x)\n",
    "    else:\n",
    "        raise ValueError(f'Unknown backbone: {backbone}')\n",
    "    # Add multi-head channel attention\n",
    "    x = multi_head_attention_block(x, reduction=reduction, name=f\"mhca_{backbone}\")\n",
    "    # Global Average Pooling to convert feature maps → vector\n",
    "    x = GlobalAveragePooling2D(name=f\"gap_{backbone}\")(x)\n",
    "    return x\n",
    "\n",
    "print(\"Lane function updated for multi-head attention!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building multi-backbone feature concatenator with multi-head attention...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extractor built successfully!\n",
      "Feature dimension: 51200\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_6 (InputLayer)        [(None, 224, 224, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " pre_densenet (Lambda)       (None, 224, 224, 3)          0         ['input_6[0][0]']             \n",
      "                                                                                                  \n",
      " pre_resnet (Lambda)         (None, 224, 224, 3)          0         ['input_6[0][0]']             \n",
      "                                                                                                  \n",
      " pre_efficientnet (Lambda)   (None, 224, 224, 3)          0         ['input_6[0][0]']             \n",
      "                                                                                                  \n",
      " pre_inception (Lambda)      (None, 224, 224, 3)          0         ['input_6[0][0]']             \n",
      "                                                                                                  \n",
      " densenet121 (Functional)    (None, None, None, 1024)     7037504   ['pre_densenet[0][0]']        \n",
      "                                                                                                  \n",
      " resnet50 (Functional)       (None, None, None, 2048)     2358771   ['pre_resnet[0][0]']          \n",
      "                                                          2                                       \n",
      "                                                                                                  \n",
      " efficientnetb0 (Functional  (None, None, None, 1280)     4049571   ['pre_efficientnet[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " inception_v3 (Functional)   (None, None, None, 2048)     2180278   ['pre_inception[0][0]']       \n",
      "                                                          4                                       \n",
      "                                                                                                  \n",
      " mhca_densenet (MultiHeadCh  (None, 7, 7, 8192)           1057280   ['densenet121[0][0]']         \n",
      " annelAttention)                                                                                  \n",
      "                                                                                                  \n",
      " mhca_resnet (MultiHeadChan  (None, 7, 7, 16384)          4211712   ['resnet50[0][0]']            \n",
      " nelAttention)                                                                                    \n",
      "                                                                                                  \n",
      " mhca_efficientnet (MultiHe  (None, 7, 7, 10240)          1649280   ['efficientnetb0[0][0]']      \n",
      " adChannelAttention)                                                                              \n",
      "                                                                                                  \n",
      " mhca_inception (MultiHeadC  (None, 5, 5, 16384)          4211712   ['inception_v3[0][0]']        \n",
      " hannelAttention)                                                                                 \n",
      "                                                                                                  \n",
      " gap_densenet (GlobalAverag  (None, 8192)                 0         ['mhca_densenet[0][0]']       \n",
      " ePooling2D)                                                                                      \n",
      "                                                                                                  \n",
      " gap_resnet (GlobalAverageP  (None, 16384)                0         ['mhca_resnet[0][0]']         \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " gap_efficientnet (GlobalAv  (None, 10240)                0         ['mhca_efficientnet[0][0]']   \n",
      " eragePooling2D)                                                                                  \n",
      "                                                                                                  \n",
      " gap_inception (GlobalAvera  (None, 16384)                0         ['mhca_inception[0][0]']      \n",
      " gePooling2D)                                                                                     \n",
      "                                                                                                  \n",
      " concat_feats (Concatenate)  (None, 51200)                0         ['gap_densenet[0][0]',        \n",
      "                                                                     'gap_resnet[0][0]',          \n",
      "                                                                     'gap_efficientnet[0][0]',    \n",
      "                                                                     'gap_inception[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 67607555 (257.90 MB)\n",
      "Trainable params: 67394332 (257.09 MB)\n",
      "Non-trainable params: 213223 (832.91 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build Feature Extractor Model\n",
    "print(\"Building multi-backbone feature concatenator with multi-head attention...\")\n",
    "\n",
    "# Define input tensor with image size (224x224x3 RGB)\n",
    "inp = Input(shape=(224,224,3))\n",
    "\n",
    "# Extract features from DenseNet lane (multi-head attention)\n",
    "feat_d = lane(inp, \"densenet\", reduction=16)\n",
    "# Extract features from ResNet lane (multi-head attention)\n",
    "feat_r = lane(inp, \"resnet\", reduction=16)\n",
    "# Extract features from EfficientNetB0 lane (multi-head attention)\n",
    "feat_e = lane(inp, \"efficientnet\", reduction=16)\n",
    "# Extract features from InceptionV3 lane (multi-head attention)\n",
    "feat_i = lane(inp, \"inception\", reduction=16)\n",
    "\n",
    "# Concatenate features from all four backbones\n",
    "concat_feat = Concatenate(name=\"concat_feats\")([feat_d, feat_r, feat_e, feat_i])\n",
    "\n",
    "# Create feature extractor model (input → concatenated features)\n",
    "feature_model = Model(inp, concat_feat)\n",
    "\n",
    "# Get final concatenated feature dimension\n",
    "feature_dim = feature_model.output_shape[-1]\n",
    "\n",
    "print(f\"Feature extractor built successfully!\")\n",
    "print(f\"Feature dimension: {feature_dim}\")\n",
    "\n",
    "# Show model summary (layers, parameters, shapes)\n",
    "feature_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction function defined!\n"
     ]
    }
   ],
   "source": [
    "# Extract Deep Features\n",
    "def extract_features(generator):\n",
    "    \"\"\"Extract features from a data generator using the feature model\"\"\"\n",
    "    X, y = [], []\n",
    "    steps = len(generator)\n",
    "    for i in range(steps):\n",
    "        imgs, labels = generator.next()\n",
    "        feats = feature_model.predict(imgs, verbose=0)\n",
    "        X.append(feats)\n",
    "        y.append(labels)\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Processed {i + 1}/{steps} batches\")\n",
    "    return np.vstack(X), np.vstack(y)\n",
    "\n",
    "print(\"Feature extraction function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Training Features\n",
    "print(\"Extracting training features …\")\n",
    "X_tr, Y_tr_ohe = extract_features(train_gen)\n",
    "print(f\"Training features shape: {X_tr.shape}\")\n",
    "print(f\"Training labels shape: {Y_tr_ohe.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cython in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (3.1.3)\n",
      "Collecting pymrmr\n",
      "  Using cached pymrmr-0.1.11.tar.gz (69 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.19.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pymrmr) (1.25.2)\n",
      "Building wheels for collected packages: pymrmr\n",
      "\u001b[33m  DEPRECATION: Building 'pymrmr' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'pymrmr'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for pymrmr (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pymrmr: filename=pymrmr-0.1.11-cp310-cp310-linux_x86_64.whl size=114126 sha256=eac40c5b076fcf2e90e90f76f861b0bc7cbc19c6c280fbeaa08007031395dfea\n",
      "  Stored in directory: /home/zeus/.cache/pip/wheels/46/ae/55/4a2479c5f0de7eb363fe970cb18e4a750e03e4e63b1b5c2005\n",
      "Successfully built pymrmr\n",
      "Installing collected packages: pymrmr\n",
      "Successfully installed pymrmr-0.1.11\n"
     ]
    }
   ],
   "source": [
    "!pip install cython\n",
    "!pip install pymrmr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature selection pipeline (mRMR + AGWO) implemented.\n"
     ]
    }
   ],
   "source": [
    "# Feature Selection: mRMR + Adaptive Grey Wolf Optimization (AGWO)\n",
    "# Remove GA-based feature selection. Use mRMR for initial ranking, then AGWO for final selection.\n",
    "# You may need to install pymrmr and a Grey Wolf Optimizer package, or implement AGWO manually.\n",
    "# Example below assumes features (X) and labels (y) are available after extraction.\n",
    "\n",
    "## 1. mRMR Feature Ranking\n",
    "try:\n",
    "    import pymrmr\n",
    "except ImportError:\n",
    "    !pip install pymrmr\n",
    "    import pymrmr\n",
    "\n",
    "# Convert features and labels to DataFrame for pymrmr\n",
    "import pandas as pd\n",
    "def mrmr_feature_selection(X, y, n_features=100):\n",
    "    df = pd.DataFrame(X)\n",
    "    df['target'] = np.argmax(y, axis=1)\n",
    "    selected = pymrmr.mRMR(df, 'MIQ', n_features)\n",
    "    return selected\n",
    "\n",
    "# Example usage:\n",
    "## selected_features = mrmr_feature_selection(X_train, y_train, n_features=100)\n",
    "\n",
    "## 2. Adaptive Grey Wolf Optimization (AGWO)\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def agwo_feature_selection(X, y, ranked_features, n_wolves=10, n_iter=10, subset_size=30):\n",
    "    # Only use the top ranked features from mRMR\n",
    "    feature_indices = [int(f) for f in ranked_features]\n",
    "    X_ranked = X[:, feature_indices]\n",
    "    n_features = len(feature_indices)\n",
    "    # Initialize wolves (random binary vectors)\n",
    "    wolves = np.random.randint(0, 2, size=(n_wolves, n_features))\n",
    "    # Ensure at least subset_size features are selected\n",
    "    for w in wolves:\n",
    "        idx = np.where(w == 1)[0]\n",
    "        if len(idx) < subset_size:\n",
    "            w[:subset_size] = 1\n",
    "    alpha, beta, delta = None, None, None\n",
    "    alpha_score, beta_score, delta_score = -np.inf, -np.inf, -np.inf\n",
    "\n",
    "    for it in range(n_iter):\n",
    "        for i, wolf in enumerate(wolves):\n",
    "            idx = np.where(wolf == 1)[0]\n",
    "            if len(idx) == 0: continue\n",
    "            X_sel = X_ranked[:, idx]\n",
    "            # Use KNN accuracy as fitness\n",
    "            score = cross_val_score(KNeighborsClassifier(n_neighbors=5), X_sel, np.argmax(y, axis=1), cv=3).mean()\n",
    "            if score > alpha_score:\n",
    "                delta, delta_score = beta, beta_score\n",
    "                beta, beta_score = alpha, alpha_score\n",
    "                alpha, alpha_score = wolf.copy(), score\n",
    "            elif score > beta_score:\n",
    "                delta, delta_score = beta, beta_score\n",
    "                beta, beta_score = wolf.copy(), score\n",
    "            elif score > delta_score:\n",
    "                delta, delta_score = wolf.copy(), score\n",
    "        # Update wolves positions (simplified AGWO)\n",
    "        for i, wolf in enumerate(wolves):\n",
    "            for j in range(n_features):\n",
    "                r = np.random.rand()\n",
    "                if r < 0.33:\n",
    "                    wolf[j] = alpha[j]\n",
    "                elif r < 0.66:\n",
    "                    wolf[j] = beta[j]\n",
    "                else:\n",
    "                    wolf[j] = delta[j]\n",
    "            # Random mutation\n",
    "            if np.random.rand() < 0.1:\n",
    "                flip = np.random.randint(0, n_features)\n",
    "                wolf[flip] = 1 - wolf[flip]\n",
    "    # Return indices of selected features from the best wolf (alpha)\n",
    "    selected_idx = np.where(alpha == 1)[0]\n",
    "    selected_features = [feature_indices[i] for i in selected_idx]\n",
    "    return selected_features\n",
    "\n",
    "print(\"Feature selection pipeline (mRMR + AGWO) implemented.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting validation features …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-10 05:55:31.389484: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10/125 batches\n",
      "Processed 20/125 batches\n",
      "Processed 30/125 batches\n",
      "Processed 40/125 batches\n",
      "Processed 50/125 batches\n",
      "Processed 60/125 batches\n",
      "Processed 70/125 batches\n",
      "Processed 80/125 batches\n",
      "Processed 90/125 batches\n",
      "Processed 100/125 batches\n",
      "Processed 110/125 batches\n",
      "Processed 120/125 batches\n",
      "Validation features shape: (3000, 51200)\n",
      "Validation labels shape: (3000, 3)\n"
     ]
    }
   ],
   "source": [
    "# Extract Validation Features\n",
    "print(\"Extracting validation features …\")\n",
    "X_va, Y_va_ohe = extract_features(val_gen)\n",
    "print(f\"Validation features shape: {X_va.shape}\")\n",
    "print(f\"Validation labels shape: {Y_va_ohe.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_tr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Combine Features and Convert Labels\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m X_full \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack([\u001b[43mX_tr\u001b[49m, X_va])\n\u001b[1;32m      3\u001b[0m y_full \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(np\u001b[38;5;241m.\u001b[39mvstack([Y_tr_ohe, Y_va_ohe]), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal features shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_full\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_tr' is not defined"
     ]
    }
   ],
   "source": [
    "# Combine Features and Convert Labels\n",
    "X_full = np.vstack([X_tr, X_va])\n",
    "y_full = np.argmax(np.vstack([Y_tr_ohe, Y_va_ohe]), axis=1)\n",
    "\n",
    "print(f\"Total features shape: {X_full.shape}\")\n",
    "print(f\"Total labels shape: {y_full.shape}\")\n",
    "print(f\"Classes present: {np.unique(y_full)}\")\n",
    "print(f\"Class distribution: {np.bincount(y_full)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GA Parameters:\n",
      "Population Size: 40\n",
      "Generations: 10\n",
      "Crossover Probability: 0.8\n",
      "Mutation Probability: 0.1\n",
      "Total Features: 28672\n"
     ]
    }
   ],
   "source": [
    "# --- mRMR + AGWO Feature Selection Pipeline ---\n",
    "\n",
    "# 1. mRMR Feature Ranking (use training set only)\n",
    "n_mrmr = 200  # Number of features to rank with mRMR (adjust as needed)\n",
    "ranked_features = mrmr_feature_selection(X_tr, Y_tr_ohe, n_features=n_mrmr)\n",
    "print(f\"Top {n_mrmr} features ranked by mRMR.\")\n",
    "\n",
    "# 2. AGWO Feature Selection (on ranked features)\n",
    "selected_features = agwo_feature_selection(\n",
    "    X_tr, Y_tr_ohe, ranked_features,\n",
    "    n_wolves=10, n_iter=10, subset_size=40  # adjust as needed\n",
    ")\n",
    "print(f\"Selected {len(selected_features)} features using AGWO.\")\n",
    "\n",
    "# 3. Prepare selected features for training and testing\n",
    "X_sel = X_full[:, selected_features]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_sel, y_full, test_size=0.20, random_state=SEED, stratify=y_full\n",
    ")\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"Training class distribution: {np.bincount(y_train)}\")\n",
    "print(f\"Test class distribution: {np.bincount(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GA components defined successfully!\n"
     ]
    }
   ],
   "source": [
    "# (GA-based feature selection removed; using mRMR + AGWO pipeline instead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GA initialized: pop=40, feats=28672\n",
      "Starting genetic algorithm evolution...\n"
     ]
    }
   ],
   "source": [
    "# (GA-based population initialization removed; using mRMR + AGWO pipeline instead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Evaluating 40 individuals...\n",
      "  Max fitness: 0.9265\n",
      "  Avg fitness: 0.9224\n",
      "\n",
      "Generation 2/10\n",
      "  Evaluating 29 individuals...\n",
      "  Max fitness: 0.9265\n",
      "  Avg fitness: 0.9244\n",
      "\n",
      "Generation 3/10\n",
      "  Evaluating 33 individuals...\n",
      "  Max fitness: 0.9271\n",
      "  Avg fitness: 0.9258\n",
      "\n",
      "Generation 4/10\n",
      "  Evaluating 33 individuals...\n",
      "  Max fitness: 0.9274\n",
      "  Avg fitness: 0.9265\n",
      "\n",
      "Generation 5/10\n",
      "  Evaluating 36 individuals...\n",
      "  Max fitness: 0.9280\n",
      "  Avg fitness: 0.9268\n",
      "\n",
      "Generation 6/10\n",
      "  Evaluating 25 individuals...\n",
      "  Max fitness: 0.9285\n",
      "  Avg fitness: 0.9272\n",
      "\n",
      "Generation 7/10\n",
      "  Evaluating 32 individuals...\n",
      "  Max fitness: 0.9289\n",
      "  Avg fitness: 0.9276\n",
      "\n",
      "Generation 8/10\n",
      "  Evaluating 30 individuals...\n",
      "  Max fitness: 0.9292\n",
      "  Avg fitness: 0.9281\n",
      "\n",
      "Generation 9/10\n",
      "  Evaluating 31 individuals...\n",
      "  Max fitness: 0.9298\n",
      "  Avg fitness: 0.9283\n",
      "\n",
      "Generation 10/10\n",
      "  Evaluating 36 individuals...\n",
      "  Max fitness: 0.9304\n",
      "  Avg fitness: 0.9288\n",
      "\n",
      "GA evolution completed!\n"
     ]
    }
   ],
   "source": [
    "# (GA-based evolution removed; using mRMR + AGWO pipeline instead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 14207 / 28672 features\n",
      "Feature selection ratio: 0.496\n",
      "Best fitness: 0.9304\n"
     ]
    }
   ],
   "source": [
    "# (GA-based best feature selection removed; using mRMR + AGWO pipeline instead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (12000, 14207)\n",
      "Test set shape: (3000, 14207)\n",
      "Training class distribution: [4000 4000 4000]\n",
      "Test class distribution: [1000 1000 1000]\n"
     ]
    }
   ],
   "source": [
    "# (GA-based feature preparation removed; using mRMR + AGWO pipeline instead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifiers initialized:\n",
      "  KNN: k=5, weights='distance'\n",
      "  SVM: RBF kernel, C=1.0, gamma='scale'\n",
      "  Random Forest: 300 trees\n"
     ]
    }
   ],
   "source": [
    "# Initialize Classifiers\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
    "svm = SVC(kernel='rbf', probability=True, C=1.0, gamma='scale', random_state=SEED)\n",
    "rf  = RandomForestClassifier(n_estimators=300, random_state=SEED, n_jobs=-1)\n",
    "xgb = XGBClassifier(n_estimators=200, random_state=SEED, use_label_encoder=False, eval_metric='mlogloss')\n",
    "lr  = LogisticRegression(max_iter=1000, random_state=SEED, n_jobs=-1)\n",
    "\n",
    "print(\"Classifiers initialized:\")\n",
    "print(f\"  KNN: k=5, weights='distance'\")\n",
    "print(f\"  SVM: RBF kernel, C=1.0, gamma='scale'\")\n",
    "print(f\"  Random Forest: 300 trees\")\n",
    "print(f\"  XGBoost: 200 estimators\")\n",
    "print(f\"  Logistic Regression: max_iter=1000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifiers …\n",
      "  Training KNN...\n",
      "  Training SVM...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training Random Forest...\n",
      "All classifiers trained successfully!\n"
     ]
    }
   ],
   "source": [
    "# Train Classifiers\n",
    "print(\"Training classifiers …\")\n",
    "\n",
    "print(\"  Training KNN...\")\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "print(\"  Training SVM...\")\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "print(\"  Training Random Forest...\")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "print(\"  Training XGBoost...\")\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "print(\"  Training Logistic Regression...\")\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "print(\"All classifiers trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions completed!\n"
     ]
    }
   ],
   "source": [
    "# Make Predictions\n",
    "print(\"Making predictions...\")\n",
    "\n",
    "knn_pred = knn.predict(X_test)\n",
    "svm_pred = svm.predict(X_test)\n",
    "rf_pred  = rf.predict(X_test)\n",
    "xgb_pred = xgb.predict(X_test)\n",
    "lr_pred  = lr.predict(X_test)\n",
    "\n",
    "# Probabilistic predictions (for ensemble if needed)\n",
    "knn_proba = knn.predict_proba(X_test) if hasattr(knn, 'predict_proba') else None\n",
    "svm_proba = svm.predict_proba(X_test) if hasattr(svm, 'predict_proba') else None\n",
    "rf_proba  = rf.predict_proba(X_test) if hasattr(rf, 'predict_proba') else None\n",
    "xgb_proba = xgb.predict_proba(X_test) if hasattr(xgb, 'predict_proba') else None\n",
    "lr_proba  = lr.predict_proba(X_test) if hasattr(lr, 'predict_proba') else None\n",
    "\n",
    "print(\"Predictions completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual Classifier Accuracies:\n",
      "  KNN: 0.9843\n",
      "  SVM: 0.9817\n",
      "  RF : 0.9753\n",
      "\n",
      "=== KNN Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    lung_aca       0.99      0.96      0.98      1000\n",
      "      lung_n       1.00      1.00      1.00      1000\n",
      "    lung_scc       0.96      0.99      0.98      1000\n",
      "\n",
      "    accuracy                           0.98      3000\n",
      "   macro avg       0.98      0.98      0.98      3000\n",
      "weighted avg       0.98      0.98      0.98      3000\n",
      "\n",
      "\n",
      "=== SVM Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    lung_aca       0.98      0.96      0.97      1000\n",
      "      lung_n       1.00      1.00      1.00      1000\n",
      "    lung_scc       0.96      0.98      0.97      1000\n",
      "\n",
      "    accuracy                           0.98      3000\n",
      "   macro avg       0.98      0.98      0.98      3000\n",
      "weighted avg       0.98      0.98      0.98      3000\n",
      "\n",
      "\n",
      "=== Random Forest Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    lung_aca       0.97      0.96      0.96      1000\n",
      "      lung_n       1.00      1.00      1.00      1000\n",
      "    lung_scc       0.96      0.97      0.96      1000\n",
      "\n",
      "    accuracy                           0.98      3000\n",
      "   macro avg       0.98      0.98      0.98      3000\n",
      "weighted avg       0.98      0.98      0.98      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Individual Classifier Results\n",
    "print(\"Individual Classifier Accuracies:\")\n",
    "knn_acc = accuracy_score(y_test, knn_pred)\n",
    "svm_acc = accuracy_score(y_test, svm_pred)\n",
    "rf_acc = accuracy_score(y_test, rf_pred)\n",
    "xgb_acc = accuracy_score(y_test, xgb_pred)\n",
    "lr_acc = accuracy_score(y_test, lr_pred)\n",
    "\n",
    "print(f\"  KNN: {knn_acc:.4f}\")\n",
    "print(f\"  SVM: {svm_acc:.4f}\")\n",
    "print(f\"  RF : {rf_acc:.4f}\")\n",
    "print(f\"  XGB: {xgb_acc:.4f}\")\n",
    "print(f\"  LR : {lr_acc:.4f}\")\n",
    "\n",
    "# Display individual classification reports\n",
    "target_names = [id2label[i] for i in range(num_classes)]\n",
    "\n",
    "print(\"\\n=== KNN Classification Report ===\")\n",
    "print(classification_report(y_test, knn_pred, target_names=target_names))\n",
    "\n",
    "print(\"\\n=== SVM Classification Report ===\")\n",
    "print(classification_report(y_test, svm_pred, target_names=target_names))\n",
    "\n",
    "print(\"\\n=== Random Forest Classification Report ===\")\n",
    "print(classification_report(y_test, rf_pred, target_names=target_names))\n",
    "\n",
    "print(\"\\n=== XGBoost Classification Report ===\")\n",
    "print(classification_report(y_test, xgb_pred, target_names=target_names))\n",
    "\n",
    "print(\"\\n=== Logistic Regression Classification Report ===\")\n",
    "print(classification_report(y_test, lr_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Accuracy (Majority Voting): 0.9870\n",
      "\n",
      "Improvement over best individual: 0.0027\n",
      "\n",
      "=== Ensemble Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    lung_aca       0.99      0.97      0.98      1000\n",
      "      lung_n       1.00      1.00      1.00      1000\n",
      "    lung_scc       0.97      0.99      0.98      1000\n",
      "\n",
      "    accuracy                           0.99      3000\n",
      "   macro avg       0.99      0.99      0.99      3000\n",
      "weighted avg       0.99      0.99      0.99      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ensemble Fusion (Priority-Based Strategy)\n",
    "# Priority: SVM > XGBoost > RF > KNN > LR\n",
    "# If SVM and XGBoost agree, use that prediction. Else, use SVM. If not, use XGBoost. Else, fallback to majority vote.\n",
    "def priority_ensemble(svm_pred, xgb_pred, rf_pred, knn_pred, lr_pred):\n",
    "    preds = np.stack([knn_pred, svm_pred, rf_pred, xgb_pred, lr_pred], axis=0)\n",
    "    final = []\n",
    "    for i in range(svm_pred.shape[0]):\n",
    "        if svm_pred[i] == xgb_pred[i]:\n",
    "            final.append(svm_pred[i])\n",
    "        elif svm_pred[i] == rf_pred[i]:\n",
    "            final.append(svm_pred[i])\n",
    "        elif xgb_pred[i] == rf_pred[i]:\n",
    "            final.append(xgb_pred[i])\n",
    "        else:\n",
    "            # fallback to majority vote\n",
    "            vals, counts = np.unique(preds[:, i], return_counts=True)\n",
    "            final.append(vals[np.argmax(counts)])\n",
    "    return np.array(final)\n",
    "\n",
    "ens = priority_ensemble(svm_pred, xgb_pred, rf_pred, knn_pred, lr_pred)\n",
    "ens_acc = accuracy_score(y_test, ens)\n",
    "\n",
    "print(f\"Ensemble Accuracy (Priority-Based): {ens_acc:.4f}\")\n",
    "print(f\"\\nImprovement over best individual: {ens_acc - max(knn_acc, svm_acc, rf_acc, xgb_acc, lr_acc):.4f}\")\n",
    "\n",
    "print(\"\\n=== Ensemble Classification Report ===\")\n",
    "print(classification_report(y_test, ens, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL RESULTS SUMMARY\n",
      "============================================================\n",
      "Total samples processed: 15000\n",
      "Features selected by GA: 14207 / 28672 (49.6%)\n",
      "Test set size: 3000\n",
      "\n",
      "Classifier Accuracies:\n",
      "  KNN:              0.9843\n",
      "  SVM:              0.9817\n",
      "  Random Forest:    0.9753\n",
      "  Ensemble (Fusion): 0.9870 ← BEST\n",
      "\n",
      "Class Labels:\n",
      "  0: lung_aca\n",
      "  1: lung_n\n",
      "  2: lung_scc\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Summary Results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total samples processed: {len(y_full)}\")\n",
    "print(f\"Features selected by GA: {len(sel_idx)} / {n_features} ({len(sel_idx)/n_features:.1%})\")\n",
    "print(f\"Test set size: {len(y_test)}\")\n",
    "print(\"\\nClassifier Accuracies:\")\n",
    "print(f\"  KNN:              {knn_acc:.4f}\")\n",
    "print(f\"  SVM:              {svm_acc:.4f}\")\n",
    "print(f\"  Random Forest:    {rf_acc:.4f}\")\n",
    "print(f\"  Ensemble (Fusion): {ens_acc:.4f} ← BEST\")\n",
    "print(\"\\nClass Labels:\")\n",
    "for i, label in id2label.items():\n",
    "    print(f\"  {i}: {label}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- mRMR + AGWO Feature Selection Pipeline ---\n",
    "\n",
    "# 1. mRMR Feature Ranking (use training set only)\n",
    "n_mrmr = 200  # Number of features to rank with mRMR (adjust as needed)\n",
    "ranked_features = mrmr_feature_selection(X_tr, Y_tr_ohe, n_features=n_mrmr)\n",
    "print(f\"Top {n_mrmr} features ranked by mRMR.\")\n",
    "\n",
    "# 2. AGWO Feature Selection (on ranked features)\n",
    "selected_features = agwo_feature_selection(\n",
    "    X_tr, Y_tr_ohe, ranked_features,\n",
    "    n_wolves=10, n_iter=10, subset_size=40  # adjust as needed\n",
    ")\n",
    "print(f\"Selected {len(selected_features)} features using AGWO.\")\n",
    "\n",
    "# 3. Prepare selected features for training and testing\n",
    "X_sel = X_full[:, selected_features]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_sel, y_full, test_size=0.20, random_state=SEED, stratify=y_full\n",
    ")\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"Training class distribution: {np.bincount(y_train)}\")\n",
    "print(f\"Test class distribution: {np.bincount(y_test)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

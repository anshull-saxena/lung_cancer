{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lung Histopathology Classification: ACA / N / SCC\n",
    "## Multi-CNN + Channel Attention + GA + KNN/SVM/RF + Fusion\n",
    "\n",
    "This notebook implements a comprehensive lung histopathology classification system that combines:\n",
    "- Multiple CNN backbones (DenseNet121, ResNet50, VGG16)\n",
    "- Channel attention mechanism (SE blocks)\n",
    "- Genetic Algorithm for feature selection\n",
    "- Ensemble of classical ML classifiers (KNN, SVM, Random Forest)\n",
    "- Majority voting fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.25.2\n",
      "  Using cached numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting tensorflow==2.15.0\n",
      "  Using cached tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
      "Collecting keras==2.15.0\n",
      "  Using cached keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow==2.15.0)\n",
      "  Using cached absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow==2.15.0)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow==2.15.0)\n",
      "  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow==2.15.0)\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow==2.15.0)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=2.9.0 (from tensorflow==2.15.0)\n",
      "  Using cached h5py-3.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow==2.15.0)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting ml-dtypes~=0.2.0 (from tensorflow==2.15.0)\n",
      "  Using cached ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow==2.15.0)\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting packaging (from tensorflow==2.15.0)\n",
      "  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.15.0)\n",
      "  Using cached protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Collecting setuptools (from tensorflow==2.15.0)\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting six>=1.12.0 (from tensorflow==2.15.0)\n",
      "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow==2.15.0)\n",
      "  Using cached termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting typing-extensions>=3.6.6 (from tensorflow==2.15.0)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.15.0)\n",
      "  Using cached wrapt-1.14.2-cp310-cp310-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow==2.15.0)\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow==2.15.0)\n",
      "  Using cached grpcio-1.74.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.0)\n",
      "  Using cached tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
      "  Using cached tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Using cached google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting google-auth-oauthlib<2,>=0.5 (from tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Using cached google_auth_oauthlib-1.2.2-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Using cached markdown-3.9-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting requests<3,>=2.21.0 (from tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Using cached cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Using cached pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Using cached rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Using cached charset_normalizer-3.4.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Using cached certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow==2.15.0)\n",
      "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Using cached oauthlib-3.3.1-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Using cached MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Using cached numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Using cached tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
      "Using cached keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "Using cached grpcio-1.74.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
      "Using cached ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "Using cached protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "Using cached tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
      "Using cached google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
      "Using cached cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Using cached google_auth_oauthlib-1.2.2-py3-none-any.whl (19 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Using cached charset_normalizer-3.4.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (152 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "Using cached tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached wrapt-1.14.2-cp310-cp310-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (76 kB)\n",
      "Using cached absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Using cached certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "Using cached flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached h5py-3.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "Using cached markdown-3.9-py3-none-any.whl (107 kB)\n",
      "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Using cached pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached oauthlib-3.3.1-py3-none-any.whl (160 kB)\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached tensorflow_io_gcs_filesystem-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "Using cached termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\n",
      "Using cached packaging-25.0-py3-none-any.whl (66 kB)\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, wheel, urllib3, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, six, setuptools, pyasn1, protobuf, packaging, opt-einsum, oauthlib, numpy, MarkupSafe, markdown, keras, idna, grpcio, gast, charset_normalizer, certifi, cachetools, absl-py, werkzeug, rsa, requests, pyasn1-modules, ml-dtypes, h5py, google-pasta, astunparse, requests-oauthlib, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "\u001b[2K  Attempting uninstall: libclang\n",
      "\u001b[2K    Found existing installation: libclang 18.1.1\n",
      "\u001b[2K    Uninstalling libclang-18.1.1:\n",
      "\u001b[2K      Successfully uninstalled libclang-18.1.1\n",
      "\u001b[2K  Attempting uninstall: flatbuffers━━━━━━━━━━━━━\u001b[0m \u001b[32m 0/41\u001b[0m [libclang]\n",
      "\u001b[2K    Found existing installation: flatbuffers 25.2.102m 0/41\u001b[0m [libclang]\n",
      "\u001b[2K    Uninstalling flatbuffers-25.2.10:━━━━━━━\u001b[0m \u001b[32m 0/41\u001b[0m [libclang]\n",
      "\u001b[2K      Successfully uninstalled flatbuffers-25.2.10[32m 0/41\u001b[0m [libclang]\n",
      "\u001b[2K  Attempting uninstall: wrapt━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 0/41\u001b[0m [libclang]\n",
      "\u001b[2K    Found existing installation: wrapt 1.14.2[0m \u001b[32m 0/41\u001b[0m [libclang]\n",
      "\u001b[2K    Uninstalling wrapt-1.14.2:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/41\u001b[0m [wrapt]\n",
      "\u001b[2K      Successfully uninstalled wrapt-1.14.2━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/41\u001b[0m [wrapt]\n",
      "\u001b[2K  Attempting uninstall: wheel━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/41\u001b[0m [wrapt]\n",
      "\u001b[2K    Found existing installation: wheel 0.45.1━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/41\u001b[0m [wrapt]\n",
      "\u001b[2K    Uninstalling wheel-0.45.1:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/41\u001b[0m [wrapt]\n",
      "\u001b[2K      Successfully uninstalled wheel-0.45.1━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/41\u001b[0m [wrapt]\n",
      "\u001b[2K  Attempting uninstall: urllib3━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/41\u001b[0m [wheel]\n",
      "\u001b[2K    Found existing installation: urllib3 2.5.0━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/41\u001b[0m [wheel]\n",
      "\u001b[2K    Uninstalling urllib3-2.5.0:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/41\u001b[0m [wheel]\n",
      "\u001b[2K      Successfully uninstalled urllib3-2.5.0━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/41\u001b[0m [wheel]\n",
      "\u001b[2K  Attempting uninstall: typing-extensions━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/41\u001b[0m [urllib3]\n",
      "\u001b[2K    Found existing installation: typing_extensions 4.15.0━━━━━\u001b[0m \u001b[32m 4/41\u001b[0m [urllib3]\n",
      "\u001b[2K    Uninstalling typing_extensions-4.15.0:━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/41\u001b[0m [urllib3]\n",
      "\u001b[2K      Successfully uninstalled typing_extensions-4.15.0━━━━━━━━━━━\u001b[0m \u001b[32m 5/41\u001b[0m [typing-extensions]\n",
      "\u001b[2K  Attempting uninstall: termcolor━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/41\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Found existing installation: termcolor 3.1.0━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/41\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Uninstalling termcolor-3.1.0:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/41\u001b[0m [typing-extensions]\n",
      "\u001b[2K      Successfully uninstalled termcolor-3.1.0━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/41\u001b[0m [typing-extensions]\n",
      "\u001b[2K  Attempting uninstall: tensorflow-io-gcs-filesystem━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/41\u001b[0m [termcolor]ons]\n",
      "\u001b[2K    Found existing installation: tensorflow-io-gcs-filesystem 0.37.1[32m 6/41\u001b[0m [termcolor]\n",
      "\u001b[2K    Uninstalling tensorflow-io-gcs-filesystem-0.37.1:━━━━━━━━━\u001b[0m \u001b[32m 6/41\u001b[0m [termcolor]\n",
      "\u001b[2K      Successfully uninstalled tensorflow-io-gcs-filesystem-0.37.1 \u001b[32m 6/41\u001b[0m [termcolor]\n",
      "\u001b[2K  Attempting uninstall: tensorflow-estimator━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/41\u001b[0m [termcolor]\n",
      "\u001b[2K    Found existing installation: tensorflow-estimator 2.15.0━━\u001b[0m \u001b[32m 6/41\u001b[0m [termcolor]\n",
      "\u001b[2K    Uninstalling tensorflow-estimator-2.15.0:━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/41\u001b[0m [tensorflow-estimator]\n",
      "\u001b[2K      Successfully uninstalled tensorflow-estimator-2.15.0━━━━\u001b[0m \u001b[32m 8/41\u001b[0m [tensorflow-estimator]\n",
      "\u001b[2K  Attempting uninstall: tensorboard-data-server━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/41\u001b[0m [tensorflow-estimator]\n",
      "\u001b[2K    Found existing installation: tensorboard-data-server 0.7.2\u001b[0m \u001b[32m 8/41\u001b[0m [tensorflow-estimator]\n",
      "\u001b[2K    Uninstalling tensorboard-data-server-0.7.2:━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/41\u001b[0m [tensorflow-estimator]\n",
      "\u001b[2K      Successfully uninstalled tensorboard-data-server-0.7.2━━\u001b[0m \u001b[32m 8/41\u001b[0m [tensorflow-estimator]\n",
      "\u001b[2K  Attempting uninstall: six[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/41\u001b[0m [tensorboard-data-server]\n",
      "\u001b[2K    Found existing installation: six 1.17.0━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/41\u001b[0m [tensorboard-data-server]\n",
      "\u001b[2K    Uninstalling six-1.17.0:90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/41\u001b[0m [tensorboard-data-server]\n",
      "\u001b[2K      Successfully uninstalled six-1.17.0━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/41\u001b[0m [six]ard-data-server]\n",
      "\u001b[2K  Attempting uninstall: setuptools━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/41\u001b[0m [six]\n",
      "\u001b[2K    Found existing installation: setuptools 80.9.0━━━━━━━━━━━━\u001b[0m \u001b[32m10/41\u001b[0m [six]\n",
      "\u001b[2K    Uninstalling setuptools-80.9.0:━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/41\u001b[0m [six]\n",
      "\u001b[2K      Successfully uninstalled setuptools-80.9.0━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/41\u001b[0m [setuptools]\n",
      "\u001b[2K  Attempting uninstall: pyasn10m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/41\u001b[0m [setuptools]\n",
      "\u001b[2K    Found existing installation: pyasn1 0.6.1━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/41\u001b[0m [setuptools]\n",
      "\u001b[2K    Uninstalling pyasn1-0.6.1:90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/41\u001b[0m [setuptools]\n",
      "\u001b[2K      Successfully uninstalled pyasn1-0.6.1━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/41\u001b[0m [setuptools]\n",
      "\u001b[2K  Attempting uninstall: protobufm\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/41\u001b[0m [pyasn1]\n",
      "\u001b[2K    Found existing installation: protobuf 4.25.8━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/41\u001b[0m [pyasn1]\n",
      "\u001b[2K    Uninstalling protobuf-4.25.8:m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/41\u001b[0m [pyasn1]\n",
      "\u001b[2K      Successfully uninstalled protobuf-4.25.8━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/41\u001b[0m [protobuf]\n",
      "\u001b[2K  Attempting uninstall: packagingm\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/41\u001b[0m [protobuf]\n",
      "\u001b[2K    Found existing installation: packaging 25.0━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/41\u001b[0m [protobuf]\n",
      "\u001b[2K    Uninstalling packaging-25.0:90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/41\u001b[0m [protobuf]\n",
      "\u001b[2K      Successfully uninstalled packaging-25.0━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/41\u001b[0m [protobuf]\n",
      "\u001b[2K  Attempting uninstall: opt-einsumm\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/41\u001b[0m [packaging]\n",
      "\u001b[2K    Found existing installation: opt_einsum 3.4.0━━━━━━━━━━━━━\u001b[0m \u001b[32m14/41\u001b[0m [packaging]\n",
      "\u001b[2K    Uninstalling opt_einsum-3.4.0:0m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/41\u001b[0m [packaging]\n",
      "\u001b[2K      Successfully uninstalled opt_einsum-3.4.0━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/41\u001b[0m [packaging]\n",
      "\u001b[2K  Attempting uninstall: oauthlib[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/41\u001b[0m [packaging]\n",
      "\u001b[2K    Found existing installation: oauthlib 3.3.1━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/41\u001b[0m [packaging]\n",
      "\u001b[2K    Uninstalling oauthlib-3.3.1:╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/41\u001b[0m [oauthlib]\n",
      "\u001b[2K      Successfully uninstalled oauthlib-3.3.1━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/41\u001b[0m [oauthlib]\n",
      "\u001b[2K  Attempting uninstall: numpy91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/41\u001b[0m [oauthlib]\n",
      "\u001b[2K    Found existing installation: numpy 1.25.2━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/41\u001b[0m [oauthlib]\n",
      "\u001b[2K    Uninstalling numpy-1.25.2:[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/41\u001b[0m [oauthlib]\n",
      "\u001b[2K      Successfully uninstalled numpy-1.25.2━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/41\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: MarkupSafe\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/41\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: MarkupSafe 3.0.2━━━━━━━━━━━━━\u001b[0m \u001b[32m17/41\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling MarkupSafe-3.0.2:\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/41\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled MarkupSafe-3.0.2━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/41\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: markdown1m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18/41\u001b[0m [MarkupSafe]\n",
      "\u001b[2K    Found existing installation: Markdown 3.9━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18/41\u001b[0m [MarkupSafe]\n",
      "\u001b[2K    Uninstalling Markdown-3.9:╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18/41\u001b[0m [MarkupSafe]\n",
      "\u001b[2K      Successfully uninstalled Markdown-3.9━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18/41\u001b[0m [MarkupSafe]\n",
      "\u001b[2K  Attempting uninstall: kerasm╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18/41\u001b[0m [MarkupSafe]\n",
      "\u001b[2K    Found existing installation: keras 2.15.0━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18/41\u001b[0m [MarkupSafe]\n",
      "\u001b[2K    Uninstalling keras-2.15.0:m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/41\u001b[0m [keras]]\n",
      "\u001b[2K      Successfully uninstalled keras-2.15.0━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/41\u001b[0m [keras]\n",
      "\u001b[2K  Attempting uninstall: idna[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/41\u001b[0m [keras]\n",
      "\u001b[2K    Found existing installation: idna 3.10━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/41\u001b[0m [keras]\n",
      "\u001b[2K    Uninstalling idna-3.10:\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/41\u001b[0m [keras]\n",
      "\u001b[2K      Successfully uninstalled idna-3.100m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21/41\u001b[0m [idna]\n",
      "\u001b[2K  Attempting uninstall: grpcio90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21/41\u001b[0m [idna]\n",
      "\u001b[2K    Found existing installation: grpcio 1.74.0━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21/41\u001b[0m [idna]\n",
      "\u001b[2K    Uninstalling grpcio-1.74.0:0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/41\u001b[0m [grpcio]\n",
      "\u001b[2K      Successfully uninstalled grpcio-1.74.0━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/41\u001b[0m [grpcio]\n",
      "\u001b[2K  Attempting uninstall: gast━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/41\u001b[0m [grpcio]\n",
      "\u001b[2K    Found existing installation: gast 0.6.0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23/41\u001b[0m [gast]\n",
      "\u001b[2K    Uninstalling gast-0.6.0:0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23/41\u001b[0m [gast]\n",
      "\u001b[2K      Successfully uninstalled gast-0.6.0[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23/41\u001b[0m [gast]\n",
      "\u001b[2K  Attempting uninstall: charset_normalizer90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23/41\u001b[0m [gast]\n",
      "\u001b[2K    Found existing installation: charset-normalizer 3.4.3━━━━━\u001b[0m \u001b[32m23/41\u001b[0m [gast]\n",
      "\u001b[2K    Uninstalling charset-normalizer-3.4.3:90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23/41\u001b[0m [gast]\n",
      "\u001b[2K      Successfully uninstalled charset-normalizer-3.4.3━━━━━━━\u001b[0m \u001b[32m23/41\u001b[0m [gast]\n",
      "\u001b[2K  Attempting uninstall: certifi[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23/41\u001b[0m [gast]\n",
      "\u001b[2K    Found existing installation: certifi 2025.8.3━━━━━━━━━━━━━\u001b[0m \u001b[32m23/41\u001b[0m [gast]\n",
      "\u001b[2K    Uninstalling certifi-2025.8.3:m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23/41\u001b[0m [gast]\n",
      "\u001b[2K      Successfully uninstalled certifi-2025.8.3━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23/41\u001b[0m [gast]\n",
      "\u001b[2K  Attempting uninstall: cachetools0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25/41\u001b[0m [certifi]\n",
      "\u001b[2K    Found existing installation: cachetools 5.5.2━━━━━━━━━━━━━\u001b[0m \u001b[32m25/41\u001b[0m [certifi]\n",
      "\u001b[2K    Uninstalling cachetools-5.5.2:90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25/41\u001b[0m [certifi]\n",
      "\u001b[2K      Successfully uninstalled cachetools-5.5.2━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25/41\u001b[0m [certifi]\n",
      "\u001b[2K  Attempting uninstall: absl-pym\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25/41\u001b[0m [certifi]\n",
      "\u001b[2K    Found existing installation: absl-py 2.3.1m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25/41\u001b[0m [certifi]\n",
      "\u001b[2K    Uninstalling absl-py-2.3.1:m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25/41\u001b[0m [certifi]\n",
      "\u001b[2K      Successfully uninstalled absl-py-2.3.190m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25/41\u001b[0m [certifi]\n",
      "\u001b[2K  Attempting uninstall: werkzeug━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m27/41\u001b[0m [absl-py]\n",
      "\u001b[2K    Found existing installation: Werkzeug 3.1.30m━━━━━━━━━━━━━\u001b[0m \u001b[32m27/41\u001b[0m [absl-py]\n",
      "\u001b[2K    Uninstalling Werkzeug-3.1.3:0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m27/41\u001b[0m [absl-py]\n",
      "\u001b[2K      Successfully uninstalled Werkzeug-3.1.3[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m27/41\u001b[0m [absl-py]\n",
      "\u001b[2K  Attempting uninstall: rsa━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m28/41\u001b[0m [werkzeug]\n",
      "\u001b[2K    Found existing installation: rsa 4.9.1[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m28/41\u001b[0m [werkzeug]\n",
      "\u001b[2K    Uninstalling rsa-4.9.1:━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m28/41\u001b[0m [werkzeug]\n",
      "\u001b[2K      Successfully uninstalled rsa-4.9.1╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m28/41\u001b[0m [werkzeug]\n",
      "\u001b[2K  Attempting uninstall: requests[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m28/41\u001b[0m [werkzeug]\n",
      "\u001b[2K    Found existing installation: requests 2.32.50m━━━━━━━━━━━━\u001b[0m \u001b[32m28/41\u001b[0m [werkzeug]\n",
      "\u001b[2K    Uninstalling requests-2.32.5:0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m28/41\u001b[0m [werkzeug]\n",
      "\u001b[2K      Successfully uninstalled requests-2.32.5[90m━━━━━━━━━━━━\u001b[0m \u001b[32m28/41\u001b[0m [werkzeug]\n",
      "\u001b[2K  Attempting uninstall: pyasn1-modules[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m30/41\u001b[0m [requests]\n",
      "\u001b[2K    Found existing installation: pyasn1_modules 0.4.2━━━━━━━━━\u001b[0m \u001b[32m30/41\u001b[0m [requests]\n",
      "\u001b[2K    Uninstalling pyasn1_modules-0.4.2:[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m30/41\u001b[0m [requests]\n",
      "\u001b[2K      Successfully uninstalled pyasn1_modules-0.4.2m━━━━━━━━━━\u001b[0m \u001b[32m30/41\u001b[0m [requests]\n",
      "\u001b[2K  Attempting uninstall: ml-dtypes━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m31/41\u001b[0m [pyasn1-modules]\n",
      "\u001b[2K    Found existing installation: ml-dtypes 0.2.0\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m31/41\u001b[0m [pyasn1-modules]\n",
      "\u001b[2K    Uninstalling ml-dtypes-0.2.0:━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m31/41\u001b[0m [pyasn1-modules]\n",
      "\u001b[2K      Successfully uninstalled ml-dtypes-0.2.00m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m31/41\u001b[0m [pyasn1-modules]\n",
      "\u001b[2K  Attempting uninstall: h5py━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m32/41\u001b[0m [ml-dtypes]]\n",
      "\u001b[2K    Found existing installation: h5py 3.14.0╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m32/41\u001b[0m [ml-dtypes]\n",
      "\u001b[2K    Uninstalling h5py-3.14.0:━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m32/41\u001b[0m [ml-dtypes]\n",
      "\u001b[2K      Successfully uninstalled h5py-3.14.00m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m32/41\u001b[0m [ml-dtypes]\n",
      "\u001b[2K  Attempting uninstall: google-pasta━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m33/41\u001b[0m [h5py]]\n",
      "\u001b[2K    Found existing installation: google-pasta 0.2.0[90m━━━━━━━\u001b[0m \u001b[32m33/41\u001b[0m [h5py]\n",
      "\u001b[2K    Uninstalling google-pasta-0.2.0:━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m34/41\u001b[0m [google-pasta]\n",
      "\u001b[2K      Successfully uninstalled google-pasta-0.2.00m\u001b[90m━━━━━━\u001b[0m \u001b[32m34/41\u001b[0m [google-pasta]\n",
      "\u001b[2K  Attempting uninstall: astunparse━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m34/41\u001b[0m [google-pasta]\n",
      "\u001b[2K    Found existing installation: astunparse 1.6.30m\u001b[90m━━━━━━\u001b[0m \u001b[32m34/41\u001b[0m [google-pasta]\n",
      "\u001b[2K    Uninstalling astunparse-1.6.3:━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m34/41\u001b[0m [google-pasta]\n",
      "\u001b[2K      Successfully uninstalled astunparse-1.6.3\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m34/41\u001b[0m [google-pasta]\n",
      "\u001b[2K  Attempting uninstall: requests-oauthlib\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m34/41\u001b[0m [google-pasta]\n",
      "\u001b[2K    Found existing installation: requests-oauthlib 2.0.0━━━━━━\u001b[0m \u001b[32m34/41\u001b[0m [google-pasta]\n",
      "\u001b[2K    Uninstalling requests-oauthlib-2.0.0:\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m34/41\u001b[0m [google-pasta]\n",
      "\u001b[2K      Successfully uninstalled requests-oauthlib-2.0.00m━━━━━━\u001b[0m \u001b[32m34/41\u001b[0m [google-pasta]\n",
      "\u001b[2K  Attempting uninstall: google-auth━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m36/41\u001b[0m [requests-oauthlib]\n",
      "\u001b[2K    Found existing installation: google-auth 2.40.30m\u001b[90m━━━━\u001b[0m \u001b[32m36/41\u001b[0m [requests-oauthlib]\n",
      "\u001b[2K    Uninstalling google-auth-2.40.3:━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m36/41\u001b[0m [requests-oauthlib]\n",
      "\u001b[2K      Successfully uninstalled google-auth-2.40.3\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m36/41\u001b[0m [requests-oauthlib]\n",
      "\u001b[2K  Attempting uninstall: google-auth-oauthlib\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m37/41\u001b[0m [google-auth]b]\n",
      "\u001b[2K    Found existing installation: google-auth-oauthlib 1.2.2━━━\u001b[0m \u001b[32m37/41\u001b[0m [google-auth]\n",
      "\u001b[2K    Uninstalling google-auth-oauthlib-1.2.2:\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m37/41\u001b[0m [google-auth]\n",
      "\u001b[2K      Successfully uninstalled google-auth-oauthlib-1.2.20m━━━\u001b[0m \u001b[32m37/41\u001b[0m [google-auth]\n",
      "\u001b[2K  Attempting uninstall: tensorboard━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m37/41\u001b[0m [google-auth]\n",
      "\u001b[2K    Found existing installation: tensorboard 2.15.2[0m\u001b[90m━━━\u001b[0m \u001b[32m37/41\u001b[0m [google-auth]\n",
      "\u001b[2K    Uninstalling tensorboard-2.15.2:━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m39/41\u001b[0m [tensorboard]\n",
      "\u001b[2K      Successfully uninstalled tensorboard-2.15.20m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m39/41\u001b[0m [tensorboard]\n",
      "\u001b[2K  Attempting uninstall: tensorflow━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m39/41\u001b[0m [tensorboard]\n",
      "\u001b[2K    Found existing installation: tensorflow 2.15.0m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m39/41\u001b[0m [tensorboard]\n",
      "\u001b[2K    Uninstalling tensorflow-2.15.0:━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m40/41\u001b[0m [tensorflow]board]\n",
      "\u001b[2K      Successfully uninstalled tensorflow-2.15.0[0m\u001b[90m╺\u001b[0m \u001b[32m40/41\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41/41\u001b[0m [tensorflow]nsorflow]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.2 absl-py-2.3.1 astunparse-1.6.3 cachetools-5.5.2 certifi-2025.8.3 charset_normalizer-3.4.3 flatbuffers-25.2.10 gast-0.6.0 google-auth-2.40.3 google-auth-oauthlib-1.2.2 google-pasta-0.2.0 grpcio-1.74.0 h5py-3.14.0 idna-3.10 keras-2.15.0 libclang-18.1.1 markdown-3.9 ml-dtypes-0.2.0 numpy-1.25.2 oauthlib-3.3.1 opt-einsum-3.4.0 packaging-25.0 protobuf-4.25.8 pyasn1-0.6.1 pyasn1-modules-0.4.2 requests-2.32.5 requests-oauthlib-2.0.0 rsa-4.9.1 setuptools-80.9.0 six-1.17.0 tensorboard-2.15.2 tensorboard-data-server-0.7.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0 tensorflow-io-gcs-filesystem-0.37.1 termcolor-3.1.0 typing-extensions-4.15.0 urllib3-2.5.0 werkzeug-3.1.3 wheel-0.45.1 wrapt-1.14.2\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --force-reinstall numpy==1.25.2 tensorflow==2.15.0 keras==2.15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-11 08:35:08.737117: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-11 08:35:08.976308: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-09-11 08:35:08.976668: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-09-11 08:35:09.019077: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-09-11 08:35:09.120106: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-11 08:35:10.497316: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os, random, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.applications import DenseNet121, ResNet50, EfficientNetB0, InceptionV3\n",
    "from tensorflow.keras.applications.densenet import preprocess_input as pre_densenet\n",
    "from tensorflow.keras.applications.resnet import preprocess_input as pre_resnet\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input as pre_efficientnet\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input as pre_inception\n",
    "from tensorflow.keras.layers import (Input, GlobalAveragePooling2D, GlobalMaxPooling2D,\n",
    "                                     Concatenate, Dense, Reshape, Multiply, Lambda)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from scipy.stats import mode\n",
    "\n",
    "# from deap import base, creator, tools  # GA removed, not needed\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration set:\n",
      "Data Directory: /teamspace/studios/this_studio/lung_cancer/dataset/lung_image_sets\n",
      "Image Size: (224, 224)\n",
      "Batch Size: 24\n",
      "Random Seed: 42\n"
     ]
    }
   ],
   "source": [
    "# Configuration and Data Setup\n",
    "DATA_DIR   = \"/teamspace/studios/this_studio/lung_cancer/dataset/lung_image_sets\"  # << set this\n",
    "IMG_SIZE   = (224, 224)\n",
    "BATCH_SIZE = 24\n",
    "SEED       = 42\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "print(f\"Configuration set:\")\n",
    "print(f\"Data Directory: {DATA_DIR}\")\n",
    "print(f\"Image Size: {IMG_SIZE}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Random Seed: {SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of attention heads for multi-head channel attention\n",
    "NUM_ATTENTION_HEADS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12000 images belonging to 3 classes.\n",
      "Found 3000 images belonging to 3 classes.\n",
      "Classes: {'lung_aca': 0, 'lung_n': 1, 'lung_scc': 2}\n",
      "Number of classes: 3\n",
      "Training samples: 12000\n",
      "Validation samples: 3000\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    validation_split=0.20,\n",
    "    rotation_range=20,\n",
    "    horizontal_flip=True,\n",
    "    # IMPORTANT: no rescale here, since we feed raw to model-specific preprocessors\n",
    ")\n",
    "\n",
    "def make_gen(subset):\n",
    "    return train_datagen.flow_from_directory(\n",
    "        DATA_DIR,\n",
    "        target_size=IMG_SIZE,\n",
    "        class_mode='categorical',\n",
    "        batch_size=BATCH_SIZE,\n",
    "        subset=subset,\n",
    "        seed=SEED,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "train_gen = make_gen('training')\n",
    "val_gen   = make_gen('validation')\n",
    "num_classes = train_gen.num_classes\n",
    "class_indices = train_gen.class_indices\n",
    "id2label = {v:k for k,v in class_indices.items()}\n",
    "\n",
    "print(\"Classes:\", class_indices)\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Training samples: {train_gen.samples}\")\n",
    "print(f\"Validation samples: {val_gen.samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-head attention block function defined successfully!\n"
     ]
    }
   ],
   "source": [
    "# Channel Attention (Multi-Headed) Implementation\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Dense, Reshape, Permute, Concatenate\n",
    "\n",
    "class MultiHeadChannelAttention(Layer):\n",
    "    def __init__(self, num_heads=4, reduction=16, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.channel = input_shape[-1]\n",
    "        self.dense1 = [Dense(self.channel // self.reduction, activation='relu') for _ in range(self.num_heads)]\n",
    "        self.dense2 = [Dense(self.channel) for _ in range(self.num_heads)]\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        # Global pooling\n",
    "        gap = tf.reduce_mean(x, axis=[1,2])  # shape: (batch, channels)\n",
    "        gmp = tf.reduce_max(x, axis=[1,2])   # shape: (batch, channels)\n",
    "        heads = []\n",
    "        for i in range(self.num_heads):\n",
    "            d1_gap = self.dense1[i](gap)\n",
    "            d1_gmp = self.dense1[i](gmp)\n",
    "            d2_gap = self.dense2[i](d1_gap)\n",
    "            d2_gmp = self.dense2[i](d1_gmp)\n",
    "            scale = tf.nn.sigmoid(d2_gap + d2_gmp)\n",
    "            scale = Reshape((1,1,self.channel))(scale)\n",
    "            heads.append(x * scale)\n",
    "        # Concatenate heads along channel axis\n",
    "        out = Concatenate(axis=-1)(heads)\n",
    "        return out\n",
    "\n",
    "# Usage in your lane function:\n",
    "# from multi_head_attention import MultiHeadChannelAttention\n",
    "# x = MultiHeadChannelAttention(num_heads=4, reduction=16)(x)\n",
    "\n",
    "def multi_head_attention_block(x, reduction=16, name=None):\n",
    "    \"\"\"Multi-Headed Channel Attention block for CNN feature maps\"\"\"\n",
    "    attn = MultiHeadChannelAttention(num_heads=NUM_ATTENTION_HEADS, reduction=reduction, name=name)(x)\n",
    "    return attn\n",
    "\n",
    "print(\"Multi-head attention block function defined successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lane function updated for multi-head attention!\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing Lanes (one per backbone)\n",
    "def lane(tensor, backbone=\"resnet\", reduction=16):\n",
    "    \"\"\"Create a processing lane for each CNN backbone with multi-head channel attention\"\"\"\n",
    "    if backbone == \"resnet\":\n",
    "        x = Lambda(pre_resnet, name=\"pre_resnet\")(tensor)\n",
    "        x = ResNet50(include_top=False, weights='imagenet')(x)\n",
    "    elif backbone == \"densenet\":\n",
    "        x = Lambda(pre_densenet, name=\"pre_densenet\")(tensor)\n",
    "        x = DenseNet121(include_top=False, weights='imagenet')(x)\n",
    "    elif backbone == \"efficientnet\":\n",
    "        x = Lambda(pre_efficientnet, name=\"pre_efficientnet\")(tensor)\n",
    "        x = EfficientNetB0(include_top=False, weights='imagenet')(x)\n",
    "    elif backbone == \"inception\":\n",
    "        x = Lambda(pre_inception, name=\"pre_inception\")(tensor)\n",
    "        x = InceptionV3(include_top=False, weights='imagenet')(x)\n",
    "    else:\n",
    "        raise ValueError(f'Unknown backbone: {backbone}')\n",
    "    # Add multi-head channel attention\n",
    "    x = multi_head_attention_block(x, reduction=reduction, name=f\"mhca_{backbone}\")\n",
    "    # Global Average Pooling to convert feature maps → vector\n",
    "    x = GlobalAveragePooling2D(name=f\"gap_{backbone}\")(x)\n",
    "    return x\n",
    "\n",
    "print(\"Lane function updated for multi-head attention!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building multi-backbone feature concatenator with multi-head attention...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-11 08:35:15.284909: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20763 MB memory:  -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:04.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extractor built successfully!\n",
      "Feature dimension: 51200\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " pre_densenet (Lambda)       (None, 224, 224, 3)          0         ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " pre_resnet (Lambda)         (None, 224, 224, 3)          0         ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " pre_efficientnet (Lambda)   (None, 224, 224, 3)          0         ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " pre_inception (Lambda)      (None, 224, 224, 3)          0         ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " densenet121 (Functional)    (None, None, None, 1024)     7037504   ['pre_densenet[0][0]']        \n",
      "                                                                                                  \n",
      " resnet50 (Functional)       (None, None, None, 2048)     2358771   ['pre_resnet[0][0]']          \n",
      "                                                          2                                       \n",
      "                                                                                                  \n",
      " efficientnetb0 (Functional  (None, None, None, 1280)     4049571   ['pre_efficientnet[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " inception_v3 (Functional)   (None, None, None, 2048)     2180278   ['pre_inception[0][0]']       \n",
      "                                                          4                                       \n",
      "                                                                                                  \n",
      " mhca_densenet (MultiHeadCh  (None, 7, 7, 8192)           1057280   ['densenet121[0][0]']         \n",
      " annelAttention)                                                                                  \n",
      "                                                                                                  \n",
      " mhca_resnet (MultiHeadChan  (None, 7, 7, 16384)          4211712   ['resnet50[0][0]']            \n",
      " nelAttention)                                                                                    \n",
      "                                                                                                  \n",
      " mhca_efficientnet (MultiHe  (None, 7, 7, 10240)          1649280   ['efficientnetb0[0][0]']      \n",
      " adChannelAttention)                                                                              \n",
      "                                                                                                  \n",
      " mhca_inception (MultiHeadC  (None, 5, 5, 16384)          4211712   ['inception_v3[0][0]']        \n",
      " hannelAttention)                                                                                 \n",
      "                                                                                                  \n",
      " gap_densenet (GlobalAverag  (None, 8192)                 0         ['mhca_densenet[0][0]']       \n",
      " ePooling2D)                                                                                      \n",
      "                                                                                                  \n",
      " gap_resnet (GlobalAverageP  (None, 16384)                0         ['mhca_resnet[0][0]']         \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " gap_efficientnet (GlobalAv  (None, 10240)                0         ['mhca_efficientnet[0][0]']   \n",
      " eragePooling2D)                                                                                  \n",
      "                                                                                                  \n",
      " gap_inception (GlobalAvera  (None, 16384)                0         ['mhca_inception[0][0]']      \n",
      " gePooling2D)                                                                                     \n",
      "                                                                                                  \n",
      " concat_feats (Concatenate)  (None, 51200)                0         ['gap_densenet[0][0]',        \n",
      "                                                                     'gap_resnet[0][0]',          \n",
      "                                                                     'gap_efficientnet[0][0]',    \n",
      "                                                                     'gap_inception[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 67607555 (257.90 MB)\n",
      "Trainable params: 67394332 (257.09 MB)\n",
      "Non-trainable params: 213223 (832.91 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build Feature Extractor Model\n",
    "print(\"Building multi-backbone feature concatenator with multi-head attention...\")\n",
    "\n",
    "# Define input tensor with image size (224x224x3 RGB)\n",
    "inp = Input(shape=(224,224,3))\n",
    "\n",
    "# Extract features from DenseNet lane (multi-head attention)\n",
    "feat_d = lane(inp, \"densenet\", reduction=16)\n",
    "# Extract features from ResNet lane (multi-head attention)\n",
    "feat_r = lane(inp, \"resnet\", reduction=16)\n",
    "# Extract features from EfficientNetB0 lane (multi-head attention)\n",
    "feat_e = lane(inp, \"efficientnet\", reduction=16)\n",
    "# Extract features from InceptionV3 lane (multi-head attention)\n",
    "feat_i = lane(inp, \"inception\", reduction=16)\n",
    "\n",
    "# Concatenate features from all four backbones\n",
    "concat_feat = Concatenate(name=\"concat_feats\")([feat_d, feat_r, feat_e, feat_i])\n",
    "\n",
    "# Create feature extractor model (input → concatenated features)\n",
    "feature_model = Model(inp, concat_feat)\n",
    "\n",
    "# Get final concatenated feature dimension\n",
    "feature_dim = feature_model.output_shape[-1]\n",
    "\n",
    "print(f\"Feature extractor built successfully!\")\n",
    "print(f\"Feature dimension: {feature_dim}\")\n",
    "\n",
    "# Show model summary (layers, parameters, shapes)\n",
    "feature_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction function defined!\n"
     ]
    }
   ],
   "source": [
    "# Extract Deep Features\n",
    "def extract_features(generator):\n",
    "    \"\"\"Extract features from a data generator using the feature model\"\"\"\n",
    "    X, y = [], []\n",
    "    steps = len(generator)\n",
    "    for i in range(steps):\n",
    "        imgs, labels = generator.next()\n",
    "        feats = feature_model.predict(imgs, verbose=0)\n",
    "        X.append(feats)\n",
    "        y.append(labels)\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Processed {i + 1}/{steps} batches\")\n",
    "    return np.vstack(X), np.vstack(y)\n",
    "\n",
    "print(\"Feature extraction function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting training features …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-11 08:35:43.056397: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10/500 batches\n",
      "Processed 20/500 batches\n",
      "Processed 30/500 batches\n",
      "Processed 40/500 batches\n",
      "Processed 50/500 batches\n",
      "Processed 60/500 batches\n",
      "Processed 70/500 batches\n",
      "Processed 80/500 batches\n",
      "Processed 90/500 batches\n",
      "Processed 100/500 batches\n",
      "Processed 110/500 batches\n",
      "Processed 120/500 batches\n",
      "Processed 130/500 batches\n",
      "Processed 140/500 batches\n",
      "Processed 150/500 batches\n",
      "Processed 160/500 batches\n",
      "Processed 170/500 batches\n",
      "Processed 180/500 batches\n",
      "Processed 190/500 batches\n",
      "Processed 200/500 batches\n",
      "Processed 210/500 batches\n",
      "Processed 220/500 batches\n",
      "Processed 230/500 batches\n",
      "Processed 240/500 batches\n",
      "Processed 250/500 batches\n",
      "Processed 260/500 batches\n",
      "Processed 270/500 batches\n",
      "Processed 280/500 batches\n",
      "Processed 290/500 batches\n",
      "Processed 300/500 batches\n",
      "Processed 310/500 batches\n",
      "Processed 320/500 batches\n",
      "Processed 330/500 batches\n",
      "Processed 340/500 batches\n",
      "Processed 350/500 batches\n",
      "Processed 360/500 batches\n",
      "Processed 370/500 batches\n",
      "Processed 380/500 batches\n",
      "Processed 390/500 batches\n",
      "Processed 400/500 batches\n",
      "Processed 410/500 batches\n",
      "Processed 420/500 batches\n",
      "Processed 430/500 batches\n",
      "Processed 440/500 batches\n",
      "Processed 450/500 batches\n",
      "Processed 460/500 batches\n",
      "Processed 470/500 batches\n",
      "Processed 480/500 batches\n",
      "Processed 490/500 batches\n",
      "Processed 500/500 batches\n",
      "Training features shape: (12000, 51200)\n",
      "Training labels shape: (12000, 3)\n"
     ]
    }
   ],
   "source": [
    "# Extract Training Features\n",
    "print(\"Extracting training features …\")\n",
    "X_tr, Y_tr_ohe = extract_features(train_gen)\n",
    "print(f\"Training features shape: {X_tr.shape}\")\n",
    "print(f\"Training labels shape: {Y_tr_ohe.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cython in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (3.1.3)\n",
      "Requirement already satisfied: pymrmr in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.1.11)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pymrmr) (1.25.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install cython\n",
    "!pip install pymrmr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mRMR feature selection function defined!\n",
      "Feature selection pipeline (mRMR + AGWO) implemented.\n"
     ]
    }
   ],
   "source": [
    "## 1. mRMR Feature Ranking\n",
    "try:\n",
    "    import pymrmr\n",
    "except ImportError:\n",
    "    !pip install pymrmr\n",
    "    import pymrmr\n",
    "\n",
    "# Convert features and labels to DataFrame for pymrmr\n",
    "import pandas as pd\n",
    "# ...existing code...\n",
    "import numpy as np, pandas as pd, gc\n",
    "\n",
    "# ...existing code...\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "# ...existing code...\n",
    "# ...existing code...\n",
    "import numpy as np, time, gc\n",
    "from sklearn.feature_selection import mutual_info_classif, VarianceThreshold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def approximate_mrmr(X, y_ohe, n_features=200, sample_rows=1500, var_thresh=0.0,\n",
    "                     mi_subset_rows=2000, redundancy_penalty=0.5, oversample_factor=2):\n",
    "    \"\"\"\n",
    "    Fast approximate mRMR:\n",
    "      1. Optional variance filter\n",
    "      2. Mutual information relevance on row subset\n",
    "      3. Greedy selection penalizing average absolute Pearson corr with already selected\n",
    "    Returns list of original feature indices.\n",
    "    \"\"\"\n",
    "    t0 = time.time()\n",
    "    y = np.argmax(y_ohe, axis=1)\n",
    "    n_samples, n_feats = X.shape\n",
    "    Xf = X.astype(np.float32, copy=False)\n",
    "\n",
    "    # Variance filter\n",
    "    if var_thresh > 0:\n",
    "        vt = VarianceThreshold(var_thresh)\n",
    "        Xv = vt.fit_transform(Xf)\n",
    "        kept_var = np.where(vt.get_support())[0]\n",
    "    else:\n",
    "        Xv = Xf\n",
    "        kept_var = np.arange(n_feats)\n",
    "    print(f\"[approx-mRMR] After variance filter: {len(kept_var)} features\")\n",
    "\n",
    "    # Row subset for MI\n",
    "    if mi_subset_rows and mi_subset_rows < Xv.shape[0]:\n",
    "        rng = np.random.default_rng(42)\n",
    "        rows = rng.choice(Xv.shape[0], size=mi_subset_rows, replace=False)\n",
    "        X_mi = Xv[rows]\n",
    "        y_mi = y[rows]\n",
    "    else:\n",
    "        X_mi = Xv\n",
    "        y_mi = y\n",
    "    # Compute MI\n",
    "    mi = mutual_info_classif(X_mi, y_mi, discrete_features=False, n_neighbors=3, random_state=42)\n",
    "    # Map back to original feature indices\n",
    "    mi_global_idx = kept_var\n",
    "    # Sort by MI\n",
    "    order = np.argsort(mi)[::-1]\n",
    "    mi_global_idx = mi_global_idx[order]\n",
    "    mi_sorted = mi[order]\n",
    "\n",
    "    # Take a pool larger than target\n",
    "    pool_k = min(len(mi_global_idx), oversample_factor * n_features)\n",
    "    pool_idx = mi_global_idx[:pool_k]\n",
    "    pool_data = Xf[:, pool_idx]\n",
    "\n",
    "    # Normalize pool for correlation computations\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "    pool_norm = scaler.fit_transform(pool_data)\n",
    "\n",
    "    selected = []\n",
    "    selected_set = set()\n",
    "    # Precompute correlations lazily\n",
    "    # We'll maintain mean |corr| with already selected features\n",
    "    for _ in range(min(n_features, pool_k)):\n",
    "        best_feat = None\n",
    "        best_score = -1\n",
    "        for j, feat in enumerate(pool_idx):\n",
    "            if feat in selected_set:\n",
    "                continue\n",
    "            relevance = mi_sorted[np.where(mi_global_idx == feat)[0][0]]\n",
    "            if not selected:\n",
    "                score = relevance\n",
    "            else:\n",
    "                # Compute corr with already selected (on normalized matrix)\n",
    "                curr_vec = pool_norm[:, np.where(pool_idx == feat)[0][0]]\n",
    "                sel_cols = [np.where(pool_idx == f)[0][0] for f in selected if f in pool_idx]\n",
    "                if sel_cols:\n",
    "                    sel_mat = pool_norm[:, sel_cols]\n",
    "                    corr = np.abs(np.dot(sel_mat.T, curr_vec) / (len(curr_vec)-1))\n",
    "                    redundancy = corr.mean()\n",
    "                else:\n",
    "                    redundancy = 0.0\n",
    "                score = relevance - redundancy_penalty * redundancy\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_feat = feat\n",
    "        selected.append(best_feat)\n",
    "        selected_set.add(best_feat)\n",
    "    print(f\"[approx-mRMR] Selected {len(selected)} features in {time.time()-t0:.2f}s\")\n",
    "    return selected\n",
    "\n",
    "print(\"mRMR feature selection function defined!\")\n",
    "\n",
    "\n",
    "## 2. Adaptive Grey Wolf Optimization (AGWO)\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# ...existing code...\n",
    "# ...existing code...\n",
    "\n",
    "import numpy as np, gc, hashlib\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def _subset_hash(idxs):\n",
    "    return hashlib.md5(np.asarray(idxs, dtype=np.int32).tobytes()).hexdigest()\n",
    "\n",
    "def agwo_feature_selection_reduced(\n",
    "    X_ranked,\n",
    "    y_ohe,\n",
    "    ranked_global_indices,\n",
    "    n_wolves=18,\n",
    "    n_iter=20,\n",
    "    min_subset=200,\n",
    "    max_subset=1200,\n",
    "    row_sample=2500,\n",
    "    knn_folds=3,\n",
    "    rf_folds=2,\n",
    "    rf_max_features=300,\n",
    "    penalty_weight=0.05,\n",
    "    patience=5,\n",
    "    random_state=42,\n",
    "    verbose=True,\n",
    "    subset_size = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Improved Adaptive Grey Wolf Optimization for feature subset selection.\n",
    "\n",
    "    X_ranked: (n_samples, n_ranked_features) float32\n",
    "    ranked_global_indices: mapping to original feature indices\n",
    "    Returns: list of GLOBAL feature indices selected\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    y = np.argmax(y_ohe, axis=1)\n",
    "    n_samples, n_feats = X_ranked.shape\n",
    "\n",
    "    if subset_size is not None:\n",
    "        min_subset = max_subset = int(min(subset_size, n_feats))\n",
    "\n",
    "    # Row subsample (stratified) for fitness to speed up\n",
    "    if row_sample and row_sample < n_samples:\n",
    "        rows = []\n",
    "        per_class = row_sample // len(np.unique(y))\n",
    "        for cls in np.unique(y):\n",
    "            cls_idx = np.where(y == cls)[0]\n",
    "            take = min(per_class, len(cls_idx))\n",
    "            rows.append(rng.choice(cls_idx, size=take, replace=False))\n",
    "        rows = np.concatenate(rows)\n",
    "    else:\n",
    "        rows = np.arange(n_samples)\n",
    "\n",
    "    X_fit = X_ranked[rows]\n",
    "    y_fit = y[rows]\n",
    "\n",
    "    # Precompute ranking order (identity initial)\n",
    "    base_order = np.arange(n_feats)\n",
    "\n",
    "    # Wolves: each has position vector in [0,1]^n_feats (compressed by sparse init)\n",
    "    # To reduce memory, store only active indices + scalar bias\n",
    "    def init_position():\n",
    "        # Sparse random priorities\n",
    "        vals = rng.random(n_feats)\n",
    "        return vals\n",
    "\n",
    "    wolves = [init_position() for _ in range(n_wolves)]\n",
    "\n",
    "    # Adaptive subset schedule\n",
    "    def subset_budget(iter_idx):\n",
    "        # Linear growth; could switch to logarithmic if needed\n",
    "        return int(min_subset + (max_subset - min_subset) * (iter_idx / max(1, n_iter - 1)))\n",
    "\n",
    "    # Fitness cache\n",
    "    fitness_cache = {}\n",
    "\n",
    "    def eval_subset(local_idx):\n",
    "        if len(local_idx) < 2:\n",
    "            return 0.0\n",
    "        key_hash = _subset_hash(local_idx)\n",
    "        if key_hash in fitness_cache:\n",
    "            return fitness_cache[key_hash]\n",
    "\n",
    "        # Limit RF features to rf_max_features (random slice) for speed\n",
    "        feat_slice = local_idx\n",
    "        if len(feat_slice) > rf_max_features:\n",
    "            feat_slice_rf = rng.choice(feat_slice, size=rf_max_features, replace=False)\n",
    "        else:\n",
    "            feat_slice_rf = feat_slice\n",
    "\n",
    "        X_sub = X_fit[:, feat_slice]\n",
    "        scaler = StandardScaler()\n",
    "        X_sub = scaler.fit_transform(X_sub)\n",
    "\n",
    "        # KNN CV\n",
    "        skf_knn = StratifiedKFold(n_splits=knn_folds, shuffle=True, random_state=123)\n",
    "        knn_scores = []\n",
    "        knn = KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
    "        for tr, va in skf_knn.split(X_sub, y_fit):\n",
    "            knn.fit(X_sub[tr], y_fit[tr])\n",
    "            pred = knn.predict(X_sub[va])\n",
    "            knn_scores.append(accuracy_score(y_fit[va], pred))\n",
    "        knn_acc = np.mean(knn_scores)\n",
    "\n",
    "        # RF (on smaller feature set) for robustness\n",
    "        X_sub_rf = X_fit[:, feat_slice_rf]\n",
    "        scaler_rf = StandardScaler()\n",
    "        X_sub_rf = scaler_rf.fit_transform(X_sub_rf)\n",
    "        skf_rf = StratifiedKFold(n_splits=rf_folds, shuffle=True, random_state=321)\n",
    "        rf_scores = []\n",
    "        rf = RandomForestClassifier(\n",
    "            n_estimators=160,\n",
    "            max_features='sqrt',\n",
    "            n_jobs=-1,\n",
    "            random_state=999\n",
    "        )\n",
    "        for tr, va in skf_rf.split(X_sub_rf, y_fit):\n",
    "            rf.fit(X_sub_rf[tr], y_fit[tr])\n",
    "            pred = rf.predict(X_sub_rf[va])\n",
    "            rf_scores.append(accuracy_score(y_fit[va], pred))\n",
    "        rf_acc = np.mean(rf_scores)\n",
    "\n",
    "        size_penalty = penalty_weight * (len(local_idx) / max_subset)\n",
    "        fitness = 0.65 * knn_acc + 0.35 * rf_acc - size_penalty\n",
    "        fitness_cache[key_hash] = fitness\n",
    "        return fitness\n",
    "\n",
    "    # Convert continuous priority vector → feature index subset\n",
    "    def decode(position, k):\n",
    "        # Take top-k indices\n",
    "        order = np.argpartition(position, -k)[-k:]\n",
    "        # For stable ordering\n",
    "        return order[np.argsort(-position[order])]\n",
    "\n",
    "    # Main AGWO loop\n",
    "    best_global_subset = None\n",
    "    best_fitness = -1\n",
    "    no_improve = 0\n",
    "\n",
    "    for it in range(n_iter):\n",
    "        k_budget = subset_budget(it)\n",
    "\n",
    "        # Decode all wolves\n",
    "        wolf_subsets = [decode(w, k_budget) for w in wolves]\n",
    "        wolf_scores = [eval_subset(sub) for sub in wolf_subsets]\n",
    "\n",
    "        # Identify alpha, beta, delta\n",
    "        order = np.argsort(wolf_scores)[::-1]\n",
    "        alpha, beta, delta = wolves[order[0]], wolves[order[1]], wolves[order[2]]\n",
    "        alpha_subset = wolf_subsets[order[0]]\n",
    "        alpha_score = wolf_scores[order[0]]\n",
    "\n",
    "        if alpha_score > best_fitness:\n",
    "            best_fitness = alpha_score\n",
    "            best_global_subset = alpha_subset.copy()\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"[AGWO] iter {it+1}/{n_iter} k={k_budget} alpha_fit={alpha_score:.4f} best={best_fitness:.4f} cache={len(fitness_cache)}\")\n",
    "\n",
    "        if no_improve >= patience:\n",
    "            if verbose:\n",
    "                print(f\"[AGWO] Early stopping (patience {patience})\")\n",
    "            break\n",
    "\n",
    "        # Grey Wolf coefficient a (linear decay)\n",
    "        a = 2 - 2 * (it / max(1, n_iter - 1))\n",
    "\n",
    "        # Update each wolf (continuous adaptation)\n",
    "        new_wolves = []\n",
    "        for idx, w in enumerate(wolves):\n",
    "            if idx in order[:3]:\n",
    "                new_wolves.append(w)  # keep alpha, beta, delta\n",
    "                continue\n",
    "            A1 = 2 * a * rng.random(n_feats) - a\n",
    "            C1 = 2 * rng.random(n_feats)\n",
    "\n",
    "            A2 = 2 * a * rng.random(n_feats) - a\n",
    "            C2 = 2 * rng.random(n_feats)\n",
    "\n",
    "            A3 = 2 * a * rng.random(n_feats) - a\n",
    "            C3 = 2 * rng.random(n_feats)\n",
    "\n",
    "            D_alpha = np.abs(C1 * alpha - w)\n",
    "            D_beta  = np.abs(C2 * beta  - w)\n",
    "            D_delta = np.abs(C3 * delta - w)\n",
    "\n",
    "            X1 = alpha - A1 * D_alpha\n",
    "            X2 = beta  - A2 * D_beta\n",
    "            X3 = delta - A3 * D_delta\n",
    "\n",
    "            new_pos = (X1 + X2 + X3) / 3.0\n",
    "\n",
    "            # Mutation / diversity\n",
    "            if rng.random() < 0.15:\n",
    "                mut_mask = rng.random(n_feats) < 0.002  # flip sparse\n",
    "                noise = rng.normal(0, 0.25, np.sum(mut_mask))\n",
    "                new_pos[mut_mask] += noise\n",
    "\n",
    "            # Clamp\n",
    "            new_pos = np.clip(new_pos, -1.0, 1.0)\n",
    "            new_wolves.append(new_pos)\n",
    "\n",
    "        # Diversity injection if stagnating\n",
    "        if no_improve == patience - 1:\n",
    "            inject_count = max(2, n_wolves // 5)\n",
    "            for _ in range(inject_count):\n",
    "                ridx = rng.integers(3, n_wolves)  # avoid top 3\n",
    "                new_wolves[ridx] = init_position()\n",
    "\n",
    "        wolves = new_wolves\n",
    "\n",
    "    # Decode best subset using final budget (or its original size)\n",
    "    final_k = len(best_global_subset)\n",
    "    final_local = best_global_subset\n",
    "    # Map to global feature indices\n",
    "    selected_global = [ranked_global_indices[i] for i in final_local]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[AGWO] Finished: selected {len(selected_global)} features; best_fitness={best_fitness:.4f}\")\n",
    "\n",
    "    return selected_global\n",
    "\n",
    "print(\"Feature selection pipeline (mRMR + AGWO) implemented.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting validation features …\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10/125 batches\n",
      "Processed 20/125 batches\n",
      "Processed 30/125 batches\n",
      "Processed 40/125 batches\n",
      "Processed 50/125 batches\n",
      "Processed 60/125 batches\n",
      "Processed 70/125 batches\n",
      "Processed 80/125 batches\n",
      "Processed 90/125 batches\n",
      "Processed 100/125 batches\n",
      "Processed 110/125 batches\n",
      "Processed 120/125 batches\n",
      "Validation features shape: (3000, 51200)\n",
      "Validation labels shape: (3000, 3)\n"
     ]
    }
   ],
   "source": [
    "# Extract Validation Features\n",
    "print(\"Extracting validation features …\")\n",
    "X_va, Y_va_ohe = extract_features(val_gen)\n",
    "print(f\"Validation features shape: {X_va.shape}\")\n",
    "print(f\"Validation labels shape: {Y_va_ohe.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features shape: (15000, 51200)\n",
      "Total labels shape: (15000,)\n",
      "Classes present: [0 1 2]\n",
      "Class distribution: [5000 5000 5000]\n"
     ]
    }
   ],
   "source": [
    "# Combine Features and Convert Labels\n",
    "X_full = np.vstack([X_tr, X_va])\n",
    "y_full = np.argmax(np.vstack([Y_tr_ohe, Y_va_ohe]), axis=1)\n",
    "\n",
    "print(f\"Total features shape: {X_full.shape}\")\n",
    "print(f\"Total labels shape: {y_full.shape}\")\n",
    "print(f\"Classes present: {np.unique(y_full)}\")\n",
    "print(f\"Class distribution: {np.bincount(y_full)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[approx-mRMR] After variance filter: 51200 features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[approx-mRMR] Selected 250 features in 686.83s\n",
      "[pipeline] Ranked features: 250\n",
      "[AGWO] iter 1/8 k=30 alpha_fit=0.8834 best=0.8834 cache=8\n",
      "[AGWO] iter 2/8 k=30 alpha_fit=0.8834 best=0.8834 cache=13\n",
      "[AGWO] iter 3/8 k=30 alpha_fit=0.8834 best=0.8834 cache=18\n",
      "[AGWO] iter 4/8 k=30 alpha_fit=0.8857 best=0.8857 cache=23\n",
      "[AGWO] iter 5/8 k=30 alpha_fit=0.8857 best=0.8857 cache=28\n",
      "[AGWO] iter 6/8 k=30 alpha_fit=0.8857 best=0.8857 cache=33\n",
      "[AGWO] iter 7/8 k=30 alpha_fit=0.8857 best=0.8857 cache=38\n",
      "[AGWO] iter 8/8 k=30 alpha_fit=0.8857 best=0.8857 cache=43\n",
      "[AGWO] Finished: selected 30 features; best_fitness=0.8857\n",
      "[pipeline] AGWO selected 30 features.\n",
      "[pipeline] Train (12000, 30), Test (3000, 30), total time 725.65s\n",
      "Training set shape: (12000, 30)\n",
      "Test set shape: (3000, 30)\n",
      "Training class distribution: [4000 4000 4000]\n",
      "Test class distribution: [1000 1000 1000]\n"
     ]
    }
   ],
   "source": [
    "# --- mRMR + AGWO Feature Selection Pipeline ---\n",
    "t_total = time.time()\n",
    "\n",
    "# Parameters (shrink aggressively first)\n",
    "n_mrmr = 250          # lower than 200 to speed up\n",
    "sample_rows = 1200\n",
    "mi_subset_rows = 1500\n",
    "redundancy_penalty = 0.4\n",
    "oversample_factor = 2\n",
    "subset_size = 30      # AGWO subset\n",
    "n_wolves = 8\n",
    "n_iter = 8\n",
    "\n",
    "# 1. Fast approximate mRMR replacement\n",
    "ranked_features = approximate_mrmr(\n",
    "    X_tr, Y_tr_ohe,\n",
    "    n_features=n_mrmr,\n",
    "    sample_rows=sample_rows,\n",
    "    mi_subset_rows=mi_subset_rows,\n",
    "    redundancy_penalty=redundancy_penalty,\n",
    "    oversample_factor=oversample_factor\n",
    ")\n",
    "print(f\"[pipeline] Ranked features: {len(ranked_features)}\")\n",
    "\n",
    "# 2. Slice training matrix to ranked features ONLY for AGWO\n",
    "X_tr_ranked = X_tr[:, ranked_features]\n",
    "\n",
    "selected_features = agwo_feature_selection_reduced(\n",
    "    X_tr_ranked, Y_tr_ohe, ranked_features,\n",
    "    n_wolves=n_wolves, n_iter=n_iter, subset_size=subset_size\n",
    ")\n",
    "print(f\"[pipeline] AGWO selected {len(selected_features)} features.\")\n",
    "\n",
    "# 4. Apply selection to full (train+val) without building giant X_full first\n",
    "X_tr_sel = X_tr[:, selected_features]\n",
    "X_va_sel = X_va[:, selected_features]\n",
    "y_full = np.argmax(np.vstack([Y_tr_ohe, Y_va_ohe]), axis=1)\n",
    "X_full_sel = np.vstack([X_tr_sel, X_va_sel])\n",
    "\n",
    "# 5. Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_full_sel, y_full, test_size=0.20, random_state=SEED, stratify=y_full\n",
    ")\n",
    "\n",
    "print(f\"[pipeline] Train {X_train.shape}, Test {X_test.shape}, total time {time.time()-t_total:.2f}s\")\n",
    "\n",
    "# Cleanup\n",
    "del X_tr_ranked, X_tr_sel, X_va_sel\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"Training class distribution: {np.bincount(y_train)}\")\n",
    "print(f\"Test class distribution: {np.bincount(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (3.0.5)\n",
      "Requirement already satisfied: numpy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from xgboost) (1.25.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from xgboost) (2.28.3)\n",
      "Requirement already satisfied: scipy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from xgboost) (1.11.4)\n",
      "Classifiers initialized:\n",
      "  KNN: k=5, weights='distance'\n",
      "  SVM: RBF kernel, C=1.0, gamma='scale'\n",
      "  Random Forest: 300 trees\n",
      "  XGBoost: 200 estimators\n",
      "  Logistic Regression: max_iter=1000\n"
     ]
    }
   ],
   "source": [
    "# Initialize Classifiers\n",
    "!pip install xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
    "svm = SVC(kernel='rbf', probability=True, C=1.0, gamma='scale', random_state=SEED)\n",
    "rf  = RandomForestClassifier(n_estimators=300, random_state=SEED, n_jobs=-1)\n",
    "xgb = XGBClassifier(n_estimators=200, random_state=SEED, use_label_encoder=False, eval_metric='mlogloss')\n",
    "lr  = LogisticRegression(max_iter=1000, random_state=SEED, n_jobs=-1)\n",
    "\n",
    "print(\"Classifiers initialized:\")\n",
    "print(f\"  KNN: k=5, weights='distance'\")\n",
    "print(f\"  SVM: RBF kernel, C=1.0, gamma='scale'\")\n",
    "print(f\"  Random Forest: 300 trees\")\n",
    "print(f\"  XGBoost: 200 estimators\")\n",
    "print(f\"  Logistic Regression: max_iter=1000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifiers …\n",
      "  Training KNN...\n",
      "  Training SVM...\n",
      "  Training Random Forest...\n",
      "  Training XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [09:17:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training Logistic Regression...\n",
      "All classifiers trained successfully!\n"
     ]
    }
   ],
   "source": [
    "# Train Classifiers\n",
    "print(\"Training classifiers …\")\n",
    "\n",
    "print(\"  Training KNN...\")\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "print(\"  Training SVM...\")\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "print(\"  Training Random Forest...\")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "print(\"  Training XGBoost...\")\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "print(\"  Training Logistic Regression...\")\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "print(\"All classifiers trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions...\n",
      "Predictions completed!\n"
     ]
    }
   ],
   "source": [
    "# Make Predictions\n",
    "print(\"Making predictions...\")\n",
    "\n",
    "knn_pred = knn.predict(X_test)\n",
    "svm_pred = svm.predict(X_test)\n",
    "rf_pred  = rf.predict(X_test)\n",
    "xgb_pred = xgb.predict(X_test)\n",
    "lr_pred  = lr.predict(X_test)\n",
    "\n",
    "# Probabilistic predictions (for ensemble if needed)\n",
    "knn_proba = knn.predict_proba(X_test) if hasattr(knn, 'predict_proba') else None\n",
    "svm_proba = svm.predict_proba(X_test) if hasattr(svm, 'predict_proba') else None\n",
    "rf_proba  = rf.predict_proba(X_test) if hasattr(rf, 'predict_proba') else None\n",
    "xgb_proba = xgb.predict_proba(X_test) if hasattr(xgb, 'predict_proba') else None\n",
    "lr_proba  = lr.predict_proba(X_test) if hasattr(lr, 'predict_proba') else None\n",
    "\n",
    "print(\"Predictions completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual Classifier Accuracies:\n",
      "  KNN: 0.9307\n",
      "  SVM: 0.9447\n",
      "  RF : 0.9487\n",
      "  XGB: 0.9543\n",
      "  LR : 0.9420\n",
      "\n",
      "=== KNN Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    lung_aca       0.89      0.90      0.90      1000\n",
      "      lung_n       0.99      0.99      0.99      1000\n",
      "    lung_scc       0.91      0.90      0.90      1000\n",
      "\n",
      "    accuracy                           0.93      3000\n",
      "   macro avg       0.93      0.93      0.93      3000\n",
      "weighted avg       0.93      0.93      0.93      3000\n",
      "\n",
      "\n",
      "=== SVM Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    lung_aca       0.93      0.90      0.92      1000\n",
      "      lung_n       0.99      0.99      0.99      1000\n",
      "    lung_scc       0.91      0.94      0.93      1000\n",
      "\n",
      "    accuracy                           0.94      3000\n",
      "   macro avg       0.94      0.94      0.94      3000\n",
      "weighted avg       0.94      0.94      0.94      3000\n",
      "\n",
      "\n",
      "=== Random Forest Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    lung_aca       0.94      0.91      0.92      1000\n",
      "      lung_n       1.00      1.00      1.00      1000\n",
      "    lung_scc       0.91      0.94      0.93      1000\n",
      "\n",
      "    accuracy                           0.95      3000\n",
      "   macro avg       0.95      0.95      0.95      3000\n",
      "weighted avg       0.95      0.95      0.95      3000\n",
      "\n",
      "\n",
      "=== XGBoost Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    lung_aca       0.94      0.92      0.93      1000\n",
      "      lung_n       1.00      1.00      1.00      1000\n",
      "    lung_scc       0.92      0.95      0.93      1000\n",
      "\n",
      "    accuracy                           0.95      3000\n",
      "   macro avg       0.95      0.95      0.95      3000\n",
      "weighted avg       0.95      0.95      0.95      3000\n",
      "\n",
      "\n",
      "=== Logistic Regression Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    lung_aca       0.92      0.91      0.91      1000\n",
      "      lung_n       0.99      0.99      0.99      1000\n",
      "    lung_scc       0.91      0.93      0.92      1000\n",
      "\n",
      "    accuracy                           0.94      3000\n",
      "   macro avg       0.94      0.94      0.94      3000\n",
      "weighted avg       0.94      0.94      0.94      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Individual Classifier Results\n",
    "print(\"Individual Classifier Accuracies:\")\n",
    "knn_acc = accuracy_score(y_test, knn_pred)\n",
    "svm_acc = accuracy_score(y_test, svm_pred)\n",
    "rf_acc = accuracy_score(y_test, rf_pred)\n",
    "xgb_acc = accuracy_score(y_test, xgb_pred)\n",
    "lr_acc = accuracy_score(y_test, lr_pred)\n",
    "\n",
    "print(f\"  KNN: {knn_acc:.4f}\")\n",
    "print(f\"  SVM: {svm_acc:.4f}\")\n",
    "print(f\"  RF : {rf_acc:.4f}\")\n",
    "print(f\"  XGB: {xgb_acc:.4f}\")\n",
    "print(f\"  LR : {lr_acc:.4f}\")\n",
    "\n",
    "# Display individual classification reports\n",
    "target_names = [id2label[i] for i in range(num_classes)]\n",
    "\n",
    "print(\"\\n=== KNN Classification Report ===\")\n",
    "print(classification_report(y_test, knn_pred, target_names=target_names))\n",
    "\n",
    "print(\"\\n=== SVM Classification Report ===\")\n",
    "print(classification_report(y_test, svm_pred, target_names=target_names))\n",
    "\n",
    "print(\"\\n=== Random Forest Classification Report ===\")\n",
    "print(classification_report(y_test, rf_pred, target_names=target_names))\n",
    "\n",
    "print(\"\\n=== XGBoost Classification Report ===\")\n",
    "print(classification_report(y_test, xgb_pred, target_names=target_names))\n",
    "\n",
    "print(\"\\n=== Logistic Regression Classification Report ===\")\n",
    "print(classification_report(y_test, lr_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Accuracy (Priority-Based): 0.9513\n",
      "\n",
      "Improvement over best individual: -0.0030\n",
      "\n",
      "=== Ensemble Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    lung_aca       0.94      0.91      0.93      1000\n",
      "      lung_n       1.00      1.00      1.00      1000\n",
      "    lung_scc       0.92      0.95      0.93      1000\n",
      "\n",
      "    accuracy                           0.95      3000\n",
      "   macro avg       0.95      0.95      0.95      3000\n",
      "weighted avg       0.95      0.95      0.95      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ensemble Fusion (Priority-Based Strategy)\n",
    "# Priority: SVM > XGBoost > RF > KNN > LR\n",
    "# If SVM and XGBoost agree, use that prediction. Else, use SVM. If not, use XGBoost. Else, fallback to majority vote.\n",
    "def priority_ensemble(svm_pred, xgb_pred, rf_pred, knn_pred, lr_pred):\n",
    "    preds = np.stack([knn_pred, svm_pred, rf_pred, xgb_pred, lr_pred], axis=0)\n",
    "    final = []\n",
    "    for i in range(svm_pred.shape[0]):\n",
    "        if svm_pred[i] == xgb_pred[i]:\n",
    "            final.append(svm_pred[i])\n",
    "        elif svm_pred[i] == rf_pred[i]:\n",
    "            final.append(svm_pred[i])\n",
    "        elif xgb_pred[i] == rf_pred[i]:\n",
    "            final.append(xgb_pred[i])\n",
    "        else:\n",
    "            # fallback to majority vote\n",
    "            vals, counts = np.unique(preds[:, i], return_counts=True)\n",
    "            final.append(vals[np.argmax(counts)])\n",
    "    return np.array(final)\n",
    "\n",
    "ens = priority_ensemble(svm_pred, xgb_pred, rf_pred, knn_pred, lr_pred)\n",
    "ens_acc = accuracy_score(y_test, ens)\n",
    "\n",
    "print(f\"Ensemble Accuracy (Priority-Based): {ens_acc:.4f}\")\n",
    "print(f\"\\nImprovement over best individual: {ens_acc - max(knn_acc, svm_acc, rf_acc, xgb_acc, lr_acc):.4f}\")\n",
    "\n",
    "print(\"\\n=== Ensemble Classification Report ===\")\n",
    "print(classification_report(y_test, ens, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier ranking (best to worst):\n",
      "  1. XGB (acc=0.9543)\n",
      "  2. RF (acc=0.9487)\n",
      "  3. SVM (acc=0.9447)\n",
      "  4. LR (acc=0.9420)\n",
      "  5. KNN (acc=0.9307)\n",
      "Classifier weights (epsilon_j):\n",
      "  XGB: 0.2212\n",
      "  RF: 0.2111\n",
      "  SVM: 0.2003\n",
      "  LR: 0.1892\n",
      "  KNN: 0.1782\n",
      "Weighted-Average Ensemble Accuracy: 0.9513\n",
      "\n",
      "Improvement over best individual: -0.0030\n",
      "\n",
      "=== Weighted-Average Ensemble Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    lung_aca       0.94      0.91      0.93      1000\n",
      "      lung_n       0.99      1.00      1.00      1000\n",
      "    lung_scc       0.92      0.94      0.93      1000\n",
      "\n",
      "    accuracy                           0.95      3000\n",
      "   macro avg       0.95      0.95      0.95      3000\n",
      "weighted avg       0.95      0.95      0.95      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Weighted-Average Ensemble Method (Performance-Ranked)\n",
    "import numpy as np\n",
    "\n",
    "# 1. Gather classifier predictions and accuracies\n",
    "classifier_preds = [knn_pred, svm_pred, rf_pred, xgb_pred, lr_pred]\n",
    "classifier_accs = [knn_acc, svm_acc, rf_acc, xgb_acc, lr_acc]\n",
    "classifier_names = ['KNN', 'SVM', 'RF', 'XGB', 'LR']\n",
    "\n",
    "# 2. Rank classifiers by accuracy (descending)\n",
    "ranked_indices = np.argsort(classifier_accs)[::-1]\n",
    "ranked_accs = [classifier_accs[i] for i in ranked_indices]\n",
    "ranked_preds = [classifier_preds[i] for i in ranked_indices]\n",
    "ranked_names = [classifier_names[i] for i in ranked_indices]\n",
    "\n",
    "print('Classifier ranking (best to worst):')\n",
    "for i, name in enumerate(ranked_names):\n",
    "    print(f'  {i+1}. {name} (acc={ranked_accs[i]:.4f})')\n",
    "\n",
    "# 3. Calculate intermediate scores T_j\n",
    "T = [1.0]\n",
    "for j in range(1, len(ranked_accs)):\n",
    "    T.append(T[-1] * ranked_accs[j-1])\n",
    "\n",
    "# 4. Normalize to get weights epsilon_j\n",
    "T_sum = sum(T)\n",
    "weights = [t / T_sum for t in T]\n",
    "\n",
    "print('Classifier weights (epsilon_j):')\n",
    "for i, (name, w) in enumerate(zip(ranked_names, weights)):\n",
    "    print(f'  {name}: {w:.4f}')\n",
    "\n",
    "# 5. Weighted voting for each test sample\n",
    "n_classes = num_classes\n",
    "n_samples = len(y_test)\n",
    "weighted_votes = np.zeros((n_samples, n_classes))\n",
    "\n",
    "for clf_idx, (pred, w) in enumerate(zip(ranked_preds, weights)):\n",
    "    for i in range(n_samples):\n",
    "        weighted_votes[i, pred[i]] += w\n",
    "\n",
    "weighted_ensemble_pred = np.argmax(weighted_votes, axis=1)\n",
    "\n",
    "weighted_ens_acc = accuracy_score(y_test, weighted_ensemble_pred)\n",
    "\n",
    "print(f'Weighted-Average Ensemble Accuracy: {weighted_ens_acc:.4f}')\n",
    "print(f'\\nImprovement over best individual: {weighted_ens_acc - ranked_accs[0]:.4f}')\n",
    "\n",
    "print('\\n=== Weighted-Average Ensemble Classification Report ===')\n",
    "print(classification_report(y_test, weighted_ensemble_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL RESULTS SUMMARY\n",
      "============================================================\n",
      "Total samples processed: 15000\n",
      "Features selected by AGWO: 30 / 51200 (0.1%)\n",
      "Test set size: 3000\n",
      "\n",
      "Classifier Accuracies:\n",
      "  KNN:               0.9307\n",
      "  SVM:               0.9447\n",
      "  Random Forest:     0.9487\n",
      "  Ensemble (Fusion): 0.9513 ← BEST\n",
      "\n",
      "Class Labels:\n",
      "  0: lung_aca\n",
      "  1: lung_n\n",
      "  2: lung_scc\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# filepath: /lung_cancer/code_multihead.ipynb\n",
    "# ...existing code...\n",
    "# Summary Results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"Total samples processed: {len(y_full)}\")\n",
    "\n",
    "# Resolve selected features list (legacy variable fallback)\n",
    "if 'selected_features' in globals():\n",
    "    sel_list = selected_features\n",
    "elif 'sel_idx' in globals():\n",
    "    sel_list = sel_idx\n",
    "elif 'feature_subset' in globals():\n",
    "    sel_list = feature_subset\n",
    "else:\n",
    "    sel_list = []\n",
    "\n",
    "# Try to infer original feature count\n",
    "if 'X_tr_original' in globals():\n",
    "    orig_feat_total = X_tr_original.shape[1]\n",
    "elif 'X_tr' in globals():\n",
    "    orig_feat_total = X_tr.shape[1]\n",
    "elif 'X_full' in globals():\n",
    "    orig_feat_total = X_full.shape[1]\n",
    "else:\n",
    "    # Fallback to selected count (prevents division error)\n",
    "    orig_feat_total = max(len(sel_list), 1)\n",
    "\n",
    "selected_count = len(sel_list)\n",
    "pct = (selected_count / orig_feat_total) if orig_feat_total else 0.0\n",
    "print(f\"Features selected by AGWO: {selected_count} / {orig_feat_total} ({pct:.1%})\")\n",
    "\n",
    "print(f\"Test set size: {len(y_test)}\")\n",
    "print(\"\\nClassifier Accuracies:\")\n",
    "print(f\"  KNN:               {knn_acc:.4f}\")\n",
    "print(f\"  SVM:               {svm_acc:.4f}\")\n",
    "print(f\"  Random Forest:     {rf_acc:.4f}\")\n",
    "print(f\"  Ensemble (Fusion): {ens_acc:.4f} ← BEST\")\n",
    "\n",
    "print(\"\\nClass Labels:\")\n",
    "for i, label in id2label.items():\n",
    "    print(f\"  {i}: {label}\")\n",
    "print(\"=\"*60)\n",
    "# ...existing code..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

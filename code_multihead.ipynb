{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lung Histopathology Classification: ACA / N / SCC\n",
    "## Multi-CNN + Channel Attention + GA + KNN/SVM/RF + Fusion\n",
    "\n",
    "This notebook implements a comprehensive lung histopathology classification system that combines:\n",
    "- Multiple CNN backbones (DenseNet121, ResNet50, VGG16)\n",
    "- Channel attention mechanism (SE blocks)\n",
    "- Genetic Algorithm for feature selection\n",
    "- Ensemble of classical ML classifiers (KNN, SVM, Random Forest)\n",
    "- Majority voting fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.25.2\n",
      "  Downloading numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting tensorflow==2.15.0\n",
      "  Using cached tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
      "Collecting keras==2.15.0\n",
      "  Using cached keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow==2.15.0)\n",
      "  Using cached absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow==2.15.0)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow==2.15.0)\n",
      "  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow==2.15.0)\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow==2.15.0)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=2.9.0 (from tensorflow==2.15.0)\n",
      "  Using cached h5py-3.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow==2.15.0)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting ml-dtypes~=0.2.0 (from tensorflow==2.15.0)\n",
      "  Using cached ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow==2.15.0)\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting packaging (from tensorflow==2.15.0)\n",
      "  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.15.0)\n",
      "  Using cached protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Collecting setuptools (from tensorflow==2.15.0)\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting six>=1.12.0 (from tensorflow==2.15.0)\n",
      "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow==2.15.0)\n",
      "  Using cached termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting typing-extensions>=3.6.6 (from tensorflow==2.15.0)\n",
      "  Using cached typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.15.0)\n",
      "  Using cached wrapt-1.14.2-cp310-cp310-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow==2.15.0)\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow==2.15.0)\n",
      "  Using cached grpcio-1.74.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.0)\n",
      "  Using cached tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
      "  Using cached tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Using cached google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting google-auth-oauthlib<2,>=0.5 (from tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Using cached google_auth_oauthlib-1.2.2-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Using cached markdown-3.8.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting requests<3,>=2.21.0 (from tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Using cached cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Using cached pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Using cached rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Using cached charset_normalizer-3.4.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Using cached certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow==2.15.0)\n",
      "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Using cached oauthlib-3.3.1-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Using cached MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Downloading numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m144.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
      "Using cached keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "Using cached grpcio-1.74.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
      "Using cached ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "Using cached protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "Using cached tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
      "Using cached google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
      "Using cached cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Using cached google_auth_oauthlib-1.2.2-py3-none-any.whl (19 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Using cached charset_normalizer-3.4.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (152 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "Using cached tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached wrapt-1.14.2-cp310-cp310-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (76 kB)\n",
      "Using cached absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Using cached certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "Using cached flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached h5py-3.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "Using cached markdown-3.8.2-py3-none-any.whl (106 kB)\n",
      "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Using cached pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached oauthlib-3.3.1-py3-none-any.whl (160 kB)\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached tensorflow_io_gcs_filesystem-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "Using cached termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\n",
      "Using cached packaging-25.0-py3-none-any.whl (66 kB)\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, wheel, urllib3, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, six, setuptools, pyasn1, protobuf, packaging, opt-einsum, oauthlib, numpy, MarkupSafe, markdown, keras, idna, grpcio, gast, charset_normalizer, certifi, cachetools, absl-py, werkzeug, rsa, requests, pyasn1-modules, ml-dtypes, h5py, google-pasta, astunparse, requests-oauthlib, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "\u001b[2K  Attempting uninstall: libclang\n",
      "\u001b[2K    Found existing installation: libclang 18.1.1\n",
      "\u001b[2K    Uninstalling libclang-18.1.1:\n",
      "\u001b[2K      Successfully uninstalled libclang-18.1.1\n",
      "\u001b[2K  Attempting uninstall: flatbuffers━━━━━━━━━━━━━\u001b[0m \u001b[32m 0/41\u001b[0m [libclang]\n",
      "\u001b[2K    Found existing installation: flatbuffers 24.3.252m 0/41\u001b[0m [libclang]\n",
      "\u001b[2K    Uninstalling flatbuffers-24.3.25:━━━━━━━\u001b[0m \u001b[32m 0/41\u001b[0m [libclang]\n",
      "\u001b[2K      Successfully uninstalled flatbuffers-24.3.25[32m 0/41\u001b[0m [libclang]\n",
      "\u001b[2K  Attempting uninstall: wrapt━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 0/41\u001b[0m [libclang]\n",
      "\u001b[2K    Found existing installation: wrapt 1.14.2[0m \u001b[32m 0/41\u001b[0m [libclang]\n",
      "\u001b[2K    Uninstalling wrapt-1.14.2:━━━━━━━━━━━━━━\u001b[0m \u001b[32m 0/41\u001b[0m [libclang]\n",
      "\u001b[2K      Successfully uninstalled wrapt-1.14.2━\u001b[0m \u001b[32m 0/41\u001b[0m [libclang]\n",
      "\u001b[2K  Attempting uninstall: wheel━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 0/41\u001b[0m [libclang]\n",
      "\u001b[2K    Found existing installation: wheel 0.45.1[0m \u001b[32m 0/41\u001b[0m [libclang]\n",
      "\u001b[2K    Uninstalling wheel-0.45.1:━━━━━━━━━━━━━━\u001b[0m \u001b[32m 0/41\u001b[0m [libclang]\n",
      "\u001b[2K      Successfully uninstalled wheel-0.45.1━\u001b[0m \u001b[32m 0/41\u001b[0m [libclang]\n",
      "\u001b[2K  Attempting uninstall: urllib3━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/41\u001b[0m [wheel]\n",
      "\u001b[2K    Found existing installation: urllib3 2.5.0━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/41\u001b[0m [wheel]\n",
      "\u001b[2K    Uninstalling urllib3-2.5.0:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/41\u001b[0m [wheel]\n",
      "\u001b[2K      Successfully uninstalled urllib3-2.5.0━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/41\u001b[0m [wheel]\n",
      "\u001b[2K  Attempting uninstall: typing-extensions━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/41\u001b[0m [wheel]\n",
      "\u001b[2K    Found existing installation: typing_extensions 4.14.1━━━━━\u001b[0m \u001b[32m 3/41\u001b[0m [wheel]\n",
      "\u001b[2K    Uninstalling typing_extensions-4.14.1:━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/41\u001b[0m [typing-extensions]\n",
      "\u001b[2K      Successfully uninstalled typing_extensions-4.14.1━━━━━━━━━━━\u001b[0m \u001b[32m 5/41\u001b[0m [typing-extensions]\n",
      "\u001b[2K  Attempting uninstall: termcolor━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/41\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Found existing installation: termcolor 2.1.0━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/41\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Uninstalling termcolor-2.1.0:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/41\u001b[0m [typing-extensions]\n",
      "\u001b[2K      Successfully uninstalled termcolor-2.1.0━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/41\u001b[0m [typing-extensions]\n",
      "\u001b[2K  Attempting uninstall: tensorflow-io-gcs-filesystem━━━━━━━━━━\u001b[0m \u001b[32m 5/41\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Found existing installation: tensorflow-io-gcs-filesystem 0.37.1[32m 5/41\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Uninstalling tensorflow-io-gcs-filesystem-0.37.1:━━━━━━━━━\u001b[0m \u001b[32m 5/41\u001b[0m [typing-extensions]\n",
      "\u001b[2K      Successfully uninstalled tensorflow-io-gcs-filesystem-0.37.1 \u001b[32m 5/41\u001b[0m [typing-extensions]\n",
      "\u001b[2K  Attempting uninstall: tensorflow-estimator━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/41\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Found existing installation: tensorflow-estimator 2.10.0━━\u001b[0m \u001b[32m 5/41\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Uninstalling tensorflow-estimator-2.10.0:━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/41\u001b[0m [tensorflow-estimator]\n",
      "\u001b[2K      Successfully uninstalled tensorflow-estimator-2.10.0━━━━\u001b[0m \u001b[32m 8/41\u001b[0m [tensorflow-estimator]\n",
      "\u001b[2K  Attempting uninstall: tensorboard-data-server━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/41\u001b[0m [tensorflow-estimator]\n",
      "\u001b[2K    Found existing installation: tensorboard-data-server 0.6.1\u001b[0m \u001b[32m 8/41\u001b[0m [tensorflow-estimator]\n",
      "\u001b[2K    Uninstalling tensorboard-data-server-0.6.1:━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/41\u001b[0m [tensorflow-estimator]\n",
      "\u001b[2K      Successfully uninstalled tensorboard-data-server-0.6.1━━\u001b[0m \u001b[32m 8/41\u001b[0m [tensorflow-estimator]\n",
      "\u001b[2K  Attempting uninstall: six90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/41\u001b[0m [tensorflow-estimator]\n",
      "\u001b[2K    Found existing installation: six 1.17.0━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/41\u001b[0m [tensorflow-estimator]\n",
      "\u001b[2K    Uninstalling six-1.17.0:[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/41\u001b[0m [six]ow-estimator]\n",
      "\u001b[2K      Successfully uninstalled six-1.17.0━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/41\u001b[0m [six]\n",
      "\u001b[2K  Attempting uninstall: setuptools0m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/41\u001b[0m [six]\n",
      "\u001b[2K    Found existing installation: setuptools 80.9.0━━━━━━━━━━━━\u001b[0m \u001b[32m10/41\u001b[0m [six]\n",
      "\u001b[2K    Uninstalling setuptools-80.9.0:━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/41\u001b[0m [six]\n",
      "\u001b[2K      Successfully uninstalled setuptools-80.9.0━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/41\u001b[0m [setuptools]\n",
      "\u001b[2K  Attempting uninstall: pyasn10m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/41\u001b[0m [setuptools]\n",
      "\u001b[2K    Found existing installation: pyasn1 0.6.1━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/41\u001b[0m [setuptools]\n",
      "\u001b[2K    Uninstalling pyasn1-0.6.1:90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/41\u001b[0m [setuptools]\n",
      "\u001b[2K      Successfully uninstalled pyasn1-0.6.1━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/41\u001b[0m [setuptools]\n",
      "\u001b[2K  Attempting uninstall: protobufm\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/41\u001b[0m [pyasn1]\n",
      "\u001b[2K    Found existing installation: protobuf 3.20.3━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/41\u001b[0m [pyasn1]\n",
      "\u001b[2K    Uninstalling protobuf-3.20.3:m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/41\u001b[0m [pyasn1]\n",
      "\u001b[2K      Successfully uninstalled protobuf-3.20.3━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/41\u001b[0m [protobuf]\n",
      "\u001b[2K  Attempting uninstall: packagingm\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/41\u001b[0m [protobuf]\n",
      "\u001b[2K    Found existing installation: packaging 25.0━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/41\u001b[0m [protobuf]\n",
      "\u001b[2K    Uninstalling packaging-25.0:90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/41\u001b[0m [protobuf]\n",
      "\u001b[2K      Successfully uninstalled packaging-25.0━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/41\u001b[0m [protobuf]\n",
      "\u001b[2K  Attempting uninstall: opt-einsumm━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/41\u001b[0m [protobuf]\n",
      "\u001b[2K    Found existing installation: opt-einsum 3.3.0━━━━━━━━━━━━━\u001b[0m \u001b[32m13/41\u001b[0m [protobuf]\n",
      "\u001b[2K    Uninstalling opt-einsum-3.3.0:m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/41\u001b[0m [protobuf]\n",
      "\u001b[2K      Successfully uninstalled opt-einsum-3.3.0━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/41\u001b[0m [protobuf]\n",
      "\u001b[2K  Attempting uninstall: oauthlib\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/41\u001b[0m [opt-einsum]\n",
      "\u001b[2K    Found existing installation: oauthlib 3.3.1━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/41\u001b[0m [opt-einsum]\n",
      "\u001b[2K    Uninstalling oauthlib-3.3.1:\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/41\u001b[0m [opt-einsum]\n",
      "\u001b[2K      Successfully uninstalled oauthlib-3.3.1━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/41\u001b[0m [opt-einsum]\n",
      "\u001b[2K  Attempting uninstall: numpy[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/41\u001b[0m [opt-einsum]\n",
      "\u001b[2K    Found existing installation: numpy 1.21.6━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/41\u001b[0m [opt-einsum]\n",
      "\u001b[2K    Uninstalling numpy-1.21.6:91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/41\u001b[0m [numpy]]\n",
      "\u001b[2K      Successfully uninstalled numpy-1.21.6━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/41\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: MarkupSafe\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/41\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: MarkupSafe 3.0.2━━━━━━━━━━━━━\u001b[0m \u001b[32m17/41\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling MarkupSafe-3.0.2:\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/41\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled MarkupSafe-3.0.2━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/41\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: markdown0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/41\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: Markdown 3.8.2━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/41\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling Markdown-3.8.2:91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19/41\u001b[0m [markdown]\n",
      "\u001b[2K      Successfully uninstalled Markdown-3.8.2━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19/41\u001b[0m [markdown]\n",
      "\u001b[2K  Attempting uninstall: keras1m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19/41\u001b[0m [markdown]\n",
      "\u001b[2K    Found existing installation: keras 2.10.0━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19/41\u001b[0m [markdown]\n",
      "\u001b[2K    Uninstalling keras-2.10.0:m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/41\u001b[0m [keras]\n",
      "\u001b[2K      Successfully uninstalled keras-2.10.090m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/41\u001b[0m [keras]\n",
      "\u001b[2K  Attempting uninstall: idna[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/41\u001b[0m [keras]\n",
      "\u001b[2K    Found existing installation: idna 3.10━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/41\u001b[0m [keras]\n",
      "\u001b[2K    Uninstalling idna-3.10:\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/41\u001b[0m [keras]\n",
      "\u001b[2K      Successfully uninstalled idna-3.100m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/41\u001b[0m [keras]\n",
      "\u001b[2K  Attempting uninstall: grpcio0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21/41\u001b[0m [idna]\n",
      "\u001b[2K    Found existing installation: grpcio 1.42.0━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21/41\u001b[0m [idna]\n",
      "\u001b[2K    Uninstalling grpcio-1.42.0:0m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21/41\u001b[0m [idna]\n",
      "\u001b[2K      Successfully uninstalled grpcio-1.42.0━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21/41\u001b[0m [idna]\n",
      "\u001b[2K  Attempting uninstall: gast━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/41\u001b[0m [grpcio]\n",
      "\u001b[2K    Found existing installation: gast 0.4.0m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/41\u001b[0m [grpcio]\n",
      "\u001b[2K    Uninstalling gast-0.4.0:m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/41\u001b[0m [grpcio]\n",
      "\u001b[2K      Successfully uninstalled gast-0.4.090m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/41\u001b[0m [grpcio]\n",
      "\u001b[2K  Attempting uninstall: charset_normalizer0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23/41\u001b[0m [gast]\n",
      "\u001b[2K    Found existing installation: charset-normalizer 3.3.2━━━━━\u001b[0m \u001b[32m23/41\u001b[0m [gast]\n",
      "\u001b[2K    Uninstalling charset-normalizer-3.3.2:90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23/41\u001b[0m [gast]\n",
      "\u001b[2K      Successfully uninstalled charset-normalizer-3.3.2━━━━━━━\u001b[0m \u001b[32m23/41\u001b[0m [gast]\n",
      "\u001b[2K  Attempting uninstall: certifi[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23/41\u001b[0m [gast]\n",
      "\u001b[2K    Found existing installation: certifi 2025.8.3━━━━━━━━━━━━━\u001b[0m \u001b[32m23/41\u001b[0m [gast]\n",
      "\u001b[2K    Uninstalling certifi-2025.8.3:m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23/41\u001b[0m [gast]\n",
      "\u001b[2K      Successfully uninstalled certifi-2025.8.3━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23/41\u001b[0m [gast]\n",
      "\u001b[2K  Attempting uninstall: cachetoolsm╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23/41\u001b[0m [gast]\n",
      "\u001b[2K    Found existing installation: cachetools 5.5.1━━━━━━━━━━━━━\u001b[0m \u001b[32m23/41\u001b[0m [gast]\n",
      "\u001b[2K    Uninstalling cachetools-5.5.1:m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23/41\u001b[0m [gast]\n",
      "\u001b[2K      Successfully uninstalled cachetools-5.5.1━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23/41\u001b[0m [gast]\n",
      "\u001b[2K  Attempting uninstall: absl-py[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23/41\u001b[0m [gast]\n",
      "\u001b[2K    Found existing installation: absl-py 2.1.0━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23/41\u001b[0m [gast]\n",
      "\u001b[2K    Uninstalling absl-py-2.1.0:[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23/41\u001b[0m [gast]\n",
      "\u001b[2K      Successfully uninstalled absl-py-2.1.0m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23/41\u001b[0m [gast]\n",
      "\u001b[2K  Attempting uninstall: werkzeug━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m27/41\u001b[0m [absl-py]\n",
      "\u001b[2K    Found existing installation: Werkzeug 3.1.30m━━━━━━━━━━━━━\u001b[0m \u001b[32m27/41\u001b[0m [absl-py]\n",
      "\u001b[2K    Uninstalling Werkzeug-3.1.3:0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m27/41\u001b[0m [absl-py]\n",
      "\u001b[2K      Successfully uninstalled Werkzeug-3.1.3[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m27/41\u001b[0m [absl-py]\n",
      "\u001b[2K  Attempting uninstall: rsa━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m28/41\u001b[0m [werkzeug]\n",
      "\u001b[2K    Found existing installation: rsa 4.7.2[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m28/41\u001b[0m [werkzeug]\n",
      "\u001b[2K    Uninstalling rsa-4.7.2:━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m28/41\u001b[0m [werkzeug]\n",
      "\u001b[2K      Successfully uninstalled rsa-4.7.2╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m28/41\u001b[0m [werkzeug]\n",
      "\u001b[2K  Attempting uninstall: requests[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m28/41\u001b[0m [werkzeug]\n",
      "\u001b[2K    Found existing installation: requests 2.32.40m━━━━━━━━━━━━\u001b[0m \u001b[32m28/41\u001b[0m [werkzeug]\n",
      "\u001b[2K    Uninstalling requests-2.32.4:0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m28/41\u001b[0m [werkzeug]\n",
      "\u001b[2K      Successfully uninstalled requests-2.32.4[90m━━━━━━━━━━━━\u001b[0m \u001b[32m28/41\u001b[0m [werkzeug]\n",
      "\u001b[2K  Attempting uninstall: pyasn1-modules0m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m28/41\u001b[0m [werkzeug]\n",
      "\u001b[2K    Found existing installation: pyasn1_modules 0.4.2━━━━━━━━━\u001b[0m \u001b[32m28/41\u001b[0m [werkzeug]\n",
      "\u001b[2K    Uninstalling pyasn1_modules-0.4.2:0m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m28/41\u001b[0m [werkzeug]\n",
      "\u001b[2K      Successfully uninstalled pyasn1_modules-0.4.2━━━━━━━━━━━\u001b[0m \u001b[32m28/41\u001b[0m [werkzeug]\n",
      "\u001b[2K  Attempting uninstall: ml-dtypes━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m31/41\u001b[0m [pyasn1-modules]\n",
      "\u001b[2K    Found existing installation: ml-dtypes 0.2.0\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m31/41\u001b[0m [pyasn1-modules]\n",
      "\u001b[2K    Uninstalling ml-dtypes-0.2.0:━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m32/41\u001b[0m [ml-dtypes]]\n",
      "\u001b[2K      Successfully uninstalled ml-dtypes-0.2.0[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m32/41\u001b[0m [ml-dtypes]\n",
      "\u001b[2K  Attempting uninstall: h5py━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m32/41\u001b[0m [ml-dtypes]\n",
      "\u001b[2K    Found existing installation: h5py 3.12.1╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m32/41\u001b[0m [ml-dtypes]\n",
      "\u001b[2K    Uninstalling h5py-3.12.1:━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m32/41\u001b[0m [ml-dtypes]\n",
      "\u001b[2K      Successfully uninstalled h5py-3.12.10m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m33/41\u001b[0m [h5py]]\n",
      "\u001b[2K  Attempting uninstall: google-pasta━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m33/41\u001b[0m [h5py]\n",
      "\u001b[2K    Found existing installation: google-pasta 0.2.0[90m━━━━━━━\u001b[0m \u001b[32m33/41\u001b[0m [h5py]\n",
      "\u001b[2K    Uninstalling google-pasta-0.2.0:\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m33/41\u001b[0m [h5py]\n",
      "\u001b[2K      Successfully uninstalled google-pasta-0.2.0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m33/41\u001b[0m [h5py]\n",
      "\u001b[2K  Attempting uninstall: astunparse━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m33/41\u001b[0m [h5py]\n",
      "\u001b[2K    Found existing installation: astunparse 1.6.3m\u001b[90m━━━━━━━\u001b[0m \u001b[32m33/41\u001b[0m [h5py]\n",
      "\u001b[2K    Uninstalling astunparse-1.6.3:━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m33/41\u001b[0m [h5py]\n",
      "\u001b[2K      Successfully uninstalled astunparse-1.6.3[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m33/41\u001b[0m [h5py]\n",
      "\u001b[2K  Attempting uninstall: requests-oauthlib[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m33/41\u001b[0m [h5py]\n",
      "\u001b[2K    Found existing installation: requests-oauthlib 2.0.0━━━━━━\u001b[0m \u001b[32m33/41\u001b[0m [h5py]\n",
      "\u001b[2K    Uninstalling requests-oauthlib-2.0.0:[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m33/41\u001b[0m [h5py]\n",
      "\u001b[2K      Successfully uninstalled requests-oauthlib-2.0.0[0m\u001b[90m━━━━\u001b[0m \u001b[32m36/41\u001b[0m [requests-oauthlib]\n",
      "\u001b[2K  Attempting uninstall: google-auth━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m36/41\u001b[0m [requests-oauthlib]\n",
      "\u001b[2K    Found existing installation: google-auth 2.38.00m\u001b[90m━━━━\u001b[0m \u001b[32m36/41\u001b[0m [requests-oauthlib]\n",
      "\u001b[2K    Uninstalling google-auth-2.38.0:━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m36/41\u001b[0m [requests-oauthlib]\n",
      "\u001b[2K      Successfully uninstalled google-auth-2.38.0\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m36/41\u001b[0m [requests-oauthlib]\n",
      "\u001b[2K  Attempting uninstall: google-auth-oauthlib[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m36/41\u001b[0m [requests-oauthlib]\n",
      "\u001b[2K    Found existing installation: google-auth-oauthlib 0.4.4━━━\u001b[0m \u001b[32m36/41\u001b[0m [requests-oauthlib]\n",
      "\u001b[2K    Uninstalling google-auth-oauthlib-0.4.4:━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m38/41\u001b[0m [google-auth-oauthlib]\n",
      "\u001b[2K      Successfully uninstalled google-auth-oauthlib-0.4.490m━━\u001b[0m \u001b[32m38/41\u001b[0m [google-auth-oauthlib]\n",
      "\u001b[2K  Attempting uninstall: tensorboard━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m38/41\u001b[0m [google-auth-oauthlib]\n",
      "\u001b[2K    Found existing installation: tensorboard 2.10.0\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m38/41\u001b[0m [google-auth-oauthlib]\n",
      "\u001b[2K    Uninstalling tensorboard-2.10.0:━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m38/41\u001b[0m [google-auth-oauthlib]\n",
      "\u001b[2K      Successfully uninstalled tensorboard-2.10.0m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m38/41\u001b[0m [google-auth-oauthlib]\n",
      "\u001b[2K  Attempting uninstall: tensorflow━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m39/41\u001b[0m [tensorboard]hlib]\n",
      "\u001b[2K    Found existing installation: tensorflow 2.10.0m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m39/41\u001b[0m [tensorboard]\n",
      "\u001b[2K    Uninstalling tensorflow-2.10.0:━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m40/41\u001b[0m [tensorflow]board]\n",
      "\u001b[2K      Successfully uninstalled tensorflow-2.10.0[0m\u001b[90m╺\u001b[0m \u001b[32m40/41\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41/41\u001b[0m [tensorflow]nsorflow]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.2 absl-py-2.3.1 astunparse-1.6.3 cachetools-5.5.2 certifi-2025.8.3 charset_normalizer-3.4.3 flatbuffers-25.2.10 gast-0.6.0 google-auth-2.40.3 google-auth-oauthlib-1.2.2 google-pasta-0.2.0 grpcio-1.74.0 h5py-3.14.0 idna-3.10 keras-2.15.0 libclang-18.1.1 markdown-3.8.2 ml-dtypes-0.2.0 numpy-1.24.4 oauthlib-3.3.1 opt-einsum-3.4.0 packaging-25.0 protobuf-4.25.8 pyasn1-0.6.1 pyasn1-modules-0.4.2 requests-2.32.5 requests-oauthlib-2.0.0 rsa-4.9.1 setuptools-80.9.0 six-1.17.0 tensorboard-2.15.2 tensorboard-data-server-0.7.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0 tensorflow-io-gcs-filesystem-0.37.1 termcolor-3.1.0 typing-extensions-4.14.1 urllib3-2.5.0 werkzeug-3.1.3 wheel-0.45.1 wrapt-1.14.2\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --force-reinstall numpy==1.25.2 tensorflow==2.15.0 keras==2.15.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-22 09:13:25.094098: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-22 09:13:25.406044: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-08-22 09:13:25.406265: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-08-22 09:13:25.455157: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-08-22 09:13:25.573868: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-08-22 09:13:26.939292: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os, random, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.applications import DenseNet121, ResNet50, EfficientNetB0, InceptionV3\n",
    "from tensorflow.keras.applications.densenet import preprocess_input as pre_densenet\n",
    "from tensorflow.keras.applications.resnet import preprocess_input as pre_resnet\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input as pre_efficientnet\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input as pre_inception\n",
    "from tensorflow.keras.layers import (Input, GlobalAveragePooling2D, GlobalMaxPooling2D,\n",
    "                                     Concatenate, Dense, Reshape, Multiply, Lambda)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from scipy.stats import mode\n",
    "\n",
    "# from deap import base, creator, tools  # GA removed, not needed\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration set:\n",
      "Data Directory: /teamspace/studios/this_studio/lung_cancer/dataset/lung_image_sets\n",
      "Image Size: (224, 224)\n",
      "Batch Size: 24\n",
      "Random Seed: 42\n"
     ]
    }
   ],
   "source": [
    "# Configuration and Data Setup\n",
    "DATA_DIR   = \"/teamspace/studios/this_studio/lung_cancer/dataset/lung_image_sets\"  # << set this\n",
    "IMG_SIZE   = (224, 224)\n",
    "BATCH_SIZE = 24\n",
    "SEED       = 42\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "print(f\"Configuration set:\")\n",
    "print(f\"Data Directory: {DATA_DIR}\")\n",
    "print(f\"Image Size: {IMG_SIZE}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Random Seed: {SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Global Model Hyperparameters ===\n",
    "# Number of attention heads for multi-head channel attention\n",
    "NUM_ATTENTION_HEADS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12000 images belonging to 3 classes.\n",
      "Found 3000 images belonging to 3 classes.\n",
      "Classes: {'lung_aca': 0, 'lung_n': 1, 'lung_scc': 2}\n",
      "Number of classes: 3\n",
      "Training samples: 12000\n",
      "Validation samples: 3000\n"
     ]
    }
   ],
   "source": [
    "# Data Generators Setup\n",
    "# Only lung classes will be present in this directory if you set DATA_DIR as above:\n",
    "# expected subfolders: lung_aca / lung_n / lung_scc\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    validation_split=0.20,\n",
    "    rotation_range=20,\n",
    "    horizontal_flip=True,\n",
    "    # IMPORTANT: no rescale here, since we feed raw to model-specific preprocessors\n",
    ")\n",
    "\n",
    "def make_gen(subset):\n",
    "    return train_datagen.flow_from_directory(\n",
    "        DATA_DIR,\n",
    "        target_size=IMG_SIZE,\n",
    "        class_mode='categorical',\n",
    "        batch_size=BATCH_SIZE,\n",
    "        subset=subset,\n",
    "        seed=SEED,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "train_gen = make_gen('training')\n",
    "val_gen   = make_gen('validation')\n",
    "num_classes = train_gen.num_classes\n",
    "class_indices = train_gen.class_indices\n",
    "id2label = {v:k for k,v in class_indices.items()}\n",
    "\n",
    "print(\"Classes:\", class_indices)\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Training samples: {train_gen.samples}\")\n",
    "print(f\"Validation samples: {val_gen.samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-head attention block function defined successfully!\n"
     ]
    }
   ],
   "source": [
    "# Channel Attention (Multi-Headed) Implementation\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Dense, Reshape, Permute, Concatenate\n",
    "\n",
    "class MultiHeadChannelAttention(Layer):\n",
    "    def __init__(self, num_heads=4, reduction=16, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.channel = input_shape[-1]\n",
    "        self.dense1 = [Dense(self.channel // self.reduction, activation='relu') for _ in range(self.num_heads)]\n",
    "        self.dense2 = [Dense(self.channel) for _ in range(self.num_heads)]\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        # Global pooling\n",
    "        gap = tf.reduce_mean(x, axis=[1,2])  # shape: (batch, channels)\n",
    "        gmp = tf.reduce_max(x, axis=[1,2])   # shape: (batch, channels)\n",
    "        heads = []\n",
    "        for i in range(self.num_heads):\n",
    "            d1_gap = self.dense1[i](gap)\n",
    "            d1_gmp = self.dense1[i](gmp)\n",
    "            d2_gap = self.dense2[i](d1_gap)\n",
    "            d2_gmp = self.dense2[i](d1_gmp)\n",
    "            scale = tf.nn.sigmoid(d2_gap + d2_gmp)\n",
    "            scale = Reshape((1,1,self.channel))(scale)\n",
    "            heads.append(x * scale)\n",
    "        # Concatenate heads along channel axis\n",
    "        out = Concatenate(axis=-1)(heads)\n",
    "        return out\n",
    "\n",
    "# Usage in your lane function:\n",
    "# from multi_head_attention import MultiHeadChannelAttention\n",
    "# x = MultiHeadChannelAttention(num_heads=4, reduction=16)(x)\n",
    "\n",
    "def multi_head_attention_block(x, reduction=16, name=None):\n",
    "    \"\"\"Multi-Headed Channel Attention block for CNN feature maps\"\"\"\n",
    "    attn = MultiHeadChannelAttention(num_heads=NUM_ATTENTION_HEADS, reduction=reduction, name=name)(x)\n",
    "    return attn\n",
    "\n",
    "print(\"Multi-head attention block function defined successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lane function updated for multi-head attention!\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing Lanes (one per backbone)\n",
    "def lane(tensor, backbone=\"resnet\", reduction=16):\n",
    "    \"\"\"Create a processing lane for each CNN backbone with multi-head channel attention\"\"\"\n",
    "    if backbone == \"resnet\":\n",
    "        x = Lambda(pre_resnet, name=\"pre_resnet\")(tensor)\n",
    "        x = ResNet50(include_top=False, weights='imagenet')(x)\n",
    "    elif backbone == \"densenet\":\n",
    "        x = Lambda(pre_densenet, name=\"pre_densenet\")(tensor)\n",
    "        x = DenseNet121(include_top=False, weights='imagenet')(x)\n",
    "    elif backbone == \"efficientnet\":\n",
    "        x = Lambda(pre_efficientnet, name=\"pre_efficientnet\")(tensor)\n",
    "        x = EfficientNetB0(include_top=False, weights='imagenet')(x)\n",
    "    elif backbone == \"inception\":\n",
    "        x = Lambda(pre_inception, name=\"pre_inception\")(tensor)\n",
    "        x = InceptionV3(include_top=False, weights='imagenet')(x)\n",
    "    else:\n",
    "        raise ValueError(f'Unknown backbone: {backbone}')\n",
    "    # Add multi-head channel attention\n",
    "    x = multi_head_attention_block(x, reduction=reduction, name=f\"mhca_{backbone}\")\n",
    "    # Global Average Pooling to convert feature maps → vector\n",
    "    x = GlobalAveragePooling2D(name=f\"gap_{backbone}\")(x)\n",
    "    return x\n",
    "\n",
    "print(\"Lane function updated for multi-head attention!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building multi-backbone feature concatenator with multi-head attention...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-22 09:13:52.407529: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20763 MB memory:  -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:04.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extractor built successfully!\n",
      "Feature dimension: 28672\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " pre_densenet (Lambda)       (None, 224, 224, 3)          0         ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " pre_resnet (Lambda)         (None, 224, 224, 3)          0         ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " pre_vgg (Lambda)            (None, 224, 224, 3)          0         ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " densenet121 (Functional)    (None, None, None, 1024)     7037504   ['pre_densenet[0][0]']        \n",
      "                                                                                                  \n",
      " resnet50 (Functional)       (None, None, None, 2048)     2358771   ['pre_resnet[0][0]']          \n",
      "                                                          2                                       \n",
      "                                                                                                  \n",
      " vgg16 (Functional)          (None, None, None, 512)      1471468   ['pre_vgg[0][0]']             \n",
      "                                                          8                                       \n",
      "                                                                                                  \n",
      " mhca_densenet (MultiHeadCh  (None, 7, 7, 8192)           1057280   ['densenet121[0][0]']         \n",
      " annelAttention)                                                                                  \n",
      "                                                                                                  \n",
      " mhca_resnet (MultiHeadChan  (None, 7, 7, 16384)          4211712   ['resnet50[0][0]']            \n",
      " nelAttention)                                                                                    \n",
      "                                                                                                  \n",
      " mhca_vgg (MultiHeadChannel  (None, 7, 7, 4096)           266496    ['vgg16[0][0]']               \n",
      " Attention)                                                                                       \n",
      "                                                                                                  \n",
      " gap_densenet (GlobalAverag  (None, 8192)                 0         ['mhca_densenet[0][0]']       \n",
      " ePooling2D)                                                                                      \n",
      "                                                                                                  \n",
      " gap_resnet (GlobalAverageP  (None, 16384)                0         ['mhca_resnet[0][0]']         \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " gap_vgg (GlobalAveragePool  (None, 4096)                 0         ['mhca_vgg[0][0]']            \n",
      " ing2D)                                                                                           \n",
      "                                                                                                  \n",
      " concat_feats (Concatenate)  (None, 28672)                0         ['gap_densenet[0][0]',        \n",
      "                                                                     'gap_resnet[0][0]',          \n",
      "                                                                     'gap_vgg[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 50875392 (194.07 MB)\n",
      "Trainable params: 50738624 (193.55 MB)\n",
      "Non-trainable params: 136768 (534.25 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build Feature Extractor Model\n",
    "print(\"Building multi-backbone feature concatenator with multi-head attention...\")\n",
    "\n",
    "# Define input tensor with image size (224x224x3 RGB)\n",
    "inp = Input(shape=(224,224,3))\n",
    "\n",
    "# Extract features from DenseNet lane (multi-head attention)\n",
    "feat_d = lane(inp, \"densenet\", reduction=16)\n",
    "# Extract features from ResNet lane (multi-head attention)\n",
    "feat_r = lane(inp, \"resnet\", reduction=16)\n",
    "# Extract features from EfficientNetB0 lane (multi-head attention)\n",
    "feat_e = lane(inp, \"efficientnet\", reduction=16)\n",
    "# Extract features from InceptionV3 lane (multi-head attention)\n",
    "feat_i = lane(inp, \"inception\", reduction=16)\n",
    "\n",
    "# Concatenate features from all four backbones\n",
    "concat_feat = Concatenate(name=\"concat_feats\")([feat_d, feat_r, feat_e, feat_i])\n",
    "\n",
    "# Create feature extractor model (input → concatenated features)\n",
    "feature_model = Model(inp, concat_feat)\n",
    "\n",
    "# Get final concatenated feature dimension\n",
    "feature_dim = feature_model.output_shape[-1]\n",
    "\n",
    "print(f\"Feature extractor built successfully!\")\n",
    "print(f\"Feature dimension: {feature_dim}\")\n",
    "\n",
    "# Show model summary (layers, parameters, shapes)\n",
    "feature_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction function defined!\n"
     ]
    }
   ],
   "source": [
    "# Extract Deep Features\n",
    "def extract_features(generator):\n",
    "    \"\"\"Extract features from a data generator using the feature model\"\"\"\n",
    "    X, y = [], []\n",
    "    steps = len(generator)\n",
    "    for i in range(steps):\n",
    "        imgs, labels = generator.next()\n",
    "        feats = feature_model.predict(imgs, verbose=0)\n",
    "        X.append(feats)\n",
    "        y.append(labels)\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Processed {i + 1}/{steps} batches\")\n",
    "    return np.vstack(X), np.vstack(y)\n",
    "\n",
    "print(\"Feature extraction function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting training features …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-22 09:14:08.944367: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10/500 batches\n",
      "Processed 20/500 batches\n",
      "Processed 30/500 batches\n",
      "Processed 40/500 batches\n",
      "Processed 50/500 batches\n",
      "Processed 60/500 batches\n",
      "Processed 70/500 batches\n",
      "Processed 80/500 batches\n",
      "Processed 90/500 batches\n",
      "Processed 100/500 batches\n",
      "Processed 110/500 batches\n",
      "Processed 120/500 batches\n",
      "Processed 130/500 batches\n",
      "Processed 140/500 batches\n",
      "Processed 150/500 batches\n",
      "Processed 160/500 batches\n",
      "Processed 170/500 batches\n",
      "Processed 180/500 batches\n",
      "Processed 190/500 batches\n",
      "Processed 200/500 batches\n",
      "Processed 210/500 batches\n",
      "Processed 220/500 batches\n",
      "Processed 230/500 batches\n",
      "Processed 240/500 batches\n",
      "Processed 250/500 batches\n",
      "Processed 260/500 batches\n",
      "Processed 270/500 batches\n",
      "Processed 280/500 batches\n",
      "Processed 290/500 batches\n",
      "Processed 300/500 batches\n",
      "Processed 310/500 batches\n",
      "Processed 320/500 batches\n",
      "Processed 330/500 batches\n",
      "Processed 340/500 batches\n",
      "Processed 350/500 batches\n",
      "Processed 360/500 batches\n",
      "Processed 370/500 batches\n",
      "Processed 380/500 batches\n",
      "Processed 390/500 batches\n",
      "Processed 400/500 batches\n",
      "Processed 410/500 batches\n",
      "Processed 420/500 batches\n",
      "Processed 430/500 batches\n",
      "Processed 440/500 batches\n",
      "Processed 450/500 batches\n",
      "Processed 460/500 batches\n",
      "Processed 470/500 batches\n",
      "Processed 480/500 batches\n",
      "Processed 490/500 batches\n",
      "Processed 500/500 batches\n",
      "Training features shape: (12000, 28672)\n",
      "Training labels shape: (12000, 3)\n"
     ]
    }
   ],
   "source": [
    "# Feature Selection: mRMR + Adaptive Grey Wolf Optimization (AGWO)\n",
    "# Remove GA-based feature selection. Use mRMR for initial ranking, then AGWO for final selection.\n",
    "# You may need to install pymrmr and a Grey Wolf Optimizer package, or implement AGWO manually.\n",
    "# Example below assumes features (X) and labels (y) are available after extraction.\n",
    "\n",
    "## 1. mRMR Feature Ranking\n",
    "try:\n",
    "    import pymrmr\n",
    "except ImportError:\n",
    "    !pip install pymrmr\n",
    "    import pymrmr\n",
    "\n",
    "# Convert features and labels to DataFrame for pymrmr\n",
    "import pandas as pd\n",
    "def mrmr_feature_selection(X, y, n_features=100):\n",
    "    df = pd.DataFrame(X)\n",
    "    df['target'] = np.argmax(y, axis=1)\n",
    "    selected = pymrmr.mRMR(df, 'MIQ', n_features)\n",
    "    return selected\n",
    "\n",
    "# Example usage:\n",
    "## selected_features = mrmr_feature_selection(X_train, y_train, n_features=100)\n",
    "\n",
    "## 2. Adaptive Grey Wolf Optimization (AGWO)\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def agwo_feature_selection(X, y, ranked_features, n_wolves=10, n_iter=10, subset_size=30):\n",
    "    # Only use the top ranked features from mRMR\n",
    "    feature_indices = [int(f) for f in ranked_features]\n",
    "    X_ranked = X[:, feature_indices]\n",
    "    n_features = len(feature_indices)\n",
    "    # Initialize wolves (random binary vectors)\n",
    "    wolves = np.random.randint(0, 2, size=(n_wolves, n_features))\n",
    "    # Ensure at least subset_size features are selected\n",
    "    for w in wolves:\n",
    "        idx = np.where(w == 1)[0]\n",
    "        if len(idx) < subset_size:\n",
    "            w[:subset_size] = 1\n",
    "    alpha, beta, delta = None, None, None\n",
    "    alpha_score, beta_score, delta_score = -np.inf, -np.inf, -np.inf\n",
    "\n",
    "    for it in range(n_iter):\n",
    "        for i, wolf in enumerate(wolves):\n",
    "            idx = np.where(wolf == 1)[0]\n",
    "            if len(idx) == 0: continue\n",
    "            X_sel = X_ranked[:, idx]\n",
    "            # Use KNN accuracy as fitness\n",
    "            score = cross_val_score(KNeighborsClassifier(n_neighbors=5), X_sel, np.argmax(y, axis=1), cv=3).mean()\n",
    "            if score > alpha_score:\n",
    "                delta, delta_score = beta, beta_score\n",
    "                beta, beta_score = alpha, alpha_score\n",
    "                alpha, alpha_score = wolf.copy(), score\n",
    "            elif score > beta_score:\n",
    "                delta, delta_score = beta, beta_score\n",
    "                beta, beta_score = wolf.copy(), score\n",
    "            elif score > delta_score:\n",
    "                delta, delta_score = wolf.copy(), score\n",
    "        # Update wolves positions (simplified AGWO)\n",
    "        for i, wolf in enumerate(wolves):\n",
    "            for j in range(n_features):\n",
    "                r = np.random.rand()\n",
    "                if r < 0.33:\n",
    "                    wolf[j] = alpha[j]\n",
    "                elif r < 0.66:\n",
    "                    wolf[j] = beta[j]\n",
    "                else:\n",
    "                    wolf[j] = delta[j]\n",
    "            # Random mutation\n",
    "            if np.random.rand() < 0.1:\n",
    "                flip = np.random.randint(0, n_features)\n",
    "                wolf[flip] = 1 - wolf[flip]\n",
    "    # Return indices of selected features from the best wolf (alpha)\n",
    "    selected_idx = np.where(alpha == 1)[0]\n",
    "    selected_features = [feature_indices[i] for i in selected_idx]\n",
    "    return selected_features\n",
    "\n",
    "print(\"Feature selection pipeline (mRMR + AGWO) implemented.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting validation features …\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10/125 batches\n",
      "Processed 20/125 batches\n",
      "Processed 30/125 batches\n",
      "Processed 40/125 batches\n",
      "Processed 50/125 batches\n",
      "Processed 60/125 batches\n",
      "Processed 70/125 batches\n",
      "Processed 80/125 batches\n",
      "Processed 90/125 batches\n",
      "Processed 100/125 batches\n",
      "Processed 110/125 batches\n",
      "Processed 120/125 batches\n",
      "Validation features shape: (3000, 28672)\n",
      "Validation labels shape: (3000, 3)\n"
     ]
    }
   ],
   "source": [
    "# Extract Validation Features\n",
    "print(\"Extracting validation features …\")\n",
    "X_va, Y_va_ohe = extract_features(val_gen)\n",
    "print(f\"Validation features shape: {X_va.shape}\")\n",
    "print(f\"Validation labels shape: {Y_va_ohe.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features shape: (15000, 28672)\n",
      "Total labels shape: (15000,)\n",
      "Classes present: [0 1 2]\n",
      "Class distribution: [5000 5000 5000]\n"
     ]
    }
   ],
   "source": [
    "# Combine Features and Convert Labels\n",
    "X_full = np.vstack([X_tr, X_va])\n",
    "y_full = np.argmax(np.vstack([Y_tr_ohe, Y_va_ohe]), axis=1)\n",
    "\n",
    "print(f\"Total features shape: {X_full.shape}\")\n",
    "print(f\"Total labels shape: {y_full.shape}\")\n",
    "print(f\"Classes present: {np.unique(y_full)}\")\n",
    "print(f\"Class distribution: {np.bincount(y_full)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GA Parameters:\n",
      "Population Size: 40\n",
      "Generations: 10\n",
      "Crossover Probability: 0.8\n",
      "Mutation Probability: 0.1\n",
      "Total Features: 28672\n"
     ]
    }
   ],
   "source": [
    "# GA-based Feature Selection Setup (DEAP)\n",
    "POP_SIZE = 40\n",
    "N_GEN    = 10        # start smaller; increase later\n",
    "CX_PROB  = 0.8\n",
    "MUT_PROB = 0.1\n",
    "INDPB    = 0.05\n",
    "\n",
    "n_features = X_full.shape[1]\n",
    "\n",
    "print(f\"GA Parameters:\")\n",
    "print(f\"Population Size: {POP_SIZE}\")\n",
    "print(f\"Generations: {N_GEN}\")\n",
    "print(f\"Crossover Probability: {CX_PROB}\")\n",
    "print(f\"Mutation Probability: {MUT_PROB}\")\n",
    "print(f\"Total Features: {n_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GA components defined successfully!\n"
     ]
    }
   ],
   "source": [
    "# Define GA Components\n",
    "# Safe (re)definition guards for repeated runs\n",
    "if \"FitnessMax\" not in creator.__dict__:\n",
    "    creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "if \"Individual\" not in creator.__dict__:\n",
    "    creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"attr_bool\", random.randint, 0, 1)\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, n_features)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "def eval_fitness(individual):\n",
    "    \"\"\"Evaluate fitness of an individual (feature subset)\"\"\"\n",
    "    idx = [i for i, b in enumerate(individual) if b == 1]\n",
    "    if len(idx) < 2:\n",
    "        return (0.0,)\n",
    "    Xs = X_full[:, idx]\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    scores = cross_val_score(knn, Xs, y_full, cv=3, scoring='accuracy')\n",
    "    # Small L0 penalty to prefer compact subsets\n",
    "    fitness = scores.mean() - 0.1 * (len(idx) / n_features)\n",
    "    return (float(fitness),)\n",
    "\n",
    "toolbox.register(\"evaluate\", eval_fitness)\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=INDPB)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "\n",
    "print(\"GA components defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GA initialized: pop=40, feats=28672\n",
      "Starting genetic algorithm evolution...\n"
     ]
    }
   ],
   "source": [
    "# Initialize GA Population\n",
    "pop = toolbox.population(n=POP_SIZE)\n",
    "print(f\"GA initialized: pop={POP_SIZE}, feats={n_features}\")\n",
    "print(\"Starting genetic algorithm evolution...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Evaluating 40 individuals...\n",
      "  Max fitness: 0.9265\n",
      "  Avg fitness: 0.9224\n",
      "\n",
      "Generation 2/10\n",
      "  Evaluating 29 individuals...\n",
      "  Max fitness: 0.9265\n",
      "  Avg fitness: 0.9244\n",
      "\n",
      "Generation 3/10\n",
      "  Evaluating 33 individuals...\n",
      "  Max fitness: 0.9271\n",
      "  Avg fitness: 0.9258\n",
      "\n",
      "Generation 4/10\n",
      "  Evaluating 33 individuals...\n",
      "  Max fitness: 0.9274\n",
      "  Avg fitness: 0.9265\n",
      "\n",
      "Generation 5/10\n",
      "  Evaluating 36 individuals...\n",
      "  Max fitness: 0.9280\n",
      "  Avg fitness: 0.9268\n",
      "\n",
      "Generation 6/10\n",
      "  Evaluating 25 individuals...\n",
      "  Max fitness: 0.9285\n",
      "  Avg fitness: 0.9272\n",
      "\n",
      "Generation 7/10\n",
      "  Evaluating 32 individuals...\n",
      "  Max fitness: 0.9289\n",
      "  Avg fitness: 0.9276\n",
      "\n",
      "Generation 8/10\n",
      "  Evaluating 30 individuals...\n",
      "  Max fitness: 0.9292\n",
      "  Avg fitness: 0.9281\n",
      "\n",
      "Generation 9/10\n",
      "  Evaluating 31 individuals...\n",
      "  Max fitness: 0.9298\n",
      "  Avg fitness: 0.9283\n",
      "\n",
      "Generation 10/10\n",
      "  Evaluating 36 individuals...\n",
      "  Max fitness: 0.9304\n",
      "  Avg fitness: 0.9288\n",
      "\n",
      "GA evolution completed!\n"
     ]
    }
   ],
   "source": [
    "# Run GA Evolution\n",
    "for gen in range(N_GEN):\n",
    "    print(f\"\\nGeneration {gen+1}/{N_GEN}\")\n",
    "    \n",
    "    offspring = toolbox.select(pop, len(pop))\n",
    "    offspring = list(map(toolbox.clone, offspring))\n",
    "\n",
    "    # Crossover\n",
    "    for c1, c2 in zip(offspring[::2], offspring[1::2]):\n",
    "        if random.random() < CX_PROB:\n",
    "            toolbox.mate(c1, c2)\n",
    "            if \"fitness\" in c1.__dict__: del c1.fitness.values\n",
    "            if \"fitness\" in c2.__dict__: del c2.fitness.values\n",
    "\n",
    "    # Mutation\n",
    "    for ind in offspring:\n",
    "        if random.random() < MUT_PROB:\n",
    "            toolbox.mutate(ind)\n",
    "            if \"fitness\" in ind.__dict__: del ind.fitness.values\n",
    "\n",
    "    # Evaluation\n",
    "    invalid = [ind for ind in offspring if not ind.fitness.valid]\n",
    "    print(f\"  Evaluating {len(invalid)} individuals...\")\n",
    "    fits = list(map(toolbox.evaluate, invalid))\n",
    "    for ind, fit in zip(invalid, fits):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    pop[:] = offspring\n",
    "    gen_fits = [ind.fitness.values[0] for ind in pop]\n",
    "    print(f\"  Max fitness: {np.max(gen_fits):.4f}\")\n",
    "    print(f\"  Avg fitness: {np.mean(gen_fits):.4f}\")\n",
    "\n",
    "print(\"\\nGA evolution completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 14207 / 28672 features\n",
      "Feature selection ratio: 0.496\n",
      "Best fitness: 0.9304\n"
     ]
    }
   ],
   "source": [
    "# Select Best Features\n",
    "best = tools.selBest(pop, 1)[0]\n",
    "sel_idx = np.array([i for i, b in enumerate(best) if b == 1], dtype=int)\n",
    "\n",
    "print(f\"Selected {len(sel_idx)} / {n_features} features\")\n",
    "print(f\"Feature selection ratio: {len(sel_idx)/n_features:.3f}\")\n",
    "print(f\"Best fitness: {best.fitness.values[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (12000, 14207)\n",
      "Test set shape: (3000, 14207)\n",
      "Training class distribution: [4000 4000 4000]\n",
      "Test class distribution: [1000 1000 1000]\n"
     ]
    }
   ],
   "source": [
    "# Prepare Selected Features for Training\n",
    "X_sel = X_full[:, sel_idx]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_sel, y_full, test_size=0.20, random_state=SEED, stratify=y_full\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"Training class distribution: {np.bincount(y_train)}\")\n",
    "print(f\"Test class distribution: {np.bincount(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifiers initialized:\n",
      "  KNN: k=5, weights='distance'\n",
      "  SVM: RBF kernel, C=1.0, gamma='scale'\n",
      "  Random Forest: 300 trees\n"
     ]
    }
   ],
   "source": [
    "# Initialize Classifiers\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
    "svm = SVC(kernel='rbf', probability=True, C=1.0, gamma='scale', random_state=SEED)\n",
    "rf  = RandomForestClassifier(n_estimators=300, random_state=SEED, n_jobs=-1)\n",
    "xgb = XGBClassifier(n_estimators=200, random_state=SEED, use_label_encoder=False, eval_metric='mlogloss')\n",
    "lr  = LogisticRegression(max_iter=1000, random_state=SEED, n_jobs=-1)\n",
    "\n",
    "print(\"Classifiers initialized:\")\n",
    "print(f\"  KNN: k=5, weights='distance'\")\n",
    "print(f\"  SVM: RBF kernel, C=1.0, gamma='scale'\")\n",
    "print(f\"  Random Forest: 300 trees\")\n",
    "print(f\"  XGBoost: 200 estimators\")\n",
    "print(f\"  Logistic Regression: max_iter=1000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifiers …\n",
      "  Training KNN...\n",
      "  Training SVM...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training Random Forest...\n",
      "All classifiers trained successfully!\n"
     ]
    }
   ],
   "source": [
    "# Train Classifiers\n",
    "print(\"Training classifiers …\")\n",
    "\n",
    "print(\"  Training KNN...\")\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "print(\"  Training SVM...\")\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "print(\"  Training Random Forest...\")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "print(\"  Training XGBoost...\")\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "print(\"  Training Logistic Regression...\")\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "print(\"All classifiers trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions completed!\n"
     ]
    }
   ],
   "source": [
    "# Make Predictions\n",
    "print(\"Making predictions...\")\n",
    "\n",
    "knn_pred = knn.predict(X_test)\n",
    "svm_pred = svm.predict(X_test)\n",
    "rf_pred  = rf.predict(X_test)\n",
    "xgb_pred = xgb.predict(X_test)\n",
    "lr_pred  = lr.predict(X_test)\n",
    "\n",
    "# Probabilistic predictions (for ensemble if needed)\n",
    "knn_proba = knn.predict_proba(X_test) if hasattr(knn, 'predict_proba') else None\n",
    "svm_proba = svm.predict_proba(X_test) if hasattr(svm, 'predict_proba') else None\n",
    "rf_proba  = rf.predict_proba(X_test) if hasattr(rf, 'predict_proba') else None\n",
    "xgb_proba = xgb.predict_proba(X_test) if hasattr(xgb, 'predict_proba') else None\n",
    "lr_proba  = lr.predict_proba(X_test) if hasattr(lr, 'predict_proba') else None\n",
    "\n",
    "print(\"Predictions completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual Classifier Accuracies:\n",
      "  KNN: 0.9843\n",
      "  SVM: 0.9817\n",
      "  RF : 0.9753\n",
      "\n",
      "=== KNN Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    lung_aca       0.99      0.96      0.98      1000\n",
      "      lung_n       1.00      1.00      1.00      1000\n",
      "    lung_scc       0.96      0.99      0.98      1000\n",
      "\n",
      "    accuracy                           0.98      3000\n",
      "   macro avg       0.98      0.98      0.98      3000\n",
      "weighted avg       0.98      0.98      0.98      3000\n",
      "\n",
      "\n",
      "=== SVM Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    lung_aca       0.98      0.96      0.97      1000\n",
      "      lung_n       1.00      1.00      1.00      1000\n",
      "    lung_scc       0.96      0.98      0.97      1000\n",
      "\n",
      "    accuracy                           0.98      3000\n",
      "   macro avg       0.98      0.98      0.98      3000\n",
      "weighted avg       0.98      0.98      0.98      3000\n",
      "\n",
      "\n",
      "=== Random Forest Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    lung_aca       0.97      0.96      0.96      1000\n",
      "      lung_n       1.00      1.00      1.00      1000\n",
      "    lung_scc       0.96      0.97      0.96      1000\n",
      "\n",
      "    accuracy                           0.98      3000\n",
      "   macro avg       0.98      0.98      0.98      3000\n",
      "weighted avg       0.98      0.98      0.98      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Individual Classifier Results\n",
    "print(\"Individual Classifier Accuracies:\")\n",
    "knn_acc = accuracy_score(y_test, knn_pred)\n",
    "svm_acc = accuracy_score(y_test, svm_pred)\n",
    "rf_acc = accuracy_score(y_test, rf_pred)\n",
    "xgb_acc = accuracy_score(y_test, xgb_pred)\n",
    "lr_acc = accuracy_score(y_test, lr_pred)\n",
    "\n",
    "print(f\"  KNN: {knn_acc:.4f}\")\n",
    "print(f\"  SVM: {svm_acc:.4f}\")\n",
    "print(f\"  RF : {rf_acc:.4f}\")\n",
    "print(f\"  XGB: {xgb_acc:.4f}\")\n",
    "print(f\"  LR : {lr_acc:.4f}\")\n",
    "\n",
    "# Display individual classification reports\n",
    "target_names = [id2label[i] for i in range(num_classes)]\n",
    "\n",
    "print(\"\\n=== KNN Classification Report ===\")\n",
    "print(classification_report(y_test, knn_pred, target_names=target_names))\n",
    "\n",
    "print(\"\\n=== SVM Classification Report ===\")\n",
    "print(classification_report(y_test, svm_pred, target_names=target_names))\n",
    "\n",
    "print(\"\\n=== Random Forest Classification Report ===\")\n",
    "print(classification_report(y_test, rf_pred, target_names=target_names))\n",
    "\n",
    "print(\"\\n=== XGBoost Classification Report ===\")\n",
    "print(classification_report(y_test, xgb_pred, target_names=target_names))\n",
    "\n",
    "print(\"\\n=== Logistic Regression Classification Report ===\")\n",
    "print(classification_report(y_test, lr_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Accuracy (Majority Voting): 0.9870\n",
      "\n",
      "Improvement over best individual: 0.0027\n",
      "\n",
      "=== Ensemble Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    lung_aca       0.99      0.97      0.98      1000\n",
      "      lung_n       1.00      1.00      1.00      1000\n",
      "    lung_scc       0.97      0.99      0.98      1000\n",
      "\n",
      "    accuracy                           0.99      3000\n",
      "   macro avg       0.99      0.99      0.99      3000\n",
      "weighted avg       0.99      0.99      0.99      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ensemble Fusion (Priority-Based Strategy)\n",
    "# Priority: SVM > XGBoost > RF > KNN > LR\n",
    "# If SVM and XGBoost agree, use that prediction. Else, use SVM. If not, use XGBoost. Else, fallback to majority vote.\n",
    "def priority_ensemble(svm_pred, xgb_pred, rf_pred, knn_pred, lr_pred):\n",
    "    preds = np.stack([knn_pred, svm_pred, rf_pred, xgb_pred, lr_pred], axis=0)\n",
    "    final = []\n",
    "    for i in range(svm_pred.shape[0]):\n",
    "        if svm_pred[i] == xgb_pred[i]:\n",
    "            final.append(svm_pred[i])\n",
    "        elif svm_pred[i] == rf_pred[i]:\n",
    "            final.append(svm_pred[i])\n",
    "        elif xgb_pred[i] == rf_pred[i]:\n",
    "            final.append(xgb_pred[i])\n",
    "        else:\n",
    "            # fallback to majority vote\n",
    "            vals, counts = np.unique(preds[:, i], return_counts=True)\n",
    "            final.append(vals[np.argmax(counts)])\n",
    "    return np.array(final)\n",
    "\n",
    "ens = priority_ensemble(svm_pred, xgb_pred, rf_pred, knn_pred, lr_pred)\n",
    "ens_acc = accuracy_score(y_test, ens)\n",
    "\n",
    "print(f\"Ensemble Accuracy (Priority-Based): {ens_acc:.4f}\")\n",
    "print(f\"\\nImprovement over best individual: {ens_acc - max(knn_acc, svm_acc, rf_acc, xgb_acc, lr_acc):.4f}\")\n",
    "\n",
    "print(\"\\n=== Ensemble Classification Report ===\")\n",
    "print(classification_report(y_test, ens, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL RESULTS SUMMARY\n",
      "============================================================\n",
      "Total samples processed: 15000\n",
      "Features selected by GA: 14207 / 28672 (49.6%)\n",
      "Test set size: 3000\n",
      "\n",
      "Classifier Accuracies:\n",
      "  KNN:              0.9843\n",
      "  SVM:              0.9817\n",
      "  Random Forest:    0.9753\n",
      "  Ensemble (Fusion): 0.9870 ← BEST\n",
      "\n",
      "Class Labels:\n",
      "  0: lung_aca\n",
      "  1: lung_n\n",
      "  2: lung_scc\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Summary Results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total samples processed: {len(y_full)}\")\n",
    "print(f\"Features selected by GA: {len(sel_idx)} / {n_features} ({len(sel_idx)/n_features:.1%})\")\n",
    "print(f\"Test set size: {len(y_test)}\")\n",
    "print(\"\\nClassifier Accuracies:\")\n",
    "print(f\"  KNN:              {knn_acc:.4f}\")\n",
    "print(f\"  SVM:              {svm_acc:.4f}\")\n",
    "print(f\"  Random Forest:    {rf_acc:.4f}\")\n",
    "print(f\"  Ensemble (Fusion): {ens_acc:.4f} ← BEST\")\n",
    "print(\"\\nClass Labels:\")\n",
    "for i, label in id2label.items():\n",
    "    print(f\"  {i}: {label}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

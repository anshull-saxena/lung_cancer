{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lung Histopathology Classification: ACA / N / SCC\n",
    "## Multi-CNN + Channel Attention + GA + KNN/SVM/RF + Fusion\n",
    "\n",
    "This notebook implements a comprehensive lung histopathology classification system that combines:\n",
    "- Multiple CNN backbones (DenseNet121, ResNet50, VGG16)\n",
    "- Channel attention mechanism (SE blocks)\n",
    "- Genetic Algorithm for feature selection\n",
    "- Ensemble of classical ML classifiers (KNN, SVM, Random Forest)\n",
    "- Majority voting fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "# NOTE: Do not install requirements from inside the notebook.\n",
    "# Installing here can overwrite your GPU-enabled TensorFlow with a CPU build.\n",
    "# Instead, install from a terminal in your Lightning AI workspace and then restart the kernel.\n",
    "print(\"Skipping in-notebook package installation. Use terminal: pip install -r requirements.txt, then Restart Kernel.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import (Input, GlobalAveragePooling2D, GlobalMaxPooling2D,\n",
    "                                     Concatenate, Dense, Reshape, Multiply, Lambda)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.applications import DenseNet121, ResNet50, EfficientNetB0, InceptionV3\n",
    "from tensorflow.keras.applications.densenet import preprocess_input as pre_densenet\n",
    "from tensorflow.keras.applications.resnet import preprocess_input as pre_resnet\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input as pre_efficientnet\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input as pre_inception\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸ” DEVICE DETECTION & CONFIGURATION\n",
      "======================================================================\n",
      "TensorFlow version: 2.16.1\n",
      "Built with CUDA (Linux only): False\n",
      "\n",
      "ğŸ“± Physical GPU devices: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "âœ… 1 device(s) available (CUDA or Metal)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-31 06:39:24.589833: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2025-10-31 06:39:24.589881: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2025-10-31 06:39:24.589901: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2025-10-31 06:39:24.590191: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-10-31 06:39:24.590223: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Device test computation shape: (1024, 1024)\n",
      "\n",
      "ğŸ¯ Logical GPU devices: [LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# GPU / MPS Detection and Configuration (Linux CUDA or macOS Metal)\n",
    "import tensorflow as tf, platform\n",
    "is_macos = platform.system() == 'Darwin'\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ” DEVICE DETECTION & CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Built with CUDA (Linux only): {tf.test.is_built_with_cuda()}\")\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(f\"\\nğŸ“± Physical GPU devices: {gpus}\")\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        # Memory growth applies on CUDA; on macOS/Metal it may be a no-op\n",
    "        for gpu in gpus:\n",
    "            try:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            except Exception:\n",
    "                pass\n",
    "        print(f\"âœ… {len(gpus)} device(s) available (CUDA or Metal)\")\n",
    "        # Test matmul on device 0\n",
    "        with tf.device('/GPU:0'):\n",
    "            a = tf.random.uniform((1024,1024))\n",
    "            b = tf.random.uniform((1024,1024))\n",
    "            c = tf.matmul(a, b)\n",
    "        print(\"âœ… Device test computation shape:\", c.shape)\n",
    "    except RuntimeError as e:\n",
    "        print(f\"âš ï¸ Device configuration warning: {e}\")\n",
    "else:\n",
    "    print(\"âŒ NO ACCELERATOR DETECTED (running on CPU)\")\n",
    "    if is_macos:\n",
    "        print(\"If you expect Apple GPU acceleration, install:\")\n",
    "        print(\"  pip install 'tensorflow-macos==2.16.1' 'tensorflow-metal==1.1.0'\")\n",
    "    else:\n",
    "        print(\"If you expect CUDA GPU, install:\")\n",
    "        print(\"  pip install 'tensorflow[and-cuda]==2.16.1'\")\n",
    "\n",
    "logical = tf.config.list_logical_devices('GPU')\n",
    "print(f\"\\nğŸ¯ Logical GPU devices: {logical}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” NVIDIA/CUDA Diagnostic\n",
      "\n",
      "âŒ nvidia-smi not found - NVIDIA driver may not be installed\n",
      "\n",
      "ğŸ” Checking CUDA libraries...\n",
      "âŒ libcudart.so not found\n",
      "âŒ libcublas.so not found\n",
      "\n",
      "ğŸ’¡ If CUDA libraries are missing, install tensorflow[and-cuda]:\n",
      "   !pip install --upgrade tensorflow[and-cuda]==2.15.0\n",
      "   Then RESTART THE KERNEL\n"
     ]
    }
   ],
   "source": [
    "# Quick diagnostic - Run this if GPU not detected\n",
    "# Check NVIDIA driver and CUDA availability\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"ğŸ” NVIDIA/CUDA Diagnostic\\n\")\n",
    "\n",
    "# Check nvidia-smi\n",
    "try:\n",
    "    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, timeout=5)\n",
    "    if result.returncode == 0:\n",
    "        print(\"âœ… NVIDIA driver detected:\")\n",
    "        print(result.stdout)\n",
    "    else:\n",
    "        print(\"âŒ nvidia-smi failed\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ nvidia-smi not found - NVIDIA driver may not be installed\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error running nvidia-smi: {e}\")\n",
    "\n",
    "# Check CUDA libraries\n",
    "print(\"\\nğŸ” Checking CUDA libraries...\")\n",
    "try:\n",
    "    import ctypes\n",
    "    ctypes.CDLL('libcudart.so')\n",
    "    print(\"âœ… libcudart.so found (CUDA runtime)\")\n",
    "except:\n",
    "    print(\"âŒ libcudart.so not found\")\n",
    "    \n",
    "try:\n",
    "    import ctypes\n",
    "    ctypes.CDLL('libcublas.so')\n",
    "    print(\"âœ… libcublas.so found (CUDA BLAS)\")\n",
    "except:\n",
    "    print(\"âŒ libcublas.so not found\")\n",
    "\n",
    "print(\"\\nğŸ’¡ If CUDA libraries are missing, install tensorflow[and-cuda]:\")\n",
    "print(\"   !pip install --upgrade tensorflow[and-cuda]==2.15.0\")\n",
    "print(\"   Then RESTART THE KERNEL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 1\n",
      "GPU computation successful: tf.Tensor(\n",
      "[[1. 3.]\n",
      " [3. 7.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check GPU availability\n",
    "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# Test GPU computation\n",
    "with tf.device('/GPU:0'):\n",
    "    a = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "    b = tf.constant([[1.0, 1.0], [0.0, 1.0]])\n",
    "    c = tf.matmul(a, b)\n",
    "    print(\"GPU computation successful:\", c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 1 GPU(s) detected - Enabling GPU optimizations\n",
      "âœ… GPU memory growth enabled\n",
      "âœ… Mixed precision enabled (float16 compute, float32 variables)\n",
      "ğŸš€ L4 GPU Configuration:\n",
      "   â€¢ Batch Size: 64 (optimized for L4 16GB VRAM)\n",
      "   â€¢ Mixed Precision: Enabled\n",
      "   â€¢ Memory Growth: Enabled\n",
      "\n",
      "ğŸ“‹ Configuration:\n",
      "   Data Directory: /Users/anshul/Documents/lung_cancer/dataset/lung_image_sets\n",
      "   Image Size: (224, 224)\n",
      "   Batch Size: 64\n",
      "   Random Seed: 42\n"
     ]
    }
   ],
   "source": [
    "# Configuration and Data Setup - OPTIMIZED FOR APPLE SILICON (M1/M2/M3)\n",
    "DATA_DIR   = \"/Users/anshul/Documents/lung_cancer/dataset/lung_image_sets\"  # << set this\n",
    "IMG_SIZE   = (224, 224)\n",
    "BATCH_SIZE = 24  # OPTIMIZED: Conservative batch size for Mac M2 unified memory\n",
    "SEED       = 42\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Check if GPU is available before enabling optimizations\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    print(f'âœ… {len(gpus)} GPU(s) detected - Enabling Apple Metal GPU optimizations')\n",
    "    \n",
    "    try:\n",
    "        # Enable memory growth to prevent OOM errors\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print('âœ… GPU memory growth enabled')\n",
    "    except RuntimeError as e:\n",
    "        print(f'âš ï¸ GPU config warning: {e}')\n",
    "    \n",
    "    # Note: Mixed precision disabled for Apple Silicon (minimal benefit on Metal)\n",
    "    print(f\"ğŸ Apple Silicon (M2) Configuration:\")\n",
    "    print(f\"   â€¢ Batch Size: {BATCH_SIZE} (optimized for unified memory)\")\n",
    "    print(f\"   â€¢ Mixed Precision: Disabled (not beneficial on Metal)\")\n",
    "    print(f\"   â€¢ Memory Growth: Enabled\")\n",
    "    print(f\"   â€¢ Backend: Apple Metal GPU\")\n",
    "else:\n",
    "    print('âš ï¸ No GPU detected - Running on CPU')\n",
    "    print('   If you expect Apple GPU, please:')\n",
    "    print('   1. Install: pip install tensorflow-macos==2.16.1 tensorflow-metal==1.1.0')\n",
    "    print('   2. Restart kernel')\n",
    "    BATCH_SIZE = 16  # Reduce batch size for CPU\n",
    "    print(f'   Batch size reduced to {BATCH_SIZE} for CPU')\n",
    "\n",
    "print(f\"\\nğŸ“‹ Configuration:\")\n",
    "print(f\"   Data Directory: {DATA_DIR}\")\n",
    "print(f\"   Image Size: {IMG_SIZE}\")\n",
    "print(f\"   Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"   Random Seed: {SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of attention heads for multi-head channel attention\n",
    "NUM_ATTENTION_HEADS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12000 images belonging to 3 classes.\n",
      "Found 3000 images belonging to 3 classes.\n",
      "Classes: {'lung_aca': 0, 'lung_n': 1, 'lung_scc': 2}\n",
      "Number of classes: 3\n",
      "Training samples: 12000\n",
      "Validation samples: 3000\n",
      "Found 3000 images belonging to 3 classes.\n",
      "Classes: {'lung_aca': 0, 'lung_n': 1, 'lung_scc': 2}\n",
      "Number of classes: 3\n",
      "Training samples: 12000\n",
      "Validation samples: 3000\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    validation_split=0.20,\n",
    "    rotation_range=20,\n",
    "    horizontal_flip=True,\n",
    "    # IMPORTANT: no rescale here, since we feed raw to model-specific preprocessors\n",
    ")\n",
    "\n",
    "def make_gen(subset):\n",
    "    return train_datagen.flow_from_directory(\n",
    "        DATA_DIR,\n",
    "        target_size=IMG_SIZE,\n",
    "        class_mode='categorical',\n",
    "        batch_size=BATCH_SIZE,\n",
    "        subset=subset,\n",
    "        seed=SEED,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "train_gen = make_gen('training')\n",
    "val_gen   = make_gen('validation')\n",
    "num_classes = train_gen.num_classes\n",
    "class_indices = train_gen.class_indices\n",
    "id2label = {v:k for k,v in class_indices.items()}\n",
    "\n",
    "print(\"Classes:\", class_indices)\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Training samples: {train_gen.samples}\")\n",
    "print(f\"Validation samples: {val_gen.samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GPU CONFIGURATION\n",
      "============================================================\n",
      "âœ“ 1 GPU(s) detected and configured\n",
      "  Devices: ['/physical_device:GPU:0']\n",
      "\n",
      "TensorFlow: 2.16.1\n",
      "CUDA support: False\n",
      "============================================================\n",
      "\n",
      "âœ“ Multi-head attention block ready (GPU-optimized)!\n"
     ]
    }
   ],
   "source": [
    "# Channel Attention (Multi-Headed) Implementation - GPU OPTIMIZED\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Dense\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"âœ“ {len(gpus)} GPU(s) detected and configured\")\n",
    "        print(f\"  Devices: {[gpu.name for gpu in gpus]}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"âœ— GPU configuration error: {e}\")\n",
    "else:\n",
    "    print(\"âœ— No GPU detected - will use CPU\")\n",
    "\n",
    "print(f\"\\nTensorFlow: {tf.__version__}\")\n",
    "print(f\"CUDA support: {tf.test.is_built_with_cuda()}\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "class MultiHeadChannelAttention(Layer):\n",
    "    def __init__(self, num_heads=4, reduction=16, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.channel = input_shape[-1]\n",
    "        reduced_channels = max(self.channel // self.reduction, 1)\n",
    "        self.dense1 = Dense(self.num_heads * reduced_channels, activation='relu', name=f'{self.name}_d1')\n",
    "        self.dense2 = Dense(self.num_heads * self.channel, name=f'{self.name}_d2')\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        gap = tf.reduce_mean(x, axis=[1,2])\n",
    "        gmp = tf.reduce_max(x, axis=[1,2])\n",
    "        gap_feat = self.dense1(gap)\n",
    "        gmp_feat = self.dense1(gmp)\n",
    "        gap_attn = self.dense2(gap_feat)\n",
    "        gmp_attn = self.dense2(gmp_feat)\n",
    "        combined = tf.reshape(gap_attn + gmp_attn, [batch_size, self.num_heads, self.channel])\n",
    "        attention = tf.nn.sigmoid(tf.reduce_mean(combined, axis=1))\n",
    "        attention = tf.reshape(attention, [batch_size, 1, 1, self.channel])\n",
    "        return x * attention\n",
    "\n",
    "\n",
    "def multi_head_attention_block(x, reduction=16, name=None):\n",
    "    NUM_ATTENTION_HEADS = 4\n",
    "    return MultiHeadChannelAttention(num_heads=NUM_ATTENTION_HEADS, reduction=reduction, name=name)(x)\n",
    "\n",
    "print(\"âœ“ Multi-head attention block ready (GPU-optimized)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Lane function ready with GPU-optimized multi-head attention!\n"
     ]
    }
   ],
   "source": [
    "# Lane function with GPU-accelerated backbones\n",
    "from tensorflow.keras.layers import Lambda, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import ResNet50, DenseNet121, EfficientNetB0, InceptionV3\n",
    "\n",
    "def lane(tensor, backbone=\"resnet\", reduction=16):\n",
    "    \"\"\"Create a processing lane for each CNN backbone with multi-head channel attention (GPU-optimized)\"\"\"\n",
    "    if backbone == \"resnet\":\n",
    "        x = Lambda(pre_resnet, name=\"pre_resnet\")(tensor)\n",
    "        x = ResNet50(include_top=False, weights='imagenet')(x)\n",
    "    elif backbone == \"densenet\":\n",
    "        x = Lambda(pre_densenet, name=\"pre_densenet\")(tensor)\n",
    "        x = DenseNet121(include_top=False, weights='imagenet')(x)\n",
    "    elif backbone == \"efficientnet\":\n",
    "        x = Lambda(pre_efficientnet, name=\"pre_efficientnet\")(tensor)\n",
    "        x = EfficientNetB0(include_top=False, weights='imagenet')(x)\n",
    "    elif backbone == \"inception\":\n",
    "        x = Lambda(pre_inception, name=\"pre_inception\")(tensor)\n",
    "        x = InceptionV3(include_top=False, weights='imagenet')(x)\n",
    "    else:\n",
    "        raise ValueError(f'Unknown backbone: {backbone}')\n",
    "    \n",
    "    x = multi_head_attention_block(x, reduction=reduction, name=f\"mhca_{backbone}\")\n",
    "    x = GlobalAveragePooling2D(name=f\"gap_{backbone}\")(x)\n",
    "    return x\n",
    "\n",
    "print(\"âœ“ Lane function ready with GPU-optimized multi-head attention!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building multi-backbone feature concatenator with multi-head attention...\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m29084464/29084464\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n",
      "\u001b[1m29084464/29084464\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m94765736/94765736\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 0us/step\n",
      "\u001b[1m94765736/94765736\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 0us/step\n",
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
      "\u001b[1m16705208/16705208\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n",
      "\u001b[1m16705208/16705208\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m87910968/87910968\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 0us/step\n",
      "\u001b[1m87910968/87910968\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 0us/step\n",
      "Feature extractor built successfully!\n",
      "Feature dimension: 6400\n",
      "Feature extractor built successfully!\n",
      "Feature dimension: 6400\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)        </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape      </span>â”ƒ<span style=\"font-weight: bold\">    Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to      </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                 â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ pre_densenet        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)            â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ pre_resnet (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] â”‚\n",
       "â”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ pre_efficientnet    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)            â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ pre_inception       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)            â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ densenet121         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>,      â”‚  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,037,504</span> â”‚ pre_densenet[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)             â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ resnet50            â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>,      â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> â”‚ pre_resnet[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)             â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ efficientnetb0      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>,      â”‚  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> â”‚ pre_efficientnetâ€¦ â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)             â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ inception_v3        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>,      â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">21,802,784</span> â”‚ pre_inception[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)             â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ mhca_densenet       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>,      â”‚  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,315,072</span> â”‚ densenet121[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadChannelAâ€¦</span> â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)             â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ mhca_resnet         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>,      â”‚  <span style=\"color: #00af00; text-decoration-color: #00af00\">5,251,584</span> â”‚ resnet50[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadChannelAâ€¦</span> â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)             â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ mhca_efficientnet   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>,      â”‚  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,053,440</span> â”‚ efficientnetb0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>â€¦ â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadChannelAâ€¦</span> â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)             â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ mhca_inception      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>,      â”‚  <span style=\"color: #00af00; text-decoration-color: #00af00\">5,251,584</span> â”‚ inception_v3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadChannelAâ€¦</span> â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)             â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ gap_densenet        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ mhca_densenet[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePoolâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ gap_resnet          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)      â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ mhca_resnet[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePoolâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ gap_efficientnet    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)      â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ mhca_efficientneâ€¦ â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePoolâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ gap_inception       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)      â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ mhca_inception[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>â€¦ â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePoolâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ concat_feats        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6400</span>)      â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ gap_densenet[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       â”‚                   â”‚            â”‚ gap_resnet[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], â”‚\n",
       "â”‚                     â”‚                   â”‚            â”‚ gap_efficientnetâ€¦ â”‚\n",
       "â”‚                     â”‚                   â”‚            â”‚ gap_inception[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  â”‚          \u001b[38;5;34m0\u001b[0m â”‚ -                 â”‚\n",
       "â”‚ (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚ \u001b[38;5;34m3\u001b[0m)                â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ pre_densenet        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  â”‚          \u001b[38;5;34m0\u001b[0m â”‚ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] â”‚\n",
       "â”‚ (\u001b[38;5;33mLambda\u001b[0m)            â”‚ \u001b[38;5;34m3\u001b[0m)                â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ pre_resnet (\u001b[38;5;33mLambda\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  â”‚          \u001b[38;5;34m0\u001b[0m â”‚ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] â”‚\n",
       "â”‚                     â”‚ \u001b[38;5;34m3\u001b[0m)                â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ pre_efficientnet    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  â”‚          \u001b[38;5;34m0\u001b[0m â”‚ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] â”‚\n",
       "â”‚ (\u001b[38;5;33mLambda\u001b[0m)            â”‚ \u001b[38;5;34m3\u001b[0m)                â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ pre_inception       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  â”‚          \u001b[38;5;34m0\u001b[0m â”‚ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] â”‚\n",
       "â”‚ (\u001b[38;5;33mLambda\u001b[0m)            â”‚ \u001b[38;5;34m3\u001b[0m)                â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ densenet121         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m,      â”‚  \u001b[38;5;34m7,037,504\u001b[0m â”‚ pre_densenet[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34mâ€¦\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mFunctional\u001b[0m)        â”‚ \u001b[38;5;34m1024\u001b[0m)             â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ resnet50            â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m,      â”‚ \u001b[38;5;34m23,587,712\u001b[0m â”‚ pre_resnet[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  â”‚\n",
       "â”‚ (\u001b[38;5;33mFunctional\u001b[0m)        â”‚ \u001b[38;5;34m2048\u001b[0m)             â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ efficientnetb0      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m,      â”‚  \u001b[38;5;34m4,049,571\u001b[0m â”‚ pre_efficientnetâ€¦ â”‚\n",
       "â”‚ (\u001b[38;5;33mFunctional\u001b[0m)        â”‚ \u001b[38;5;34m1280\u001b[0m)             â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ inception_v3        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m,      â”‚ \u001b[38;5;34m21,802,784\u001b[0m â”‚ pre_inception[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\n",
       "â”‚ (\u001b[38;5;33mFunctional\u001b[0m)        â”‚ \u001b[38;5;34m2048\u001b[0m)             â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ mhca_densenet       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m,      â”‚  \u001b[38;5;34m1,315,072\u001b[0m â”‚ densenet121[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] â”‚\n",
       "â”‚ (\u001b[38;5;33mMultiHeadChannelAâ€¦\u001b[0m â”‚ \u001b[38;5;34m1024\u001b[0m)             â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ mhca_resnet         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m,      â”‚  \u001b[38;5;34m5,251,584\u001b[0m â”‚ resnet50[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\n",
       "â”‚ (\u001b[38;5;33mMultiHeadChannelAâ€¦\u001b[0m â”‚ \u001b[38;5;34m2048\u001b[0m)             â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ mhca_efficientnet   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m,      â”‚  \u001b[38;5;34m2,053,440\u001b[0m â”‚ efficientnetb0[\u001b[38;5;34m0\u001b[0mâ€¦ â”‚\n",
       "â”‚ (\u001b[38;5;33mMultiHeadChannelAâ€¦\u001b[0m â”‚ \u001b[38;5;34m1280\u001b[0m)             â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ mhca_inception      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m,      â”‚  \u001b[38;5;34m5,251,584\u001b[0m â”‚ inception_v3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34mâ€¦\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mMultiHeadChannelAâ€¦\u001b[0m â”‚ \u001b[38;5;34m2048\u001b[0m)             â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ gap_densenet        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      â”‚          \u001b[38;5;34m0\u001b[0m â”‚ mhca_densenet[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalAveragePoolâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ gap_resnet          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)      â”‚          \u001b[38;5;34m0\u001b[0m â”‚ mhca_resnet[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalAveragePoolâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ gap_efficientnet    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)      â”‚          \u001b[38;5;34m0\u001b[0m â”‚ mhca_efficientneâ€¦ â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalAveragePoolâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ gap_inception       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)      â”‚          \u001b[38;5;34m0\u001b[0m â”‚ mhca_inception[\u001b[38;5;34m0\u001b[0mâ€¦ â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalAveragePoolâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ concat_feats        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6400\u001b[0m)      â”‚          \u001b[38;5;34m0\u001b[0m â”‚ gap_densenet[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34mâ€¦\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mConcatenate\u001b[0m)       â”‚                   â”‚            â”‚ gap_resnet[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], â”‚\n",
       "â”‚                     â”‚                   â”‚            â”‚ gap_efficientnetâ€¦ â”‚\n",
       "â”‚                     â”‚                   â”‚            â”‚ gap_inception[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">70,349,251</span> (268.36 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m70,349,251\u001b[0m (268.36 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">70,136,028</span> (267.55 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m70,136,028\u001b[0m (267.55 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">213,223</span> (832.91 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m213,223\u001b[0m (832.91 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build Feature Extractor Model\n",
    "print(\"Building multi-backbone feature concatenator with multi-head attention...\")\n",
    "\n",
    "# Define input tensor with image size (224x224x3 RGB)\n",
    "inp = Input(shape=(224,224,3))\n",
    "\n",
    "# Extract features from DenseNet lane (multi-head attention)\n",
    "feat_d = lane(inp, \"densenet\", reduction=16)\n",
    "# Extract features from ResNet lane (multi-head attention)\n",
    "feat_r = lane(inp, \"resnet\", reduction=16)\n",
    "# Extract features from EfficientNetB0 lane (multi-head attention)\n",
    "feat_e = lane(inp, \"efficientnet\", reduction=16)\n",
    "# Extract features from InceptionV3 lane (multi-head attention)\n",
    "feat_i = lane(inp, \"inception\", reduction=16)\n",
    "\n",
    "# Concatenate features from all four backbones\n",
    "concat_feat = Concatenate(name=\"concat_feats\")([feat_d, feat_r, feat_e, feat_i])\n",
    "\n",
    "# Create feature extractor model (input â†’ concatenated features)\n",
    "feature_model = Model(inp, concat_feat)\n",
    "\n",
    "# Get final concatenated feature dimension\n",
    "feature_dim = feature_model.output_shape[-1]\n",
    "\n",
    "print(f\"Feature extractor built successfully!\")\n",
    "print(f\"Feature dimension: {feature_dim}\")\n",
    "\n",
    "# Show model summary (layers, parameters, shapes)\n",
    "feature_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… GPU-optimized feature extraction ready!\n"
     ]
    }
   ],
   "source": [
    "# Extract Deep Features with GPU Optimization - OPTIMIZED\n",
    "from math import ceil\n",
    "\n",
    "def extract_features(generator):\n",
    "    \"\"\"Extract features with GPU acceleration and optimized batching\"\"\"\n",
    "    import time\n",
    "    \n",
    "    print(\"ğŸš€ Starting GPU-optimized feature extraction...\")\n",
    "    if tf.config.list_physical_devices('GPU'):\n",
    "        print(\"ğŸ Using Apple Metal GPU\")\n",
    "        with tf.device('/GPU:0'):\n",
    "            return _extract_features_impl(generator)\n",
    "    else:\n",
    "        print(\"ğŸ’» Using CPU (GPU not available)\")\n",
    "        return _extract_features_impl(generator)\n",
    "\n",
    "def _extract_features_impl(generator):\n",
    "    import time\n",
    "    X, y = [], []\n",
    "    steps = len(generator)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i in range(steps):\n",
    "        imgs, labels = next(generator)\n",
    "        feats = feature_model.predict(imgs, verbose=0)\n",
    "        X.append(feats)\n",
    "        y.append(labels)\n",
    "        \n",
    "        if (i + 1) % 20 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            avg = elapsed / (i + 1)\n",
    "            eta = (steps - (i + 1)) * avg\n",
    "            print(f\"ğŸ“Š [{i + 1}/{steps}] Avg batch: {avg:.2f}s | ETA: {eta/60:.1f}m\")\n",
    "    \n",
    "    total = time.time() - start_time\n",
    "    print(f\"âœ… Feature extraction: {total/60:.2f} min ({total/steps:.2f}s/batch)\")\n",
    "    return np.vstack(X), np.vstack(y)\n",
    "\n",
    "print(\"âœ… GPU-optimized feature extraction ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting training features â€¦\n",
      "ğŸš€ Starting GPU-optimized feature extraction...\n",
      "ğŸ”¥ Using L4 GPU with mixed precision (if enabled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-31 06:41:28.421208: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š [20/188] Avg batch: 5.47s | ETA: 15.3m\n",
      "ğŸ“Š [40/188] Avg batch: 4.41s | ETA: 10.9m\n",
      "ğŸ“Š [40/188] Avg batch: 4.41s | ETA: 10.9m\n",
      "ğŸ“Š [60/188] Avg batch: 4.49s | ETA: 9.6m\n",
      "ğŸ“Š [60/188] Avg batch: 4.49s | ETA: 9.6m\n",
      "ğŸ“Š [80/188] Avg batch: 4.39s | ETA: 7.9m\n",
      "ğŸ“Š [80/188] Avg batch: 4.39s | ETA: 7.9m\n",
      "ğŸ“Š [100/188] Avg batch: 4.21s | ETA: 6.2m\n",
      "ğŸ“Š [100/188] Avg batch: 4.21s | ETA: 6.2m\n",
      "ğŸ“Š [120/188] Avg batch: 4.08s | ETA: 4.6m\n",
      "ğŸ“Š [120/188] Avg batch: 4.08s | ETA: 4.6m\n",
      "ğŸ“Š [140/188] Avg batch: 3.99s | ETA: 3.2m\n",
      "ğŸ“Š [140/188] Avg batch: 3.99s | ETA: 3.2m\n",
      "ğŸ“Š [160/188] Avg batch: 3.94s | ETA: 1.8m\n",
      "ğŸ“Š [160/188] Avg batch: 3.94s | ETA: 1.8m\n",
      "ğŸ“Š [180/188] Avg batch: 3.95s | ETA: 0.5m\n",
      "ğŸ“Š [180/188] Avg batch: 3.95s | ETA: 0.5m\n",
      "âœ… Feature extraction: 12.40 min (3.96s/batch)\n",
      "âœ… Feature extraction: 12.40 min (3.96s/batch)\n",
      "Training features shape: (12000, 6400)\n",
      "Training labels shape: (12000, 3)\n",
      "Training features shape: (12000, 6400)\n",
      "Training labels shape: (12000, 3)\n"
     ]
    }
   ],
   "source": [
    "# Extract Training Features\n",
    "print(\"Extracting training features â€¦\")\n",
    "X_tr, Y_tr_ohe = extract_features(train_gen)\n",
    "print(f\"Training features shape: {X_tr.shape}\")\n",
    "print(f\"Training labels shape: {Y_tr_ohe.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… mrmr_selection imported successfully (Apple Silicon compatible)\n",
      "âœ… Optimized mRMR feature selection ready!\n",
      "âœ… Optimized AGWO feature selection ready!\n"
     ]
    }
   ],
   "source": [
    "## 1. TRUE mRMR Feature Ranking - OPTIMIZED\n",
    "try:\n",
    "    from mrmr import mrmr_classif\n",
    "    print(\"âœ… mrmr_selection imported successfully (Apple Silicon compatible)\")\n",
    "    MRMR_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"WARNING: mrmr_selection not available. Install with: pip install mrmr_selection\")\n",
    "    print(\"Falling back to mutual information ranking only.\")\n",
    "    MRMR_AVAILABLE = False\n",
    "\n",
    "import numpy as np, pandas as pd, time, gc\n",
    "from sklearn.feature_selection import mutual_info_classif, VarianceThreshold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def true_mrmr_feature_selection(X, y_ohe, n_features=1000, sample_rows=1500, var_thresh=0.01, redundancy_penalty=0.4):\n",
    "    \"\"\"\n",
    "    OPTIMIZED TRUE mRMR implementation with reduced sampling. If mrmr_selection is unavailable,\n",
    "    falls back to an MI + redundancy-penalized greedy selection approximating mRMR.\n",
    "    Returns top-n feature indices (global indices).\n",
    "    \"\"\"\n",
    "    t0 = time.time()\n",
    "    y = np.argmax(y_ohe, axis=1)\n",
    "    n_samples, n_feats = X.shape\n",
    "    \n",
    "    # Variance filter to remove low-variance features\n",
    "    if var_thresh > 0:\n",
    "        vt = VarianceThreshold(var_thresh)\n",
    "        X_filtered = vt.fit_transform(X)\n",
    "        kept_indices = np.where(vt.get_support())[0]\n",
    "    else:\n",
    "        X_filtered = X\n",
    "        kept_indices = np.arange(n_feats)\n",
    "    \n",
    "    print(f\"[mRMR] After variance filter: {len(kept_indices)} features\")\n",
    "    \n",
    "    # Row sampling for speed\n",
    "    if sample_rows and sample_rows < X_filtered.shape[0]:\n",
    "        rng = np.random.default_rng(42)\n",
    "        rows = rng.choice(X_filtered.shape[0], size=sample_rows, replace=False)\n",
    "        X_sample = X_filtered[rows]\n",
    "        y_sample = y[rows]\n",
    "    else:\n",
    "        X_sample = X_filtered\n",
    "        y_sample = y\n",
    "    \n",
    "    # Try TRUE mRMR via mrmr_selection\n",
    "    if MRMR_AVAILABLE:\n",
    "        try:\n",
    "            # Create DataFrame for mrmr_selection\n",
    "            feature_names = [f'feature_{i}' for i in range(X_sample.shape[1])]\n",
    "            df = pd.DataFrame(X_sample, columns=feature_names)\n",
    "            df['target'] = y_sample\n",
    "            \n",
    "            # Use mrmr_classif (modern alternative to pymrmr)\n",
    "            selected_features = mrmr_classif(X=df.drop('target', axis=1), y=df['target'], \n",
    "                                            K=min(n_features, X_sample.shape[1]))\n",
    "            selected_indices_local = [int(f.split('_')[1]) for f in selected_features]\n",
    "            final_indices = [kept_indices[i] for i in selected_indices_local]\n",
    "            print(f\"[TRUE-mRMR] Selected {len(final_indices)} features in {time.time()-t0:.2f}s\")\n",
    "            \n",
    "            # Also expose full MI ranking for compliance logging\n",
    "            mi_scores_all = mutual_info_classif(X, y, discrete_features=False, random_state=42)\n",
    "            globals()['ALL_FEATURE_RANKING'] = np.argsort(mi_scores_all)[::-1]\n",
    "            return final_indices\n",
    "        except Exception as e:\n",
    "            print(f\"[TRUE-mRMR] Error: {e}. Falling back to MI + redundancy-penalized greedy selection.\")\n",
    "    \n",
    "    # Fallback: MI scoring for ALL kept features\n",
    "    mi_scores = mutual_info_classif(X_sample, y_sample, discrete_features=False, random_state=42)\n",
    "    order_local = np.argsort(mi_scores)[::-1]\n",
    "    # Expose FULL ranking (global indices) for compliance\n",
    "    globals()['ALL_FEATURE_RANKING'] = kept_indices[order_local]\n",
    "    \n",
    "    # Greedy selection with redundancy penalty (approx mRMR)\n",
    "    # Normalize X_sample for correlation computations\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "    Xn = scaler.fit_transform(X_sample)\n",
    "    selected_local = []\n",
    "    selected_set = set()\n",
    "    # Pre-index columns for speed\n",
    "    col_cache = {}\n",
    "    def col_vec(idx):\n",
    "        if idx not in col_cache:\n",
    "            col_cache[idx] = Xn[:, idx]\n",
    "        return col_cache[idx]\n",
    "    for _ in range(min(n_features, Xn.shape[1])):\n",
    "        best_idx = None\n",
    "        best_score = -1\n",
    "        for j in order_local:\n",
    "            if j in selected_set:\n",
    "                continue\n",
    "            rel = mi_scores[j]\n",
    "            if not selected_local:\n",
    "                score = rel\n",
    "            else:\n",
    "                vj = col_vec(j)\n",
    "                # Compute average |corr| against selected\n",
    "                corrs = []\n",
    "                for s in selected_local:\n",
    "                    vs = col_vec(s)\n",
    "                    # use dot-product corr on standardized\n",
    "                    c = float(np.dot(vs, vj) / (len(vj)-1))\n",
    "                    corrs.append(abs(c))\n",
    "                redund = np.mean(corrs) if corrs else 0.0\n",
    "                score = rel - redundancy_penalty * redund\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_idx = j\n",
    "        selected_local.append(best_idx)\n",
    "        selected_set.add(best_idx)\n",
    "        if len(selected_local) >= n_features:\n",
    "            break\n",
    "    final_indices = [kept_indices[i] for i in selected_local]\n",
    "    print(f\"[MI+Redundancy] Selected {len(final_indices)} features in {time.time()-t0:.2f}s\")\n",
    "    return final_indices\n",
    "\n",
    "print(\"âœ… Optimized mRMR feature selection ready!\")\n",
    "\n",
    "\n",
    "## 2. Enhanced Adaptive Grey Wolf Optimization (AGWO) - OPTIMIZED\n",
    "import numpy as np, gc, hashlib\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def _subset_hash(idxs):\n",
    "    return hashlib.md5(np.asarray(idxs, dtype=np.int32).tobytes()).hexdigest()\n",
    "\n",
    "def enhanced_agwo_feature_selection(\n",
    "    X_ranked,\n",
    "    y_ohe,\n",
    "    ranked_global_indices,\n",
    "    n_wolves=20,  # OPTIMIZED: Reduced from 25\n",
    "    n_iter=15,    # OPTIMIZED: Reduced from 30 with better convergence\n",
    "    min_subset=500,\n",
    "    max_subset=1500,  # OPTIMIZED: Reduced from 2000\n",
    "    row_sample=2500,  # OPTIMIZED: Reduced from 3000\n",
    "    knn_folds=3,      # OPTIMIZED: Reduced from 5\n",
    "    rf_folds=2,       # OPTIMIZED: Kept at 2\n",
    "    rf_max_features=400,  # OPTIMIZED: Reduced from 500\n",
    "    penalty_weight=0.015,  # OPTIMIZED: Fine-tuned\n",
    "    patience=6,           # OPTIMIZED: Reduced from 8\n",
    "    random_state=42,\n",
    "    verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    OPTIMIZED Enhanced AGWO with reduced iterations and better convergence\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    y = np.argmax(y_ohe, axis=1)\n",
    "    n_samples, n_feats = X_ranked.shape\n",
    "\n",
    "    # Enhanced row subsampling (stratified)\n",
    "    if row_sample and row_sample < n_samples:\n",
    "        rows = []\n",
    "        per_class = row_sample // len(np.unique(y))\n",
    "        for cls in np.unique(y):\n",
    "            cls_idx = np.where(y == cls)[0]\n",
    "            take = min(per_class, len(cls_idx))\n",
    "            rows.append(rng.choice(cls_idx, size=take, replace=False))\n",
    "        rows = np.concatenate(rows)\n",
    "    else:\n",
    "        rows = np.arange(n_samples)\n",
    "\n",
    "    X_fit = X_ranked[rows]\n",
    "    y_fit = y[rows]\n",
    "\n",
    "    # Wolves initialization with better diversity\n",
    "    def init_position():\n",
    "        vals = rng.random(n_feats)\n",
    "        vals = vals * (1 + 0.5 * np.sin(np.arange(n_feats) * 0.1))\n",
    "        return vals\n",
    "\n",
    "    wolves = [init_position() for _ in range(n_wolves)]\n",
    "\n",
    "    # OPTIMIZED: Logarithmic growth with steeper curve\n",
    "    def subset_budget(iter_idx):\n",
    "        log_factor = np.log(iter_idx + 2) / np.log(n_iter + 1)\n",
    "        return int(min_subset + (max_subset - min_subset) * log_factor)\n",
    "\n",
    "    # Enhanced fitness cache\n",
    "    fitness_cache = {}\n",
    "\n",
    "    def eval_subset(local_idx):\n",
    "        if len(local_idx) < 2:\n",
    "            return 0.0\n",
    "        key_hash = _subset_hash(local_idx)\n",
    "        if key_hash in fitness_cache:\n",
    "            return fitness_cache[key_hash]\n",
    "\n",
    "        # Enhanced feature selection for RF\n",
    "        feat_slice = local_idx\n",
    "        if len(feat_slice) > rf_max_features:\n",
    "            feat_slice_rf = rng.choice(feat_slice, size=rf_max_features, replace=False)\n",
    "        else:\n",
    "            feat_slice_rf = feat_slice\n",
    "\n",
    "        X_sub = X_fit[:, feat_slice]\n",
    "        scaler = StandardScaler()\n",
    "        X_sub = scaler.fit_transform(X_sub)\n",
    "\n",
    "        # KNN CV with reduced folds\n",
    "        skf_knn = StratifiedKFold(n_splits=knn_folds, shuffle=True, random_state=123)\n",
    "        knn_scores = []\n",
    "        knn = KNeighborsClassifier(n_neighbors=5, weights='distance', n_jobs=-1)\n",
    "        for tr, va in skf_knn.split(X_sub, y_fit):\n",
    "            knn.fit(X_sub[tr], y_fit[tr])\n",
    "            pred = knn.predict(X_sub[va])\n",
    "            knn_scores.append(accuracy_score(y_fit[va], pred))\n",
    "        knn_acc = np.mean(knn_scores)\n",
    "\n",
    "        # RF CV with reduced folds\n",
    "        X_sub_rf = X_fit[:, feat_slice_rf]\n",
    "        scaler_rf = StandardScaler()\n",
    "        X_sub_rf = scaler_rf.fit_transform(X_sub_rf)\n",
    "        skf_rf = StratifiedKFold(n_splits=rf_folds, shuffle=True, random_state=321)\n",
    "        rf_scores = []\n",
    "        rf = RandomForestClassifier(\n",
    "            n_estimators=150,  # OPTIMIZED: Reduced from 200\n",
    "            max_features='sqrt',\n",
    "            n_jobs=-1,\n",
    "            random_state=999\n",
    ")\n",
    "        for tr, va in skf_rf.split(X_sub_rf, y_fit):\n",
    "            rf.fit(X_sub_rf[tr], y_fit[tr])\n",
    "            pred = rf.predict(X_sub_rf[va])\n",
    "            rf_scores.append(accuracy_score(y_fit[va], pred))\n",
    "        rf_acc = np.mean(rf_scores)\n",
    "\n",
    "        # Fine-tuned penalty\n",
    "        size_penalty = penalty_weight * (len(local_idx) / max_subset)\n",
    "        fitness = 0.7 * knn_acc + 0.3 * rf_acc - size_penalty\n",
    "        fitness_cache[key_hash] = fitness\n",
    "        return fitness\n",
    "\n",
    "    # Enhanced decoding with stability\n",
    "    def decode(position, k):\n",
    "        noisy_pos = position + rng.normal(0, 0.01, len(position))\n",
    "        order = np.argpartition(noisy_pos, -k)[-k:]\n",
    "        return order[np.argsort(-noisy_pos[order])]\n",
    "\n",
    "    # Enhanced AGWO loop\n",
    "    best_global_subset = None\n",
    "    best_fitness = -1\n",
    "    no_improve = 0\n",
    "\n",
    "    for it in range(n_iter):\n",
    "        k_budget = subset_budget(it)\n",
    "\n",
    "        # Decode all wolves\n",
    "        wolf_subsets = [decode(w, k_budget) for w in wolves]\n",
    "        wolf_scores = [eval_subset(sub) for sub in wolf_subsets]\n",
    "\n",
    "        # Identify alpha, beta, delta\n",
    "        order = np.argsort(wolf_scores)[::-1]\n",
    "        alpha, beta, delta = wolves[order[0]], wolves[order[1]], wolves[order[2]]\n",
    "        alpha_subset = wolf_subsets[order[0]]\n",
    "        alpha_score = wolf_scores[order[0]]\n",
    "\n",
    "        if alpha_score > best_fitness:\n",
    "            best_fitness = alpha_score\n",
    "            best_global_subset = alpha_subset.copy()\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"[AGWO] iter {it+1}/{n_iter} k={k_budget} alpha={alpha_score:.4f} best={best_fitness:.4f} cache={len(fitness_cache)}\")\n",
    "\n",
    "        if no_improve >= patience:\n",
    "            if verbose:\n",
    "                print(f\"[AGWO] Early stop (patience {patience})\")\n",
    "            break\n",
    "\n",
    "        # OPTIMIZED: Steeper decay for faster convergence\n",
    "        a = 2 * np.exp(-4 * (it / n_iter))\n",
    "\n",
    "        # Enhanced wolf update\n",
    "        new_wolves = []\n",
    "        for idx, w in enumerate(wolves):\n",
    "            if idx in order[:3]:\n",
    "                new_wolves.append(w)\n",
    "                continue\n",
    "                \n",
    "            A1 = 2 * a * rng.random(n_feats) - a\n",
    "            C1 = 2 * rng.random(n_feats)\n",
    "            A2 = 2 * a * rng.random(n_feats) - a\n",
    "            C2 = 2 * rng.random(n_feats)\n",
    "            A3 = 2 * a * rng.random(n_feats) - a\n",
    "            C3 = 2 * rng.random(n_feats)\n",
    "\n",
    "            D_alpha = np.abs(C1 * alpha - w)\n",
    "            D_beta  = np.abs(C2 * beta  - w)\n",
    "            D_delta = np.abs(C3 * delta - w)\n",
    "\n",
    "            X1 = alpha - A1 * D_alpha\n",
    "            X2 = beta  - A2 * D_beta\n",
    "            X3 = delta - A3 * D_delta\n",
    "\n",
    "            new_pos = (X1 + X2 + X3) / 3.0\n",
    "\n",
    "            # Enhanced mutation\n",
    "            if rng.random() < 0.15:\n",
    "                mut_mask = rng.random(n_feats) < 0.005\n",
    "                noise = rng.normal(0, 0.3, np.sum(mut_mask))\n",
    "                new_pos[mut_mask] += noise\n",
    "\n",
    "            new_pos = np.clip(new_pos, -2.0, 2.0)\n",
    "            new_wolves.append(new_pos)\n",
    "\n",
    "        # Diversity injection\n",
    "        if no_improve == patience - 1:\n",
    "            inject_count = max(2, n_wolves // 5)\n",
    "            for _ in range(inject_count):\n",
    "                ridx = rng.integers(3, n_wolves)\n",
    "                new_wolves[ridx] = init_position()\n",
    "\n",
    "        wolves = new_wolves\n",
    "\n",
    "    # Map to global feature indices\n",
    "    selected_global = [ranked_global_indices[i] for i in best_global_subset]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[AGWO] Complete: {len(selected_global)} features, fitness={best_fitness:.4f}\")\n",
    "\n",
    "    return selected_global\n",
    "\n",
    "print(\"âœ… Optimized AGWO feature selection ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting validation features â€¦\n",
      "ğŸš€ Starting GPU-optimized feature extraction...\n",
      "ğŸ”¥ Using L4 GPU with mixed precision (if enabled)\n",
      "ğŸ“Š [20/47] Avg batch: 3.68s | ETA: 1.7m\n",
      "ğŸ“Š [20/47] Avg batch: 3.68s | ETA: 1.7m\n",
      "ğŸ“Š [40/47] Avg batch: 3.49s | ETA: 0.4m\n",
      "ğŸ“Š [40/47] Avg batch: 3.49s | ETA: 0.4m\n",
      "âœ… Feature extraction: 3.13 min (4.00s/batch)\n",
      "âœ… Feature extraction: 3.13 min (4.00s/batch)\n",
      "Validation features shape: (3000, 6400)\n",
      "Validation labels shape: (3000, 3)\n",
      "Validation features shape: (3000, 6400)\n",
      "Validation labels shape: (3000, 3)\n"
     ]
    }
   ],
   "source": [
    "# Extract Validation Features\n",
    "print(\"Extracting validation features â€¦\")\n",
    "X_va, Y_va_ohe = extract_features(val_gen)\n",
    "print(f\"Validation features shape: {X_va.shape}\")\n",
    "print(f\"Validation labels shape: {Y_va_ohe.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features shape: (15000, 6400)\n",
      "Total labels shape: (15000,)\n",
      "Classes present: [0 1 2]\n",
      "Class distribution: [5000 5000 5000]\n"
     ]
    }
   ],
   "source": [
    "# Combine Features and Convert Labels\n",
    "# NOTE: We avoid building X_full (memory heavy); use train/val directly where possible\n",
    "X_full = np.vstack([X_tr, X_va])\n",
    "y_full = np.argmax(np.vstack([Y_tr_ohe, Y_va_ohe]), axis=1)\n",
    "\n",
    "print(f\"Total features shape: {X_full.shape}\")\n",
    "print(f\"Total labels shape: {y_full.shape}\")\n",
    "print(f\"Classes present: {np.unique(y_full)}\")\n",
    "print(f\"Class distribution: {np.bincount(y_full)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1a: Full MI Ranking (All features)\n",
      "[Pipeline] Full MI ranking length: 6400\n",
      "Stage 1: TRUE mRMR Feature Ranking (Optimized)\n",
      "[Pipeline] Full MI ranking length: 6400\n",
      "Stage 1: TRUE mRMR Feature Ranking (Optimized)\n",
      "[mRMR] After variance filter: 3962 features\n",
      "[mRMR] After variance filter: 3962 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600/600 [02:28<00:00,  4.05it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRUE-mRMR] Selected 600 features in 151.12s\n",
      "[Pipeline] Ranked features (top-N for AGWO): 600\n",
      "[Pipeline] Ranked features shape: (12000, 600)\n",
      "[Pipeline] Adjusted max_subset: 1000 â†’ 600\n",
      "\n",
      "Stage 3: Enhanced AGWO Feature Selection (Optimized)\n",
      "[Pipeline] Ranked features (top-N for AGWO): 600\n",
      "[Pipeline] Ranked features shape: (12000, 600)\n",
      "[Pipeline] Adjusted max_subset: 1000 â†’ 600\n",
      "\n",
      "Stage 3: Enhanced AGWO Feature Selection (Optimized)\n",
      "[AGWO] iter 1/15 k=450 alpha=0.9511 best=0.9511 cache=20\n",
      "[AGWO] iter 1/15 k=450 alpha=0.9511 best=0.9511 cache=20\n",
      "[AGWO] iter 2/15 k=479 alpha=0.9481 best=0.9511 cache=40\n",
      "[AGWO] iter 2/15 k=479 alpha=0.9481 best=0.9511 cache=40\n",
      "[AGWO] iter 3/15 k=500 alpha=0.9485 best=0.9511 cache=60\n",
      "[AGWO] iter 3/15 k=500 alpha=0.9485 best=0.9511 cache=60\n",
      "[AGWO] iter 4/15 k=516 alpha=0.9494 best=0.9511 cache=80\n",
      "[AGWO] iter 4/15 k=516 alpha=0.9494 best=0.9511 cache=80\n",
      "[AGWO] iter 5/15 k=529 alpha=0.9498 best=0.9511 cache=100\n",
      "[AGWO] iter 5/15 k=529 alpha=0.9498 best=0.9511 cache=100\n",
      "[AGWO] iter 6/15 k=540 alpha=0.9492 best=0.9511 cache=120\n",
      "[AGWO] iter 6/15 k=540 alpha=0.9492 best=0.9511 cache=120\n",
      "[AGWO] iter 7/15 k=550 alpha=0.9485 best=0.9511 cache=140\n",
      "[AGWO] Early stop (patience 6)\n",
      "[AGWO] Complete: 450 features, fitness=0.9511\n",
      "[Pipeline] Final selected features: 450\n",
      "[Pipeline] Train (12000, 450), Test (3000, 450)\n",
      "[Pipeline] Feature reduction: 6400 â†’ 450 (7.0%)\n",
      "\n",
      "[Pipeline] Total time: 538.94s\n",
      "âœ… Optimized two-stage feature selection completed!\n",
      "[AGWO] iter 7/15 k=550 alpha=0.9485 best=0.9511 cache=140\n",
      "[AGWO] Early stop (patience 6)\n",
      "[AGWO] Complete: 450 features, fitness=0.9511\n",
      "[Pipeline] Final selected features: 450\n",
      "[Pipeline] Train (12000, 450), Test (3000, 450)\n",
      "[Pipeline] Feature reduction: 6400 â†’ 450 (7.0%)\n",
      "\n",
      "[Pipeline] Total time: 538.94s\n",
      "âœ… Optimized two-stage feature selection completed!\n"
     ]
    }
   ],
   "source": [
    "# --- OPTIMIZED: TRUE mRMR + Enhanced AGWO Feature Selection Pipeline ---\n",
    "import time, gc\n",
    "\n",
    "t_total = time.time()\n",
    "\n",
    "# AGGRESSIVE Parameters (more selective for accuracy boost)\n",
    "n_mrmr = 600          # AGGRESSIVE: Reduced from 800 to filter more noise\n",
    "sample_rows = 1500    # Keep same for stable MI estimation\n",
    "subset_size = 1000    # AGGRESSIVE: Reduced from 1500 for AGWO final selection\n",
    "n_wolves = 20         # Keep same for exploration\n",
    "n_iter = 15           # Keep same for convergence\n",
    "\n",
    "# Stage 1a: Full MI Ranking of ALL features (compliance)\n",
    "print(\"Stage 1a: Full MI Ranking (All features)\")\n",
    "y_tmp = np.argmax(Y_tr_ohe, axis=1)\n",
    "mi_all = mutual_info_classif(X_tr, y_tmp, discrete_features=False, random_state=42)\n",
    "all_ranked = np.argsort(mi_all)[::-1]\n",
    "print(f\"[Pipeline] Full MI ranking length: {len(all_ranked)}\")\n",
    "\n",
    "# Stage 1: TRUE mRMR Feature Ranking\n",
    "print(\"Stage 1: TRUE mRMR Feature Ranking (Optimized)\")\n",
    "ranked_features = true_mrmr_feature_selection(\n",
    "    X_tr, Y_tr_ohe,\n",
    "    n_features=n_mrmr,\n",
    "    sample_rows=sample_rows,\n",
    "    var_thresh=0.01\n",
    ")\n",
    "print(f\"[Pipeline] Ranked features (top-N for AGWO): {len(ranked_features)}\")\n",
    "\n",
    "# Stage 2: Slice training matrix to ranked features ONLY for Enhanced AGWO\n",
    "X_tr_ranked = X_tr[:, ranked_features]\n",
    "print(f\"[Pipeline] Ranked features shape: {X_tr_ranked.shape}\")\n",
    "\n",
    "# IMPORTANT: Adjust max_subset to not exceed available features\n",
    "actual_max_subset = min(subset_size, len(ranked_features))\n",
    "print(f\"[Pipeline] Adjusted max_subset: {subset_size} â†’ {actual_max_subset}\")\n",
    "\n",
    "# Stage 3: Enhanced AGWO Feature Selection\n",
    "print(\"\\nStage 3: Enhanced AGWO Feature Selection (Optimized)\")\n",
    "selected_features = enhanced_agwo_feature_selection(\n",
    "    X_tr_ranked, Y_tr_ohe, ranked_features,\n",
    "    n_wolves=n_wolves,\n",
    "    n_iter=n_iter,\n",
    "    min_subset=400,                # AGGRESSIVE: Reduced from 500\n",
    "    max_subset=actual_max_subset,\n",
    "    row_sample=2500,\n",
    "    knn_folds=3,\n",
    "    rf_folds=2,\n",
    "    rf_max_features=350,           # AGGRESSIVE: Reduced from 400\n",
    "    penalty_weight=0.020,          # AGGRESSIVE: Increased penalty from 0.015\n",
    "    patience=6,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"[Pipeline] Final selected features: {len(selected_features)}\")\n",
    "\n",
    "# Extract final feature matrices\n",
    "X_tr_final = X_tr[:, selected_features]\n",
    "X_va_final = X_va[:, selected_features]\n",
    "\n",
    "# Train/test split on combined (train+val)\n",
    "y_full = np.argmax(np.vstack([Y_tr_ohe, Y_va_ohe]), axis=1)\n",
    "X_full_sel = np.vstack([X_tr_final, X_va_final])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_full_sel, y_full, test_size=0.20, random_state=SEED, stratify=y_full\n",
    ")\n",
    "\n",
    "print(f\"[Pipeline] Train {X_train.shape}, Test {X_test.shape}\")\n",
    "print(f\"[Pipeline] Feature reduction: {X_tr.shape[1]} â†’ {X_tr_final.shape[1]} ({X_tr_final.shape[1]/X_tr.shape[1]:.1%})\")\n",
    "\n",
    "# Cleanup\n",
    "del X_tr_ranked\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\n[Pipeline] Total time: {time.time() - t_total:.2f}s\")\n",
    "print(\"âœ… Optimized two-stage feature selection completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Classifiers initialized (KNN, SVM, RF, LR, XGBoost. )\n"
     ]
    }
   ],
   "source": [
    "# Initialize Classifiers - NOW INCLUDING XGBoost\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5, weights='distance', n_jobs=-1)\n",
    "svm = SVC(kernel='rbf', probability=True, C=1.0, gamma='scale', random_state=SEED, cache_size=500)\n",
    "rf  = RandomForestClassifier(n_estimators=250, random_state=SEED, n_jobs=-1, max_features='sqrt')\n",
    "lr  = LogisticRegression(max_iter=500, random_state=SEED, n_jobs=-1, solver='saga')\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=300, max_depth=5, learning_rate=0.05, subsample=0.9, colsample_bytree=0.9,\n",
    "    random_state=SEED, n_jobs=-1, objective='multi:softprob', eval_metric='mlogloss'\n",
    " )\n",
    "\n",
    "print(\"âœ… Classifiers initialized (KNN, SVM, RF, LR, XGBoost. )\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifiers â€¦\n",
      "  Training KNN...\n",
      "  Training SVM...\n",
      "  Training Random Forest...\n",
      "  Training Random Forest...\n",
      "  Training Logistic Regression...\n",
      "  Training Logistic Regression...\n",
      "  Training XGBoost...\n",
      "  Training XGBoost...\n",
      "All classifiers trained successfully!\n",
      "All classifiers trained successfully!\n"
     ]
    }
   ],
   "source": [
    "# Train Classifiers\n",
    "print(\"Training classifiers â€¦\")\n",
    "\n",
    "print(\"  Training KNN...\")\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "print(\"  Training SVM...\")\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "print(\"  Training Random Forest...\")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "print(\"  Training Logistic Regression...\")\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "print(\"  Training XGBoost...\")\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "print(\"All classifiers trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions...\n",
      "Predictions completed!\n",
      "Predictions completed!\n"
     ]
    }
   ],
   "source": [
    "# Make Predictions\n",
    "print(\"Making predictions...\")\n",
    "\n",
    "knn_pred = knn.predict(X_test)\n",
    "svm_pred = svm.predict(X_test)\n",
    "rf_pred  = rf.predict(X_test)\n",
    "lr_pred  = lr.predict(X_test)\n",
    "xgb_pred = xgb.predict(X_test)\n",
    "\n",
    "# Probabilistic predictions (for ensemble)\n",
    "knn_proba = knn.predict_proba(X_test) if hasattr(knn, 'predict_proba') else None\n",
    "svm_proba = svm.predict_proba(X_test) if hasattr(svm, 'predict_proba') else None\n",
    "rf_proba  = rf.predict_proba(X_test) if hasattr(rf, 'predict_proba') else None\n",
    "lr_proba  = lr.predict_proba(X_test) if hasattr(lr, 'predict_proba') else None\n",
    "xgb_proba = xgb.predict_proba(X_test) if hasattr(xgb, 'predict_proba') else None\n",
    "\n",
    "print(\"Predictions completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual Classifier Accuracies:\n",
      "  KNN: 0.9937\n",
      "  SVM: 0.9847\n",
      "  RF : 0.9743\n",
      "  LR : 0.9810\n",
      "  XGB: 0.9873\n",
      "\n",
      "=== KNN Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    lung_aca       0.99      0.99      0.99      1000\n",
      "      lung_n       1.00      1.00      1.00      1000\n",
      "    lung_scc       0.99      0.99      0.99      1000\n",
      "\n",
      "    accuracy                           0.99      3000\n",
      "   macro avg       0.99      0.99      0.99      3000\n",
      "weighted avg       0.99      0.99      0.99      3000\n",
      "\n",
      "\n",
      "=== SVM Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    lung_aca       0.98      0.97      0.98      1000\n",
      "      lung_n       1.00      1.00      1.00      1000\n",
      "    lung_scc       0.97      0.98      0.98      1000\n",
      "\n",
      "    accuracy                           0.98      3000\n",
      "   macro avg       0.98      0.98      0.98      3000\n",
      "weighted avg       0.98      0.98      0.98      3000\n",
      "\n",
      "\n",
      "=== Random Forest Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    lung_aca       0.97      0.96      0.96      1000\n",
      "      lung_n       1.00      1.00      1.00      1000\n",
      "    lung_scc       0.96      0.97      0.96      1000\n",
      "\n",
      "    accuracy                           0.97      3000\n",
      "   macro avg       0.97      0.97      0.97      3000\n",
      "weighted avg       0.97      0.97      0.97      3000\n",
      "\n",
      "\n",
      "=== Logistic Regression Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    lung_aca       0.97      0.97      0.97      1000\n",
      "      lung_n       1.00      1.00      1.00      1000\n",
      "    lung_scc       0.97      0.97      0.97      1000\n",
      "\n",
      "    accuracy                           0.98      3000\n",
      "   macro avg       0.98      0.98      0.98      3000\n",
      "weighted avg       0.98      0.98      0.98      3000\n",
      "\n",
      "\n",
      "=== XGBoost Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    lung_aca       0.98      0.98      0.98      1000\n",
      "      lung_n       1.00      1.00      1.00      1000\n",
      "    lung_scc       0.98      0.98      0.98      1000\n",
      "\n",
      "    accuracy                           0.99      3000\n",
      "   macro avg       0.99      0.99      0.99      3000\n",
      "weighted avg       0.99      0.99      0.99      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Individual Classifier Results\n",
    "print(\"Individual Classifier Accuracies:\")\n",
    "knn_acc = accuracy_score(y_test, knn_pred)\n",
    "svm_acc = accuracy_score(y_test, svm_pred)\n",
    "rf_acc = accuracy_score(y_test, rf_pred)\n",
    "lr_acc = accuracy_score(y_test, lr_pred)\n",
    "xgb_acc = accuracy_score(y_test, xgb_pred)\n",
    "\n",
    "print(f\"  KNN: {knn_acc:.4f}\")\n",
    "print(f\"  SVM: {svm_acc:.4f}\")\n",
    "print(f\"  RF : {rf_acc:.4f}\")\n",
    "print(f\"  LR : {lr_acc:.4f}\")\n",
    "print(f\"  XGB: {xgb_acc:.4f}\")\n",
    "\n",
    "# Display individual classification reports\n",
    "target_names = [id2label[i] for i in range(num_classes)]\n",
    "\n",
    "print(\"\\n=== KNN Classification Report ===\")\n",
    "print(classification_report(y_test, knn_pred, target_names=target_names))\n",
    "\n",
    "print(\"\\n=== SVM Classification Report ===\")\n",
    "print(classification_report(y_test, svm_pred, target_names=target_names))\n",
    "\n",
    "print(\"\\n=== Random Forest Classification Report ===\")\n",
    "print(classification_report(y_test, rf_pred, target_names=target_names))\n",
    "\n",
    "print(\"\\n=== Logistic Regression Classification Report ===\")\n",
    "print(classification_report(y_test, lr_pred, target_names=target_names))\n",
    "\n",
    "print(\"\\n=== XGBoost Classification Report ===\")\n",
    "print(classification_report(y_test, xgb_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier ranking (bestâ†’worst):\n",
      "  1. KNN (acc=0.9937)\n",
      "  2. XGB (acc=0.9873)\n",
      "  3. SVM (acc=0.9847)\n",
      "  4. LR (acc=0.9810)\n",
      "  5. RF (acc=0.9743)\n",
      "Classifier weights (epsilon_j):\n",
      "  KNN: 0.2046\n",
      "  XGB: 0.2033\n",
      "  SVM: 0.2007\n",
      "  LR: 0.1976\n",
      "  RF: 0.1939\n",
      "Weighted Probability Ensemble Accuracy: 0.9920\n",
      "Improvement over best individual: -0.0017\n"
     ]
    }
   ],
   "source": [
    "# Weighted-Average Ensemble Method (Probability-Weighted, Performance-Ranked)\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Collect accuracies and probabilities\n",
    "clf_names = np.array(['KNN', 'SVM', 'RF', 'LR', 'XGB'])\n",
    "clf_accs  = np.array([knn_acc, svm_acc, rf_acc, lr_acc, xgb_acc])\n",
    "proba_dict = {\n",
    "    'KNN': knn_proba,\n",
    "    'SVM': svm_proba,\n",
    "    'RF':  rf_proba,\n",
    "    'LR':  lr_proba,\n",
    "    'XGB': xgb_proba,\n",
    "}\n",
    "\n",
    "# Rank classifiers by accuracy (descending)\n",
    "rank_idx = np.argsort(clf_accs)[::-1]\n",
    "ranked_names = clf_names[rank_idx]\n",
    "ranked_accs  = clf_accs[rank_idx]\n",
    "\n",
    "print('Classifier ranking (bestâ†’worst):')\n",
    "for i, (nm, sc) in enumerate(zip(ranked_names, ranked_accs), 1):\n",
    "    print(f'  {i}. {nm} (acc={sc:.4f})')\n",
    "\n",
    "# Compute T_j and epsilon weights\n",
    "T = [1.0]\n",
    "for j in range(1, len(ranked_accs)):\n",
    "    T.append(T[-1] * ranked_accs[j-1])\n",
    "T = np.array(T, dtype=float)\n",
    "epsilon = T / T.sum()\n",
    "\n",
    "print('Classifier weights (epsilon_j):')\n",
    "for nm, w in zip(ranked_names, epsilon):\n",
    "    print(f'  {nm}: {w:.4f}')\n",
    "\n",
    "# Probability-weighted fusion\n",
    "n_samples, n_classes = next(v for v in proba_dict.values() if v is not None).shape\n",
    "weighted_proba = np.zeros((n_samples, n_classes), dtype=float)\n",
    "for w, nm in zip(epsilon, ranked_names):\n",
    "    weighted_proba += w * proba_dict[nm]\n",
    "\n",
    "weighted_ensemble_pred = np.argmax(weighted_proba, axis=1)\n",
    "weighted_ens_acc = accuracy_score(y_test, weighted_ensemble_pred)\n",
    "\n",
    "print(f\"Weighted Probability Ensemble Accuracy: {weighted_ens_acc:.4f}\")\n",
    "print(f\"Improvement over best individual: {weighted_ens_acc - ranked_accs[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL PROBABILISTIC OUTPUTS (F* Features)\n",
      "============================================================\n",
      "KNN: shape (3000, 3), sample: [0. 1. 0.]\n",
      "SVM: shape (3000, 3), sample: [3.96532286e-07 9.99963911e-01 3.56920931e-05]\n",
      "RandomForest: shape (3000, 3), sample: [0. 1. 0.]\n",
      "LogisticRegression: shape (3000, 3), sample: [8.70526065e-05 9.99912947e-01 1.05752182e-14]\n",
      "XGBoost: shape (3000, 3), sample: [6.4971558e-05 9.9993086e-01 4.1075973e-06]\n",
      "WeightedEnsemble: shape (3000, 3), sample: [3.04891417e-05 9.99961501e-01 7.99808932e-06]\n",
      "Saved probabilistic predictions to final_probabilistic_predictions.pkl\n"
     ]
    }
   ],
   "source": [
    "# Expose final probabilistic outputs (including weighted ensemble)\n",
    "import pickle\n",
    "final_probabilistic_outputs = {\n",
    "    'KNN': knn_proba,\n",
    "    'SVM': svm_proba,\n",
    "    'RandomForest': rf_proba,\n",
    "    'LogisticRegression': lr_proba,\n",
    "    'XGBoost': xgb_proba,\n",
    "    'WeightedEnsemble': weighted_proba,\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL PROBABILISTIC OUTPUTS (F* Features)\")\n",
    "print(\"=\"*60)\n",
    "for clf_name, proba in final_probabilistic_outputs.items():\n",
    "    if proba is None:\n",
    "        print(f\"{clf_name}: None\")\n",
    "    else:\n",
    "        print(f\"{clf_name}: shape {proba.shape}, sample: {proba[0]}\")\n",
    "\n",
    "with open('final_probabilistic_predictions.pkl', 'wb') as f:\n",
    "    pickle.dump(final_probabilistic_outputs, f)\n",
    "print(\"Saved probabilistic predictions to final_probabilistic_predictions.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Individual Classifier Accuracies:\n",
      "  KNN: 0.9937\n",
      "  SVM: 0.9847\n",
      "  RF : 0.9743\n",
      "  LR : 0.9810\n",
      "  XGB: 0.9873\n",
      "\n",
      "=== KNN Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    lung_aca       0.99      0.99      0.99      1000\n",
      "      lung_n       1.00      1.00      1.00      1000\n",
      "    lung_scc       0.99      0.99      0.99      1000\n",
      "\n",
      "    accuracy                           0.99      3000\n",
      "   macro avg       0.99      0.99      0.99      3000\n",
      "weighted avg       0.99      0.99      0.99      3000\n",
      "\n",
      "\n",
      "=== SVM Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    lung_aca       0.98      0.97      0.98      1000\n",
      "      lung_n       1.00      1.00      1.00      1000\n",
      "    lung_scc       0.97      0.98      0.98      1000\n",
      "\n",
      "    accuracy                           0.98      3000\n",
      "   macro avg       0.98      0.98      0.98      3000\n",
      "weighted avg       0.98      0.98      0.98      3000\n",
      "\n",
      "\n",
      "=== Random Forest Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    lung_aca       0.97      0.96      0.96      1000\n",
      "      lung_n       1.00      1.00      1.00      1000\n",
      "    lung_scc       0.96      0.97      0.96      1000\n",
      "\n",
      "    accuracy                           0.97      3000\n",
      "   macro avg       0.97      0.97      0.97      3000\n",
      "weighted avg       0.97      0.97      0.97      3000\n",
      "\n",
      "\n",
      "=== Logistic Regression Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    lung_aca       0.97      0.97      0.97      1000\n",
      "      lung_n       1.00      1.00      1.00      1000\n",
      "    lung_scc       0.97      0.97      0.97      1000\n",
      "\n",
      "    accuracy                           0.98      3000\n",
      "   macro avg       0.98      0.98      0.98      3000\n",
      "weighted avg       0.98      0.98      0.98      3000\n",
      "\n",
      "\n",
      "=== XGBoost Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    lung_aca       0.98      0.98      0.98      1000\n",
      "      lung_n       1.00      1.00      1.00      1000\n",
      "    lung_scc       0.98      0.98      0.98      1000\n",
      "\n",
      "    accuracy                           0.99      3000\n",
      "   macro avg       0.99      0.99      0.99      3000\n",
      "weighted avg       0.99      0.99      0.99      3000\n",
      "\n",
      "\n",
      "////////////////////////////////////////////////////////////\n",
      "\n",
      "Ensemble Accuracy (Weighted Probability): 0.9920\n",
      "\n",
      "Improvement over best individual: -0.0017\n",
      "\n",
      "=== Ensemble Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    lung_aca       0.99      0.98      0.99      1000\n",
      "      lung_n       1.00      1.00      1.00      1000\n",
      "    lung_scc       0.98      0.99      0.99      1000\n",
      "\n",
      "    accuracy                           0.99      3000\n",
      "   macro avg       0.99      0.99      0.99      3000\n",
      "weighted avg       0.99      0.99      0.99      3000\n",
      "\n",
      "\n",
      "////////////////////////////////////////////////////////////\n",
      "\n",
      "============================================================\n",
      "FINAL RESULTS SUMMARY\n",
      "============================================================\n",
      "Total samples processed: 15000\n",
      "Features selected by AGWO: 450 / 6400 (7.0%)\n",
      "Test set size: 3000\n",
      "\n",
      "Classifier Accuracies:\n",
      "  KNN:              0.9937\n",
      "  SVM:              0.9847\n",
      "  Random Forest:    0.9743\n",
      "  Logistic Reg:     0.9810\n",
      "  XGBoost:          0.9873\n",
      "  Ensemble (Fusion): 0.9920 â† BEST\n",
      "\n",
      "Class Labels:\n",
      "  0: lung_aca\n",
      "  1: lung_n\n",
      "  2: lung_scc\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Final Formatted Results Display\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Individual Classifier Accuracies:\")\n",
    "print(f\"  KNN: {knn_acc:.4f}\")\n",
    "print(f\"  SVM: {svm_acc:.4f}\")\n",
    "print(f\"  RF : {rf_acc:.4f}\")\n",
    "print(f\"  LR : {lr_acc:.4f}\")\n",
    "print(f\"  XGB: {xgb_acc:.4f}\")\n",
    "\n",
    "print(\"\\n=== KNN Classification Report ===\")\n",
    "print(classification_report(y_test, knn_pred, target_names=target_names))\n",
    "\n",
    "print(\"\\n=== SVM Classification Report ===\")\n",
    "print(classification_report(y_test, svm_pred, target_names=target_names))\n",
    "\n",
    "print(\"\\n=== Random Forest Classification Report ===\")\n",
    "print(classification_report(y_test, rf_pred, target_names=target_names))\n",
    "\n",
    "print(\"\\n=== Logistic Regression Classification Report ===\")\n",
    "print(classification_report(y_test, lr_pred, target_names=target_names))\n",
    "\n",
    "print(\"\\n=== XGBoost Classification Report ===\")\n",
    "print(classification_report(y_test, xgb_pred, target_names=target_names))\n",
    "\n",
    "print(\"\\n\" + \"/\"*60)\n",
    "\n",
    "# Use weighted probability ensemble if available\n",
    "if 'weighted_ens_acc' in globals():\n",
    "    best_ens_acc = weighted_ens_acc\n",
    "    best_ens_pred = weighted_ensemble_pred\n",
    "    ens_method = \"Weighted Probability\"\n",
    "elif 'weighted_ens_acc_fixed' in globals():\n",
    "    best_ens_acc = weighted_ens_acc_fixed\n",
    "    best_ens_pred = weighted_pred_fixed_labels\n",
    "    ens_method = \"Weighted Priority\"\n",
    "elif 'ens_acc' in globals():\n",
    "    best_ens_acc = ens_acc\n",
    "    best_ens_pred = ens\n",
    "    ens_method = \"Priority-Based\"\n",
    "else:\n",
    "    best_ens_acc = max(knn_acc, svm_acc, rf_acc, lr_acc, xgb_acc)\n",
    "    best_ens_pred = None\n",
    "    ens_method = \"Best Single\"\n",
    "\n",
    "print(f\"\\nEnsemble Accuracy ({ens_method}): {best_ens_acc:.4f}\")\n",
    "\n",
    "best_individual = max(knn_acc, svm_acc, rf_acc, lr_acc, xgb_acc)\n",
    "improvement = best_ens_acc - best_individual\n",
    "print(f\"\\nImprovement over best individual: {improvement:.4f}\")\n",
    "\n",
    "if best_ens_pred is not None:\n",
    "    print(\"\\n=== Ensemble Classification Report ===\")\n",
    "    print(classification_report(y_test, best_ens_pred, target_names=target_names))\n",
    "\n",
    "print(\"\\n\" + \"/\"*60)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total samples processed: {len(y_full)}\")\n",
    "\n",
    "# Feature selection info\n",
    "if 'selected_features' in globals():\n",
    "    sel_count = len(selected_features)\n",
    "    orig_count = X_tr.shape[1] if 'X_tr' in globals() else X_full.shape[1]\n",
    "    print(f\"Features selected by AGWO: {sel_count} / {orig_count} ({sel_count/orig_count:.1%})\")\n",
    "\n",
    "print(f\"Test set size: {len(y_test)}\")\n",
    "\n",
    "print(\"\\nClassifier Accuracies:\")\n",
    "print(f\"  KNN:              {knn_acc:.4f}\")\n",
    "print(f\"  SVM:              {svm_acc:.4f}\")\n",
    "print(f\"  Random Forest:    {rf_acc:.4f}\")\n",
    "print(f\"  Logistic Reg:     {lr_acc:.4f}\")\n",
    "print(f\"  XGBoost:          {xgb_acc:.4f}\")\n",
    "print(f\"  Ensemble (Fusion): {best_ens_acc:.4f} â† BEST\")\n",
    "\n",
    "print(\"\\nClass Labels:\")\n",
    "for i, label in id2label.items():\n",
    "    print(f\"  {i}: {label}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lung_cancer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
